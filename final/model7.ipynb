{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 22:29:54.726057: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-14 22:29:54.753535: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-14 22:29:54.753565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-14 22:29:54.754249: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-14 22:29:54.758914: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-14 22:29:55.308755: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13100, 30, 64)\n"
     ]
    }
   ],
   "source": [
    "X_data = np.load(\"../data/data_64_30.npy\")\n",
    "X_data = np.transpose(X_data, (0, 2, 1))\n",
    "print(X_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13100, 30)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    \"../data/LJSpeech-1.1/metadata.csv\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"ID\", \"Text1\", \"Text2\"],\n",
    ")\n",
    "texts = data[\"Text1\"].to_list()\n",
    "ID = data[\"ID\"].to_list()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "num_classes = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "Y_data = pad_sequences(sequences, padding=\"post\", maxlen=30)\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer(tf.keras.layers.Layer):  # Change the base class to tf.keras.layers.Layer\n",
    "    def __init__(self, input_shape, filters=32, kernel_size=3, **kwargs):\n",
    "        super(ConvolutionalLayer, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Extract the number of filters from the input shape\n",
    "        if isinstance(input_shape, tuple):\n",
    "            self.filters = input_shape[-1]\n",
    "\n",
    "        self.conv1 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\")\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "\n",
    "        self.conv2 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\")\n",
    "        self.batch_norm2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.ReLU()\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        conv1_out = self.relu1(self.batch_norm1(self.conv1(inputs), training=training))\n",
    "        conv2_out = self.relu2(self.batch_norm2(self.conv2(conv1_out), training=training))\n",
    "        # print(\"CNN output shape is \",conv2_out.shape)\n",
    "        return conv2_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn_layer = ConvolutionalLayer(input_shape=(30, 64))  # Adjust input shape\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        cnn_output = self.cnn_layer(inputs, training=training)  # Explicitly call the 'call' method\n",
    "        return cnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask):\n",
    "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_qk = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_qk += mask\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_qk)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = tf.keras.layers.Dense(hidden)\n",
    "        self.linear2 = tf.keras.layers.Dense(d_model)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostionalEmbedding(tf.keras.Model):\n",
    "    def __init__(self,vocab_size=num_classes,embedding_dim=64):\n",
    "        super(PostionalEmbedding,self).__init__()\n",
    "        self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=30)\n",
    "    \n",
    "    def call(self,input,training=None):\n",
    "        if input.shape == (100,30,64):\n",
    "            return input\n",
    "        else:\n",
    "            output = self.embedding(input)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = tf.keras.layers.Dense(3 * d_model, use_bias=False)\n",
    "        self.linear_layer = tf.keras.layers.Dense(d_model, activation='relu')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        if len(x.shape) == 2:\n",
    "            x = tf.expand_dims(tf.expand_dims(x, axis=0), axis=1)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        batch_size, _, _ = x.shape\n",
    "\n",
    "        qkv = self.qkv_layer(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
    "        values = tf.reshape(values, (batch_size, -1, self.num_heads * self.head_dim))\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticrossHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MulticrossHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = tf.keras.layers.Dense(3 * d_model, use_bias=False)\n",
    "        self.linear_layer = tf.keras.layers.Dense(d_model, activation='relu')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        if len(x.shape) == 2:\n",
    "            x = tf.expand_dims(tf.expand_dims(x, axis=0), axis=1)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x,encoder_output,  mask):\n",
    "        batch_size, _, _ = x.shape\n",
    "\n",
    "        qkv = self.qkv_layer(x)\n",
    "        q, k, _ = tf.split(qkv, 3, axis=-1)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "\n",
    "        qkv = self.qkv_layer(encoder_output)\n",
    "        _, _, v = tf.split(qkv, 3, axis=-1)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
    "        values = tf.reshape(values, (batch_size, -1, self.num_heads * self.head_dim))\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, hidden, dropout_rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.cnn_layer = CNN()\n",
    "        self.multihead_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feedforward = PositionwiseFeedForward(d_model, hidden, drop_prob=dropout_rate)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "\n",
    "    def call(self, x, training=None, mask=None):\n",
    "        x = self.cnn_layer(x)\n",
    "        x_att = self.multihead_attention(x, mask)\n",
    "        x = x + x_att\n",
    "        x = self.dropout(x)\n",
    "        x = self.feedforward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, hidden, dropout_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden = hidden\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Define layers\n",
    "        self.encoder_layers = [EncoderLayer(d_model, num_heads, hidden, dropout_rate) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, x, training=None, mask=None):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.encoder_layers[i](x, training=training, mask=mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, vocab_size, hidden, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.embedding = PostionalEmbedding(vocab_size=vocab_size, embedding_dim=64)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attention = MulticrossHeadAttention(d_model, num_heads)\n",
    "        self.feedforward = PositionwiseFeedForward(d_model, hidden, drop_prob=dropout_rate)\n",
    "\n",
    "    def call(self, inputs, encoder_output, training=None, mask=None):\n",
    "        # Self-attention on the decoder side\n",
    "        x = self.embedding(inputs)\n",
    "        x_att_self = self.self_attention(x, mask)\n",
    "        x = x + x_att_self\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Cross-attention with the encoder output\n",
    "        x_att_cross = self.cross_attention(x, encoder_output, mask)\n",
    "        x = x + x_att_cross\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Feedforward layer\n",
    "        x = self.feedforward(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, num_layers, d_model, num_heads, hidden, dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden = hidden\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.flatten = layers.Flatten()\n",
    "        # self.output_layer = layers.Dense(Y_data.shape[-1], activation='softmax')\n",
    "        self.output_layer = layers.Dense(30, activation = \"relu\")\n",
    "        # Modify the output layer in the Decoder class\n",
    "        # self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "\n",
    "\n",
    "        # Define layers\n",
    "        self.decoder_layers = [DecoderLayer(d_model, num_heads, vocab_size, hidden, dropout_rate) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, x,encoder_output, training=None, mask=None):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.decoder_layers[i](x,encoder_output, training=training, mask=mask)\n",
    "        x = self.flatten(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self ,d_model,num_heads,hidden,num_layers,voacb_size,dropout_rate=0.1):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden = hidden\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.encoder = Encoder(d_model=d_model,num_heads=num_heads,hidden=hidden,num_layers=num_layers)\n",
    "        self.decoder = Decoder(d_model=d_model,num_heads=num_heads,num_layers=num_layers,hidden=hidden,dropout_rate=dropout_rate,vocab_size=voacb_size)\n",
    "\n",
    "    def call(self,x,training=None,mask=None):\n",
    "        encoder_output = self.encoder(x[:,:,:64],training,mask)\n",
    "        decoder_output = self.decoder(x[:,:,64],encoder_output,training,mask)\n",
    "        return decoder_output\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 22:29:56.575034: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-14 22:29:56.602856: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-14 22:29:56.603046: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-14 22:29:56.604132: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-14 22:29:56.604345: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-14 22:29:56.604487: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-14 22:29:56.677741: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-14 22:29:56.677988: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-14 22:29:56.678118: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-14 22:29:56.678205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2796 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "d_model = 64\n",
    "num_heads = 8\n",
    "hidden = 2048\n",
    "num_layer = 5                                                                                                                                                                                                                                                                  \n",
    "vocab_size = num_classes\n",
    "dropout_rate =0.1\n",
    "transformer = Transformer(d_model=d_model,num_heads=num_heads,hidden=hidden,num_layers=num_layer,voacb_size=vocab_size,dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "# loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "loss_fn = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = keras.metrics.MeanSquaredError()\n",
    "val_acc_metric = keras.metrics.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13100, 30, 65)\n"
     ]
    }
   ],
   "source": [
    "Y_data_copy = Y_data\n",
    "Y_data_copy = Y_data_copy[:,:,np.newaxis]\n",
    "result = np.concatenate((X_data, Y_data_copy), axis=-1)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.build(input_shape=(100,30,65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 22:29:58.057133: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2023-12-14 22:29:58.130967: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-14 22:29:58.316823: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-14 22:29:59.883222: I external/local_xla/xla/service/service.cc:168] XLA service 0x1cfef9b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-14 22:29:59.883253: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2023-12-14 22:29:59.886866: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702573199.961398   21795 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fda303afee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fda303afee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Training loss at step 0: 735.1713\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 651.9716\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 463.0652\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 640.8448\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 623.5383\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 513.1354\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 595.5078\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 628.4067\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 588.5087\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 652.2629\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 654.3896\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 756.7444\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 533.5228\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 537.4056\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 615.9465\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 607.6205\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 755.7678\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 696.0709\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 759.6199\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 680.2559\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 726.7502\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 740.1453\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 644.7668\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 837.5030\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 721.8677\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 623.7848\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 753.0257\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 741.9982\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 677.2465\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 662.2714\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 622.6977\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 682.9101\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 665.3325\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 791.2839\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 672.2357\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 741.1065\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 676.6646\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 682.3513\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 751.3423\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 781.9693\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 732.0388\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 703.7653\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 745.0587\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 705.9020\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 697.1786\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 645.0779\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 763.7576\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 766.1877\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 775.1886\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 711.1981\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 748.4490\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 686.2092\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 674.1131\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 651.4384\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 740.1234\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 827.4146\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 644.6911\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 609.1736\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 590.2107\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 878.0856\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 742.0483\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 604.8589\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 542.8945\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 502.4047\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 616.1935\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 458.5443\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 500.8924\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 688.7137\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 878.8795\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 849.2935\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 784.3613\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 878.4274\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 970.3065\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 788.9663\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 812.3666\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 779.2020\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 916.3013\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 702.7294\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 488.8435\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 706.4773\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 542.6886\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 502.7105\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 666.8208\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 752.9510\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 581.8513\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 550.6495\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 530.5441\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 417.6968\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 383.7919\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 460.9052\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 448.5516\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 360.9046\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 439.1188\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 397.4002\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 428.5899\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 518.0388\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 455.3310\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 472.6898\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 522.2953\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 572.6104\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 402.8830\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 356.2843\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 577.4739\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 485.9273\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 468.7948\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 714.2986\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 715.7107\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 664.3226\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 489.0067\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 705.4524\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 816.7477\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 468.9894\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 434.7589\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 495.8081\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 475.8026\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 526.0251\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 417.3242\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 386.5226\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 614.4069\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 481.3540\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 444.5945\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 494.8766\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 386.9918\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 436.5472\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 464.8664\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 495.7489\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 551.4199\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 587.8774\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 552.2899\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 596.5483\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 559.2842\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 1: 623.4355\n",
      "\n",
      "Epoch 1 ended\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss at step 0: 731.0959\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 648.1743\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 459.8845\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 638.2332\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 620.0001\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 511.7644\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 594.7156\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 625.7645\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 586.0874\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 650.0544\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 653.6213\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 757.0221\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 531.2424\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 536.9314\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 616.1882\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 607.5388\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 755.2073\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 695.5655\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 759.4791\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 680.0084\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 726.5898\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 739.9249\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 644.8791\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 837.0080\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 722.0215\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 623.5504\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 752.8524\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 742.0526\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 677.4098\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 662.3816\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 622.4508\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 682.4371\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 665.2692\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 791.1295\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 672.1617\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 740.9854\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 676.7529\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 682.2662\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 751.5977\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 781.9266\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 731.5700\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 703.7083\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 745.3468\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 706.0402\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 697.4661\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 645.2785\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 763.7240\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 766.2117\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 775.0987\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 711.3282\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 748.1559\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 686.1635\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 674.1054\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 651.2578\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 740.1624\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 827.3992\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 644.8536\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 609.4479\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 590.3373\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 878.1185\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 741.8171\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 604.7494\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 542.8603\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 502.2807\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 615.8990\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 458.8503\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 500.6209\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 688.5129\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 878.7325\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 849.1144\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 784.2281\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 878.5072\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 970.5223\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 788.6215\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 811.9444\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 779.0778\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 916.2355\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 702.5405\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 488.9397\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 706.1149\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 542.3777\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 502.5795\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 666.6921\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 752.9026\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 581.7009\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 550.2816\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 529.2090\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 417.3901\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 384.0414\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 460.7490\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 448.0629\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 360.4941\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 438.4279\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 396.4824\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 427.8188\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 517.2645\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 454.6495\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 472.1129\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 521.7750\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 572.4600\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 402.9934\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 356.4986\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 577.3946\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 485.5516\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 468.3941\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 714.0700\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 715.8616\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 664.2462\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 489.0375\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 705.5322\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 816.5561\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 468.9283\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 434.6769\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 495.7812\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 475.8732\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 526.0270\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 417.3235\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 386.4521\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 614.4376\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 481.3482\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 444.5919\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 494.9243\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 386.8865\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 436.4280\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 464.7407\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 495.7634\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 551.4846\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 587.8590\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 552.2311\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 596.4221\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 559.0751\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 2: 623.0920\n",
      "\n",
      "Epoch 2 ended\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss at step 0: 730.9068\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 648.0439\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 459.8611\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 638.0314\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 620.0518\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 511.7350\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 594.6426\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 625.7327\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 585.9962\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 650.0652\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 653.5977\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 757.0246\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 531.4822\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 536.9758\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 616.2349\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 607.4791\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 755.2742\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 695.5759\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 759.4528\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 680.0106\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 726.6700\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 740.0773\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 644.9700\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 836.7706\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 722.0775\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 623.4907\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 752.7969\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 742.0458\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 677.0898\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 662.8476\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 622.4122\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 682.3088\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 665.4875\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 790.8787\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 671.4524\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 739.9692\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 675.7723\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 681.4507\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 750.4202\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 780.6744\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 730.4747\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 702.4430\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 743.4305\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 705.5758\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 695.6985\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 643.1582\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 762.8482\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 764.6026\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 773.4487\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 709.6047\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 746.6689\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 685.5388\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 673.8765\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 650.7581\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 739.6593\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 826.4719\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 643.7127\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 608.2310\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 589.2823\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 876.1962\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 740.0262\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 603.1715\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 541.7474\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 500.3815\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 614.7714\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 456.6679\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 499.0611\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 687.2022\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 877.3517\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 847.7057\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 782.1006\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 877.1203\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 968.8875\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 787.2246\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 810.3072\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 777.5862\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 914.6850\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 700.9224\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 486.1244\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 703.7264\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 539.4426\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 498.1167\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 661.9811\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 747.4297\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 578.1397\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 548.1785\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 526.6880\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 413.2485\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 381.5452\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 457.6973\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 445.3542\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 358.8321\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 434.5667\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 392.9284\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 424.0762\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 512.4769\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 449.5881\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 469.1198\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 513.9211\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 567.1788\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 403.6155\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 354.0677\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 575.4699\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 483.7680\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 466.3654\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 712.1358\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 714.8554\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 662.5784\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 487.8338\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 703.1665\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 814.0956\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 468.5075\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 434.1978\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 493.1906\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 474.4179\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 523.2388\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 415.0092\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 384.7492\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 612.2095\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 477.1295\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 441.6411\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 494.5396\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 383.9027\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 434.8794\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 462.7865\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 494.4916\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 550.0450\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 585.6011\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 549.9567\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 592.6240\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 555.2911\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 3: 621.5508\n",
      "\n",
      "Epoch 3 ended\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss at step 0: 726.4294\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 645.8735\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 454.2868\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 632.7897\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 616.2151\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 508.4008\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 588.9346\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 619.5164\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 581.7302\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 639.2194\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 643.6243\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 744.3872\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 515.6013\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 517.9302\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 587.5432\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 578.9324\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 710.8743\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 662.6295\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 715.3091\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 627.6102\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 664.8689\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 656.4124\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 564.6215\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 733.7223\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 610.4523\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 504.2625\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 624.8801\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 591.6274\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 574.0112\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 526.0861\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 503.8238\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 516.5058\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 506.6141\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 578.3774\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 504.1878\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 536.7065\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 478.0704\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 491.6548\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 557.7714\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 542.3973\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 497.1088\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 447.3903\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 483.9456\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 448.4250\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 495.6752\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 445.2655\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 472.7196\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 507.4618\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 459.3083\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 448.1928\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 437.0085\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 433.3513\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 394.4004\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 374.2088\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 453.8519\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 518.0535\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 413.3654\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 360.7731\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 361.3956\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 555.7654\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 428.0779\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 363.2321\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 334.1120\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 299.7173\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 383.1926\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 259.3534\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 295.1414\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 409.2299\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 499.0124\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 481.2980\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 426.7950\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 480.1227\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 506.7758\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 484.9451\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 463.0881\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 454.7443\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 535.3308\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 428.6027\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 476.0795\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 518.2057\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 422.6598\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 398.6946\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 537.7958\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 576.5719\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 400.1681\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 388.8464\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 375.7382\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 318.4474\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 284.1822\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 307.2617\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 311.6081\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 251.8754\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 295.7826\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 236.7331\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 234.5011\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 303.7439\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 269.3724\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 246.9624\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 284.5287\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 335.1456\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 233.9062\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 189.0566\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 338.8695\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 272.1730\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 256.9470\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 421.7545\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 386.2592\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 403.9865\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 323.3023\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 395.8269\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 456.5655\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 229.9952\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 232.6362\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 254.5705\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 253.0497\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 269.5471\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 225.1214\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 191.3221\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 307.8547\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 252.5008\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 228.4595\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 247.3968\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 192.7343\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 214.7021\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 229.1031\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 232.3191\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 258.7384\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 271.0296\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 248.1920\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 271.6175\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 252.8250\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 4: 430.8595\n",
      "\n",
      "Epoch 4 ended\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss at step 0: 311.4417\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 270.6906\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 205.8133\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 273.6125\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 252.8958\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 207.9288\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 242.9587\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 256.2484\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 238.6389\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 281.9729\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 246.9626\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 324.5575\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 215.7754\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 211.2308\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 256.3860\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 240.6664\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 312.2196\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 281.6827\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 296.1344\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 271.7284\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 291.5729\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 277.8304\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 250.3464\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 329.4838\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 307.2394\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 245.1971\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 292.8466\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 279.9095\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 263.3333\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 257.7197\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 236.4345\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 274.7870\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 266.5617\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 289.2429\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 247.9040\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 280.2139\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 244.3509\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 259.5181\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 277.0957\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 318.7323\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 260.1378\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 254.6488\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 270.5532\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 284.5538\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 285.2365\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 256.6456\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 284.0701\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 249.0426\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 293.2002\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 281.0596\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 253.2729\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 282.1880\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 221.5003\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 229.0942\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 237.8745\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 285.8446\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 214.3175\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 205.5189\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 195.5536\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 304.6861\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 244.6441\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 206.1313\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 197.6686\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 164.6557\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 198.5090\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 157.0776\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 171.7677\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 228.2917\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 290.2792\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 296.0252\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 272.1161\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 318.2526\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 328.0080\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 272.5267\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 288.6021\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 306.4630\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 301.4344\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 242.7366\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 193.8517\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 284.8468\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 214.0641\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 211.6442\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 267.0355\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 300.3874\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 225.6607\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 209.0271\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 197.8141\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 165.9178\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 154.8158\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 187.2361\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 195.4989\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 153.0394\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 183.6584\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 162.3054\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 159.6044\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 191.6614\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 169.6549\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 178.8982\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 172.7186\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 208.6850\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 140.0992\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 112.6693\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 217.5974\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 166.3683\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 159.4733\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 281.0161\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 260.9609\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 226.7045\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 175.8322\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 262.1302\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 301.2616\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 184.4415\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 173.1505\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 193.4235\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 174.4804\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 196.5688\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 156.5512\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 138.6933\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 230.6409\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 169.0896\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 158.0521\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 198.3732\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 143.5786\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 136.5969\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 154.5696\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 160.9721\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 191.8713\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 197.0840\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 175.7797\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 180.0803\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 179.4675\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 5: 232.8981\n",
      "\n",
      "Epoch 5 ended\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss at step 0: 242.8141\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 207.1117\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 153.3765\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 185.8121\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 187.8475\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 151.9171\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 163.8701\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 173.4040\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 159.6597\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 166.4187\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 187.5359\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 200.8414\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 146.4486\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 144.0124\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 155.1354\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 155.0189\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 182.5686\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 172.3995\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 181.3636\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 162.5005\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 171.6975\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 181.6546\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 152.9904\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 191.3984\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 171.9422\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 146.8843\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 194.1059\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 197.2152\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 167.7924\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 196.1669\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 176.5391\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 196.4480\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 223.1931\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 228.8370\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 185.1638\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 244.4062\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 173.9397\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 205.9329\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 264.4228\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 228.5335\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 244.3291\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 223.9207\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 193.9719\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 220.5811\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 231.0225\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 178.3908\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 219.5741\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 187.0310\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 218.9519\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 180.9458\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 195.9884\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 183.8675\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 174.0891\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 149.5973\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 181.1341\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 204.8073\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 171.3756\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 159.4160\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 154.0763\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 268.3794\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 194.2395\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 164.3255\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 137.0535\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 122.3995\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 143.8977\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 121.4659\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 121.9916\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 181.6353\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 236.9853\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 212.7294\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 196.1789\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 244.4802\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 243.5604\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 199.1105\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 221.1689\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 202.4523\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 237.7491\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 203.0847\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 133.4808\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 196.1223\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 154.1294\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 139.0711\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 217.9842\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 238.1259\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 173.6288\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 150.3863\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 162.2111\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 124.8586\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 118.9233\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 138.5043\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 147.0189\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 107.0306\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 140.6736\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 122.9620\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 111.2285\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 159.5085\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 128.7855\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 133.6103\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 150.1336\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 150.7449\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 104.3286\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 87.6881\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 153.5690\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 128.5774\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 114.5040\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 182.5972\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 188.0681\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 172.4644\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 127.8930\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 177.5318\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 205.9056\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 120.8512\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 109.1665\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 144.8853\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 131.8081\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 137.6324\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 123.9392\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 111.7788\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 150.3487\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 114.2949\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 112.1057\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 121.1349\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 106.5015\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 116.6490\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 116.2371\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 126.9800\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 137.9801\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 158.8580\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 130.9072\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 136.8317\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 127.2396\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 6: 169.3409\n",
      "\n",
      "Epoch 6 ended\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss at step 0: 252.1213\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 206.8253\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 119.8215\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 194.1468\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 188.7149\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 131.1304\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 172.3181\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 178.3732\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 133.0492\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 170.3252\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 199.3822\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 183.3077\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 145.0016\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 185.3083\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 153.2573\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 141.6915\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 194.0627\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 162.2764\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 170.9486\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 191.9750\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 164.5570\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 179.4110\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 159.1913\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 180.8113\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 156.0050\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 164.4424\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 163.1885\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 165.3906\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 174.6415\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 165.6374\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 132.7969\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 178.4495\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 137.4870\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 191.2135\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 173.3201\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 167.4366\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 151.4370\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 186.1999\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 154.0646\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 185.4737\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 221.5174\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 174.4935\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 151.4250\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 190.5619\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 168.8343\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 150.3902\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 199.4252\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 187.7732\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 161.3375\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 175.4471\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 178.0273\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 168.9691\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 153.5786\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 136.1046\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 153.1868\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 172.0294\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 140.8300\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 131.4639\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 115.4752\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 217.9893\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 175.3473\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 160.1935\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 122.5673\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 127.3118\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 155.0262\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 99.0220\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 122.4148\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 193.7288\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 224.2442\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 202.2631\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 234.6430\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 250.3196\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 217.8655\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 218.9457\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 237.4062\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 180.4175\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 206.0569\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 166.6414\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 117.6579\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 189.6544\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 133.8222\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 117.2607\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 161.5402\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 181.1058\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 131.4499\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 124.7953\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 120.7878\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 94.5274\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 88.8454\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 104.2057\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 109.2156\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 80.6528\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 102.9823\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 97.8461\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 94.3024\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 118.5794\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 100.9186\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 104.1768\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 105.7806\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 115.2303\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 82.4658\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 75.0105\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 125.8316\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 102.6952\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 85.3999\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 154.7670\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 152.9969\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 145.6699\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 103.7745\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 145.2090\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 203.0854\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 119.5993\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 105.3934\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 115.8185\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 108.5475\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 122.5023\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 98.3519\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 88.5885\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 136.6030\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 95.4743\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 97.9542\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 115.4160\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 105.4511\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 105.9155\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 99.8088\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 105.7455\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 119.8947\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 123.6458\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 114.9671\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 128.2835\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 132.4824\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 7: 150.0851\n",
      "\n",
      "Epoch 7 ended\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss at step 0: 175.2785\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 160.5524\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 119.6323\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 158.4747\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 137.9228\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 124.3970\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 148.8680\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 139.3923\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 136.6429\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 158.7090\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 140.2702\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 157.7169\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 132.4813\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 135.2922\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 128.5175\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 128.5843\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 150.0810\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 127.8075\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 164.0648\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 165.2654\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 156.1353\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 147.7462\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 129.3972\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 139.6669\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 151.1215\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 135.5127\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 139.5446\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 137.1340\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 126.9575\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 130.0143\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 120.0979\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 141.4348\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 131.6030\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 151.7816\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 121.0038\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 139.1020\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 146.3185\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 134.7813\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 135.9271\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 132.8265\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 153.4311\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 152.5837\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 160.7913\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 138.1858\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 131.7584\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 136.8393\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 137.1082\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 154.1074\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 176.9031\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 173.1168\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 153.8300\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 217.6819\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 186.8641\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 111.4134\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 186.0717\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 246.3336\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 157.5510\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 113.7216\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 146.7832\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 238.5622\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 139.0677\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 130.3836\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 137.5849\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 123.5747\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 135.3977\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 99.0854\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 104.3966\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 125.6771\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 166.1706\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 171.9525\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 181.9986\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 181.2884\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 198.7252\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 154.6352\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 177.0569\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 174.4462\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 233.6385\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 149.7018\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 107.5628\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 179.7457\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 135.6052\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 94.2981\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 134.2986\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 171.3033\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 136.0998\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 111.6465\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 102.8332\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 89.9930\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 89.3525\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 93.4545\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 103.5877\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 85.3270\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 96.2876\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 86.3455\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 92.8986\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 104.8328\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 99.9810\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 95.5300\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 107.4155\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 109.9726\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 78.5194\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 71.9661\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 115.0483\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 96.4488\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 94.5095\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 142.0375\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 141.1936\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 139.0787\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 101.3937\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 121.5516\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 174.3521\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 104.8584\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 94.5025\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 110.3268\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 109.2811\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 114.0847\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 86.7474\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 74.5654\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 107.1027\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 86.8616\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 90.8652\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 108.5233\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 78.9033\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 94.7248\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 97.4457\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 95.6991\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 102.5905\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 103.9203\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 104.8637\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 106.6462\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 107.6124\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 8: 132.6799\n",
      "\n",
      "Epoch 8 ended\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss at step 0: 166.1568\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 137.8498\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 98.2749\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 114.4431\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 117.5540\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 95.8763\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 117.5572\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 119.5565\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 98.0752\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 114.1666\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 113.0232\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 122.0380\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 90.8683\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 100.2513\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 113.3453\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 110.3863\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 138.9950\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 112.4625\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 133.2634\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 113.9006\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 117.5755\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 118.1961\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 106.2378\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 122.4045\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 115.7486\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 100.8530\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 127.4501\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 117.7809\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 110.2168\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 122.3070\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 95.3126\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 109.4550\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 117.7273\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 134.8550\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 104.0719\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 121.7637\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 114.1685\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 108.2990\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 124.6813\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 125.5494\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 127.8091\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 115.8256\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 117.5628\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 114.8385\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 122.6496\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 115.6183\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 121.8300\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 117.6225\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 112.2415\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 118.0976\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 138.9803\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 131.1798\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 127.0507\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 132.4477\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 114.6184\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 156.5453\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 132.7173\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 112.6023\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 93.1378\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 169.1231\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 150.3227\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 114.9193\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 101.4996\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 103.7613\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 131.4757\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 88.8271\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 81.2718\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 112.1234\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 152.4030\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 145.4666\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 141.8248\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 168.4917\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 176.6212\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 127.5066\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 168.3852\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 137.1195\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 159.9786\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 137.1325\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 93.5298\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 133.6320\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 97.8892\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 108.2746\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 146.3830\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 131.7037\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 117.7996\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 127.7772\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 129.2044\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 93.9444\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 77.6455\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 89.6282\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 95.2383\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 73.5147\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 96.8150\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 86.1703\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 95.5838\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 107.3201\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 100.6758\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 94.9204\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 104.2554\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 107.9399\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 84.3106\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 80.1681\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 103.6079\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 91.7392\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 100.0463\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 137.1919\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 120.6464\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 118.5803\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 94.8104\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 120.6039\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 135.7745\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 87.7980\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 89.5297\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 100.3453\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 87.7904\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 100.6320\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 80.6418\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 73.7547\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 101.1469\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 73.5107\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 71.4023\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 98.5168\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 73.1861\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 78.1746\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 81.1637\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 87.0303\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 95.9257\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 95.2820\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 94.6853\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 99.6593\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 101.9188\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 9: 113.0057\n",
      "\n",
      "Epoch 9 ended\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss at step 0: 154.2376\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 129.8323\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 96.5502\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 137.9000\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 132.2350\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 86.3815\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 100.1396\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 123.2756\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 114.0302\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 103.9666\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 99.1943\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 140.7303\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 93.3455\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 82.3917\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 94.7640\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 104.4937\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 136.6575\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 103.9144\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 127.6323\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 104.0236\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 106.4621\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 116.2695\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 104.9318\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 134.4316\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 108.4242\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 101.9189\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 124.5435\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 101.9749\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 108.4191\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 111.4803\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 89.2142\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 106.6562\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 107.7093\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 107.3360\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 103.8467\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 114.7484\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 97.9996\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 98.5900\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 118.0388\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 110.9592\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 104.7333\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 107.1249\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 112.5430\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 97.4787\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 113.8827\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 102.9912\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 117.2175\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 116.4503\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 111.6841\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 96.4875\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 113.3911\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 105.8413\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 101.5219\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 90.0881\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 100.2755\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 117.5980\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 88.7117\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 88.2564\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 85.3853\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 166.8224\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 113.3373\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 105.9760\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 92.1957\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 76.6571\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 90.1495\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 73.4750\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 78.2710\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 113.0376\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 131.4365\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 136.4593\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 126.7820\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 132.7584\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 158.2027\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 131.9124\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 137.5372\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 130.6269\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 163.1402\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 98.6905\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 78.7602\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 141.7443\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 117.6784\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 91.2888\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 98.6948\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 144.7214\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 115.5629\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 91.4642\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 93.2724\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 79.7012\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 73.7121\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 87.4530\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 88.1664\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 76.9440\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 75.9776\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 67.1139\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 78.2555\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 96.3835\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 90.9981\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 80.5846\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 85.5655\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 98.7904\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 71.8017\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 67.5576\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 94.8241\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 90.7757\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 79.6949\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 115.0625\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 108.1396\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 115.2983\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 82.7904\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 101.8611\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 128.9049\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 69.6655\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 66.1980\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 85.6339\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 76.3179\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 87.0800\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 74.7705\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 64.4740\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 85.2311\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 73.1982\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 62.9055\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 78.7977\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 62.4968\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 71.4497\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 72.4505\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 77.8415\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 93.1697\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 96.3728\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 91.0359\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 101.3952\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 83.5713\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 10: 101.9266\n",
      "\n",
      "Epoch 10 ended\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss at step 0: 130.9949\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 114.8772\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 73.0707\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 93.8406\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 101.7668\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 83.6915\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 82.0386\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 82.4536\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 89.7091\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 104.7176\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 95.4501\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 110.2944\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 83.4360\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 83.8897\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 87.4689\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 87.0918\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 105.8164\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 102.1809\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 107.8967\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 96.7030\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 108.5652\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 102.9287\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 90.1530\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 117.0699\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 99.7734\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 81.1773\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 112.0645\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 109.3306\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 89.4731\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 89.5462\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 86.9251\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 101.7889\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 93.8488\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 105.4278\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 92.5861\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 93.3573\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 89.3854\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 93.1809\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 103.0415\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 103.8174\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 98.9746\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 82.5141\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 95.3465\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 101.5161\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 91.2515\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 87.7130\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 107.7763\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 101.3104\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 95.4534\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 97.2959\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 100.9817\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 95.2812\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 95.5365\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 84.3743\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 96.4778\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 103.0029\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 85.2787\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 80.8261\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 72.6845\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 120.3311\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 97.4558\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 85.8510\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 75.5775\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 66.5401\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 82.0280\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 70.5488\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 65.3963\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 91.5708\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 118.2332\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 111.1094\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 112.5469\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 120.4051\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 138.6620\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 120.4035\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 120.9339\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 113.4473\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 144.8101\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 96.0008\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 66.6108\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 121.0367\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 90.7420\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 77.0704\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 92.0265\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 122.1144\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 78.4810\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 85.7113\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 81.1828\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 66.8089\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 59.4127\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 68.4211\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 74.2667\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 58.5737\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 70.8895\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 59.1758\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 60.6024\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 73.3326\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 74.5454\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 70.2719\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 76.8366\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 79.8766\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 59.7844\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 55.8740\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 85.0636\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 69.0991\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 67.8440\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 109.1084\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 100.9782\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 100.3470\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 71.0177\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 95.7437\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 113.3153\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 70.4994\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 65.4553\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 73.7760\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 71.9878\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 72.2971\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 62.0246\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 59.9172\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 80.0007\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 66.4747\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 66.2239\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 70.7865\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 58.9461\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 62.4240\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 66.2792\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 72.0187\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 85.6878\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 83.9158\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 74.2267\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 87.5733\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 79.5123\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 11: 89.1028\n",
      "\n",
      "Epoch 11 ended\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss at step 0: 135.8922\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 98.9175\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 68.5339\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 98.6490\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 93.3824\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 85.6151\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 90.8148\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 94.2035\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 75.7376\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 90.5496\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 90.1831\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 100.3536\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 83.4910\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 91.2383\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 92.8386\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 83.6449\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 111.9330\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 106.1580\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 96.2255\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 84.3269\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 112.7633\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 110.1112\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 88.5117\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 105.5976\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 113.0919\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 101.7899\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 100.4558\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 99.7931\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 102.7144\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 93.6559\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 78.9205\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 93.6974\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 103.4593\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 108.9726\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 81.9523\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 104.6965\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 91.3433\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 89.2182\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 126.7201\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 129.3624\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 87.4217\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 100.4722\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 110.4118\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 83.5805\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 96.2404\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 100.9774\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 99.6159\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 95.3001\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 104.8073\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 90.2129\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 97.3705\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 94.0095\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 87.3581\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 79.9269\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 96.4753\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 105.0636\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 80.3289\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 82.1320\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 73.7547\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 110.7234\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 91.8858\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 83.9171\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 73.4087\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 63.1335\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 86.3479\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 59.0039\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 60.3664\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 79.0111\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 101.7742\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 109.5975\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 96.4948\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 105.9980\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 118.6993\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 93.0737\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 119.3532\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 116.4026\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 108.1291\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 96.7208\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 72.8555\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 88.9429\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 69.5091\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 65.6256\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 84.4823\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 96.9227\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 85.1841\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 85.2262\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 83.3506\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 60.3248\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 62.2990\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 73.4989\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 70.5547\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 63.1808\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 70.6513\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 69.2024\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 62.9789\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 73.7436\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 75.1832\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 71.3192\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 71.0226\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 78.6295\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 56.6805\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 53.1818\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 74.0882\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 67.6712\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 67.0692\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 93.8643\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 93.5386\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 90.6462\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 68.0692\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 90.5442\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 99.7434\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 62.9427\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 62.1192\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 68.5171\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 69.5693\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 71.0432\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 60.2156\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 55.1258\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 75.4530\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 59.9761\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 59.2229\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 65.7374\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 58.1802\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 57.5725\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 60.0684\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 67.1688\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 64.6961\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 78.5909\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 66.8396\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 72.0312\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 74.8281\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 12: 85.8679\n",
      "\n",
      "Epoch 12 ended\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss at step 0: 116.8683\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 98.6956\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 74.7141\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 98.2777\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 90.8879\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 69.6828\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 90.8011\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 96.7239\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 75.4160\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 80.3915\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 88.7261\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 99.1301\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 70.9678\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 85.7163\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 93.7360\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 90.9001\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 94.0173\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 103.3437\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 115.4369\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 87.8027\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 101.6936\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 105.3065\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 97.5372\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 103.1936\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 94.1274\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 106.2010\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 120.2130\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 109.2056\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 96.1839\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 117.9633\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 119.7105\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 100.5437\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 92.1222\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 140.1803\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 115.5344\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 96.5319\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 96.8095\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 120.2462\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 111.5661\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 97.3399\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 113.6060\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 117.6868\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 100.4532\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 94.1654\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 114.0277\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 93.7246\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 92.4509\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 102.8517\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 112.3435\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 97.5512\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 88.7462\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 100.0253\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 80.3893\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 80.0493\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 85.5949\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 99.0422\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 75.9496\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 71.2084\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 77.3291\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 98.8692\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 90.8367\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 69.8938\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 64.0371\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 57.4895\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 67.9056\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 54.1129\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 60.4438\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 85.2451\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 100.2478\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 101.6906\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 93.0676\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 107.2677\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 110.0242\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 80.1923\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 89.3266\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 97.6219\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 104.6499\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 83.9337\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 63.4382\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 82.3489\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 64.6400\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 60.2586\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 76.6203\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 93.5422\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 74.2031\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 62.3564\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 67.7478\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 53.3301\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 50.9510\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 59.3685\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 64.1827\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 52.0329\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 64.3243\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 53.3095\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 56.1458\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 71.1705\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 62.8984\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 60.3232\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 66.3123\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 76.0872\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 51.9704\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 43.2006\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 65.5351\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 58.5980\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 60.7912\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 89.1924\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 83.7610\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 84.8546\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 61.1114\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 82.6841\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 96.5399\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 60.4162\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 52.8708\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 64.8565\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 58.5671\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 63.5451\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 53.9331\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 50.0311\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 67.0311\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 61.8669\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 52.8051\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 62.4173\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 53.9111\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 56.4368\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 54.0522\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 58.5931\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 66.5161\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 72.9789\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 65.8995\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 71.4093\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 72.4736\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 13: 82.5415\n",
      "\n",
      "Epoch 13 ended\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss at step 0: 99.9763\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 97.7948\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 64.7504\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 87.4422\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 85.7007\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 74.0932\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 86.8437\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 95.5519\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 95.2097\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 95.8041\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 84.3952\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 108.6401\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 79.3968\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 73.9993\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 77.0357\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 86.2709\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 102.4083\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 88.8291\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 100.8877\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 100.8822\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 107.8454\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 97.4098\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 95.3809\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 125.2240\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 82.3667\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 83.8652\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 125.6934\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 119.4565\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 89.5668\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 97.5223\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 117.1182\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 115.8769\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 91.3863\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 116.6948\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 123.2721\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 135.0719\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 102.5148\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 89.9939\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 135.6515\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 132.4025\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 88.5075\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 92.8461\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 138.8212\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 120.8455\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 100.0870\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 85.3766\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 139.4072\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 117.4738\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 91.2829\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 96.5808\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 121.9260\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 113.0774\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 84.4894\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 89.4101\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 116.0549\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 119.2959\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 75.9531\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 86.8300\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 96.7357\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 155.9396\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 99.2973\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 82.4092\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 85.8282\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 79.3759\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 81.6648\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 56.4477\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 71.1657\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 107.2664\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 121.3631\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 100.6000\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 110.4998\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 130.7749\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 113.3727\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 103.0040\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 118.6684\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 101.9472\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 111.2467\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 90.3740\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 67.1189\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 92.2093\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 67.0314\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 61.8301\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 98.2771\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 106.6777\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 70.8640\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 64.9851\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 79.4326\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 58.8699\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 54.1490\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 62.3048\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 62.2384\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 49.6769\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 64.7240\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 55.5529\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 61.9941\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 69.0311\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 59.3283\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 58.8178\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 71.6628\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 77.9574\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 53.3185\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 45.2685\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 68.2530\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 55.1302\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 55.7420\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 84.4968\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 82.8169\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 82.1632\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 64.9914\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 75.5905\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 91.5235\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 60.7919\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 56.5146\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 62.9594\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 58.1809\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 64.0593\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 54.9464\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 54.5722\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 66.8075\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 56.0371\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 55.6342\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 60.6588\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 50.7240\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 51.9164\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 55.4184\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 62.8032\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 70.9862\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 66.1565\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 59.1997\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 70.1639\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 64.9851\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 14: 86.5955\n",
      "\n",
      "Epoch 14 ended\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss at step 0: 94.9096\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 77.0645\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 65.0157\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 100.5579\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 92.8775\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 76.7877\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 94.2654\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 97.9441\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 93.0150\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 92.7198\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 88.2823\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 94.2213\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 70.9170\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 70.0161\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 81.4003\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 68.5187\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 88.7928\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 89.4440\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 105.9092\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 87.3769\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 103.8949\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 113.3676\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 94.3869\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 105.1119\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 98.1143\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 94.1756\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 87.1671\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 91.6517\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 100.3902\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 99.7430\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 91.8139\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 84.3474\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 100.0835\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 111.2985\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 74.2122\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 94.0356\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 100.6472\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 86.9509\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 90.9217\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 93.7874\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 106.3010\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 85.2572\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 80.8452\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 92.2563\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 105.3515\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 89.7545\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 87.2035\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 87.7502\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 113.2971\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 91.4367\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 79.3266\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 94.3971\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 96.3587\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 83.3456\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 84.7896\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 103.0904\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 88.3170\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 83.2907\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 64.2121\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 111.7712\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 95.0307\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 73.2140\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 72.0575\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 71.2579\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 86.6850\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 58.3408\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 55.1461\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 77.2111\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 123.0709\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 118.1345\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 96.7079\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 113.2195\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 161.7429\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 97.9247\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 100.4269\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 116.9582\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 155.8307\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 97.6322\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 56.0551\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 92.4742\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 83.7011\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 72.8143\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 73.1936\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 84.2885\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 77.2590\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 78.3899\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 74.4493\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 55.0089\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 49.1339\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 65.3779\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 72.3892\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 54.9656\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 67.1689\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 49.9759\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 58.1418\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 72.1497\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 67.3703\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 65.6552\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 61.2858\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 69.0799\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 53.5365\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 43.6969\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 68.9262\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 56.2175\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 51.0606\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 73.3928\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 82.7670\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 81.1236\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 66.3641\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 74.1430\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 89.2230\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 53.5603\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 49.1255\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 56.9132\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 58.3935\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 62.1786\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 50.4817\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 45.8032\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 65.8779\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 56.0215\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 50.6048\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 61.5133\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 48.0136\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 54.8506\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 54.4011\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 57.9189\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 60.3438\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 64.6908\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 59.4649\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 64.2039\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 65.6727\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 15: 81.1137\n",
      "\n",
      "Epoch 15 ended\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss at step 0: 90.2026\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 79.9118\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 59.8719\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 86.3133\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 80.2342\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 67.7784\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 83.3307\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 95.0657\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 81.9540\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 96.1355\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 87.5497\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 106.2630\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 73.0563\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 76.3223\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 81.0272\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 78.6797\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 95.7494\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 80.5610\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 90.1662\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 80.7345\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 89.1196\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 85.1931\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 76.3027\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 93.5822\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 82.5116\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 76.3585\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 83.2125\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 87.3005\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 77.2049\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 76.0089\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 76.1474\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 79.0065\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 80.0296\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 88.2682\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 69.8900\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 75.1482\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 78.9511\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 83.2284\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 88.2542\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 87.7962\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 88.3764\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 84.3345\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 76.6199\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 80.6040\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 93.8325\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 75.4458\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 79.0500\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 85.6139\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 94.6161\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 77.1304\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 79.4864\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 77.8541\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 80.2896\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 75.0395\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 73.7342\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 88.0328\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 79.5823\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 79.5112\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 57.4710\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 92.7703\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 82.0342\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 71.7080\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 61.5647\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 61.2088\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 77.7480\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 61.3445\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 62.2065\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 76.1208\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 96.6372\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 99.3338\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 88.6704\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 100.3408\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 113.2242\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 111.8364\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 101.0108\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 86.5677\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 127.3986\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 95.7922\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 60.3395\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 84.7383\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 65.5298\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 68.3914\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 85.1196\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 84.0335\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 70.6226\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 81.8006\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 78.6601\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 57.0464\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 51.0821\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 55.3592\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 62.6861\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 50.3334\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 66.4733\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 52.6098\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 60.1791\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 63.1470\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 60.9163\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 56.7699\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 67.3058\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 73.7906\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 49.4675\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 40.0186\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 63.1146\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 59.3175\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 54.6876\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 71.7253\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 80.5546\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 71.6809\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 55.7070\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 77.4984\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 84.5497\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 53.8320\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 47.1250\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 54.8474\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 53.5496\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 64.6942\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 55.9096\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 45.6880\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 61.2505\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 52.2081\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 49.9097\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 49.0801\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 44.2368\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 45.6602\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 50.2258\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 55.1738\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 59.3348\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 61.0836\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 57.0951\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 61.1547\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 58.4353\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 16: 74.3841\n",
      "\n",
      "Epoch 16 ended\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss at step 0: 93.9821\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 74.0042\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 49.4494\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 76.2829\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 80.8179\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 58.6497\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 77.0163\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 95.5303\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 84.6674\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 91.9655\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 86.2988\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 100.8845\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 63.8617\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 63.9438\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 73.0452\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 71.5325\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 94.4961\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 76.2383\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 88.3545\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 79.8744\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 84.6003\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 87.2288\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 73.9917\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 86.6777\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 81.9158\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 72.6204\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 83.3797\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 75.3739\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 75.5022\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 71.3908\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 69.4950\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 72.7539\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 80.4441\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 83.2046\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 70.3345\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 72.0671\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 63.6893\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 70.0757\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 81.7489\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 83.2770\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 76.7447\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 72.7584\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 69.3497\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 66.2868\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 76.6523\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 68.7871\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 79.4679\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 74.1440\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 82.1950\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 74.6939\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 73.6871\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 68.3006\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 68.5604\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 65.1557\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 70.1137\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 89.8571\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 73.9213\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 72.2401\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 61.3942\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 86.8814\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 83.1664\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 65.6832\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 59.4153\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 55.6315\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 68.9678\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 52.5571\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 54.9779\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 78.3723\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 89.2860\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 96.3659\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 84.8846\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 84.8847\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 104.3816\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 81.8805\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 77.9615\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 76.0873\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 93.9333\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 76.0726\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 49.2625\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 75.9093\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 57.4953\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 61.4723\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 67.0641\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 74.6585\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 57.3709\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 59.2418\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 58.5786\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 41.7069\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 44.9822\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 51.1454\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 52.1765\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 45.6756\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 55.9196\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 45.9077\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 45.2220\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 53.5620\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 48.9750\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 44.5950\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 54.5973\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 63.6571\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 44.5114\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 37.9281\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 55.0290\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 51.1259\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 49.4474\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 64.9773\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 73.9030\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 69.3211\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 52.3835\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 73.2126\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 79.8419\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 50.8469\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 48.4434\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 51.4064\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 51.7468\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 55.5824\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 48.8223\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 44.9319\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 57.3537\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 46.1541\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 45.0477\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 50.9961\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 42.9503\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 45.1468\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 48.3973\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 53.7582\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 57.4498\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 62.6083\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 51.1390\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 58.6830\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 58.7028\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 17: 67.6900\n",
      "\n",
      "Epoch 17 ended\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss at step 0: 81.7056\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 69.8874\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 51.5161\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 77.6813\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 65.6562\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 60.5085\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 90.0905\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 86.9672\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 76.8307\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 93.9523\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 94.0368\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 102.8093\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 66.4564\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 64.9176\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 69.8821\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 64.9152\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 87.0070\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 71.0001\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 75.9634\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 71.9695\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 82.1596\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 81.8558\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 71.0839\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 82.1454\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 72.0537\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 65.0667\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 74.1130\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 75.4162\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 71.9212\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 71.6427\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 66.0930\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 68.3612\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 69.2021\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 75.4667\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 62.7872\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 69.2814\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 65.0563\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 68.0176\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 71.5308\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 75.7472\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 71.7725\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 68.8429\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 69.3113\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 64.9285\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 65.7781\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 62.9273\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 76.8004\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 70.3872\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 75.1089\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 72.2051\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 72.2111\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 61.9598\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 64.6278\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 57.0345\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 64.0878\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 75.0362\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 59.8104\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 63.3806\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 54.6269\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 85.3008\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 66.5956\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 54.8793\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 53.0590\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 46.4878\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 63.9430\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 44.1726\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 47.5884\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 61.5649\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 78.1381\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 78.3127\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 73.2291\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 91.1544\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 90.1031\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 85.0080\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 81.2435\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 75.8206\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 90.4789\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 69.3491\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 52.9125\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 70.8446\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 51.7972\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 50.9443\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 69.2766\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 75.8977\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 57.4096\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 55.4192\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 49.8320\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 43.0161\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 43.3839\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 49.9665\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 45.5835\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 40.3172\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 51.2436\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 44.4617\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 46.6550\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 52.6156\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 47.6903\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 50.3311\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 54.2452\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 61.3913\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 44.2225\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 38.6184\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 55.7753\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 49.2568\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 51.0630\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 68.0148\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 62.3309\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 61.8729\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 50.7039\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 63.1090\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 77.6718\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 45.7467\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 43.2081\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 51.3029\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 49.3990\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 49.6348\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 44.5238\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 39.4691\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 59.3299\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 44.7584\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 43.4542\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 46.2043\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 42.5218\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 43.4992\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 48.1960\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 48.4249\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 52.4941\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 54.3847\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 47.9491\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 54.0522\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 52.5476\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 18: 63.5038\n",
      "\n",
      "Epoch 18 ended\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss at step 0: 70.6614\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 64.1693\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 47.1543\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 64.0374\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 65.1690\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 50.9683\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 69.6502\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 73.3385\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 67.8639\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 90.0533\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 79.3949\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 87.8474\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 63.3714\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 64.1016\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 65.3270\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 65.7292\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 83.1203\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 71.8415\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 83.4979\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 72.5795\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 79.6488\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 76.7016\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 70.0943\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 79.3926\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 71.7623\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 67.4040\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 80.0345\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 68.3532\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 68.5412\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 73.7068\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 62.0695\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 63.1481\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 73.1839\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 81.1464\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 65.6086\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 74.0420\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 68.4629\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 60.0645\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 76.1971\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 75.9435\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 67.5353\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 74.7433\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 79.0694\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 62.1775\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 69.9589\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 65.6608\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 73.8458\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 66.8994\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 68.5005\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 69.5275\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 63.9762\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 62.8293\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 64.7389\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 57.6190\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 63.7824\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 73.4654\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 61.0983\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 55.7104\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 54.6506\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 73.3863\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 66.1248\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 52.4224\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 49.7226\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 44.9520\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 60.5987\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 43.9985\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 46.7019\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 59.4870\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 70.6429\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 73.3787\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 64.1154\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 75.1520\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 79.7177\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 73.7238\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 72.9310\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 69.1139\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 81.6472\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 60.7496\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 44.6674\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 70.2437\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 50.7372\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 52.5461\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 61.4018\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 65.5306\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 51.1794\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 50.6479\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 51.0791\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 41.5059\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 40.2026\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 45.3774\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 47.2010\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 40.1486\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 43.8885\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 37.4343\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 43.2104\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 50.9683\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 46.1160\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 45.7782\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 51.5648\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 54.0190\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 38.1949\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 37.2512\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 56.9632\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 45.7403\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 40.8943\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 64.0882\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 65.4573\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 59.3268\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 43.2652\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 59.9597\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 71.3526\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 41.0611\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 39.7729\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 45.6614\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 47.0802\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 49.9713\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 44.0513\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 40.0617\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 57.6723\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 42.7167\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 45.4856\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 47.8622\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 41.2696\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 39.0923\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 41.6337\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 45.4911\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 51.9158\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 49.4926\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 46.0182\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 48.1718\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 50.6496\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 19: 60.2505\n",
      "\n",
      "Epoch 19 ended\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss at step 0: 72.9702\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 57.1142\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 42.1667\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 62.5888\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 60.4257\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 49.0091\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 58.8749\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 67.6769\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 65.5399\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 79.7146\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 87.7163\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 98.4077\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 66.0340\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 59.1588\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 64.7646\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 68.7183\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 82.6856\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 74.5761\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 82.3428\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 74.2751\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 74.5705\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 69.8782\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 62.8909\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 75.4411\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 67.3103\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 60.8050\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 74.8109\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 72.2138\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 65.7634\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 66.2676\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 60.8305\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 58.4209\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 61.5545\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 75.9857\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 61.5987\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 64.3353\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 67.4642\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 56.6995\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 72.6150\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 74.4529\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 63.8018\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 64.1070\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 81.6178\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 68.5918\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 61.2231\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 66.5312\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 90.5415\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 69.1296\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 75.1108\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 83.1313\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 80.3513\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 63.1554\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 77.5390\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 73.0184\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 67.4752\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 81.6840\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 79.5104\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 61.2058\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 53.8458\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 94.0176\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 81.9226\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 65.5722\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 75.1908\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 74.9006\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 77.9821\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 61.5044\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 66.9102\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 84.6665\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 87.1203\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 96.8722\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 74.2536\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 106.5408\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 105.9898\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 88.7231\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 93.3459\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 85.9700\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 97.6383\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 70.3818\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 59.9881\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 93.2846\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 57.4289\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 55.2716\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 77.9287\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 71.5693\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 57.2279\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 56.6027\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 50.4911\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 46.7734\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 44.4110\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 46.1325\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 44.9074\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 39.7863\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 46.5709\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 39.0121\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 41.4333\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 50.5849\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 48.0459\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 44.6457\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 48.7747\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 50.3949\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 40.5183\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 33.7411\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 52.0050\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 50.8876\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 47.1427\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 64.4951\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 65.1215\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 65.7188\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 47.8038\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 58.4626\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 74.6098\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 46.9030\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 42.2957\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 50.9903\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 49.9216\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 51.2102\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 46.2535\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 42.8237\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 52.8656\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 39.6008\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 45.3225\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 51.9987\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 40.1599\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 43.0524\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 46.9619\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 52.8890\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 54.7334\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 51.3692\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 55.2103\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 55.5450\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 48.5908\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 20: 64.2001\n",
      "\n",
      "Epoch 20 ended\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 100\n",
    "training_loss_values = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nStart of epoch {epoch}\")\n",
    "\n",
    "    # Initialize variables to accumulate loss\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step in range(0, len(result), batch_size):\n",
    "        x_batch_train = result[step:step+batch_size, :, :]\n",
    "        y_batch_train = result[step:step+batch_size, :, 64]\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Run the forward pass of the layer.\n",
    "            logits = transformer(x_batch_train, training=True)  # Logits for this minibatch\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, transformer.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, transformer.trainable_weights))\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss_value\n",
    "        num_batches += 1\n",
    "\n",
    "        # Log every 100 batches.\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Training loss at step {step}: {float(loss_value):.4f}\"\n",
    "            )\n",
    "            print(f\"Seen so far: {step} samples\")\n",
    "\n",
    "    # Calculate and print the average loss for the epoch\n",
    "    avg_loss = total_loss / num_batches\n",
    "    training_loss_values.append(avg_loss)\n",
    "    print(f\"Average training loss for epoch {epoch + 1}: {float(avg_loss):.4f}\")\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1} ended\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training loss\n",
    "plt.plot(range(1, epochs + 1), training_loss_values, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('Average Loss vs Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 757ms/step\n",
      "Shape of Predictions: (100, 30)\n"
     ]
    }
   ],
   "source": [
    "predictions = transformer.predict(result[:100, :, :], batch_size=batch_size)\n",
    "\n",
    "# 'predictions' will contain the model's predictions for the test_data\n",
    "print(\"Shape of Predictions:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.       185.88185 4379.4717   650.7443     0.         0.\n",
      "    0.         0.         0.         0.         0.         0.\n",
      "    0.         0.         0.         0.         0.         0.\n",
      "    0.         0.         0.         0.         0.         0.\n",
      "    0.         0.         0.         0.         0.         0.     ]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   5.  173. 4409.  656.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.]\n"
     ]
    }
   ],
   "source": [
    "print(result[1,:,64])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
