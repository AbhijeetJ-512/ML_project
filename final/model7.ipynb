{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.load(\"../data/data_64_30.npy\")\n",
    "X_data = np.transpose(X_data, (0, 2, 1))\n",
    "print(X_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"../data/LJSpeech-1.1/metadata.csv\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"ID\", \"Text1\", \"Text2\"],\n",
    ")\n",
    "texts = data[\"Text1\"].to_list()\n",
    "ID = data[\"ID\"].to_list()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "num_classes = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "Y_data = pad_sequences(sequences, padding=\"post\", maxlen=30)\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer(tf.keras.layers.Layer):  # Change the base class to tf.keras.layers.Layer\n",
    "    def __init__(self, input_shape, filters=32, kernel_size=3, **kwargs):\n",
    "        super(ConvolutionalLayer, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Extract the number of filters from the input shape\n",
    "        if isinstance(input_shape, tuple):\n",
    "            self.filters = input_shape[-1]\n",
    "\n",
    "        self.conv1 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\")\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "\n",
    "        self.conv2 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\")\n",
    "        self.batch_norm2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.ReLU()\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        conv1_out = self.relu1(self.batch_norm1(self.conv1(inputs), training=training))\n",
    "        conv2_out = self.relu2(self.batch_norm2(self.conv2(conv1_out), training=training))\n",
    "        # print(\"CNN output shape is \",conv2_out.shape)\n",
    "        return conv2_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn_layer = ConvolutionalLayer(input_shape=(30, 64))  # Adjust input shape\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        cnn_output = self.cnn_layer(inputs, training=training)  # Explicitly call the 'call' method\n",
    "        return cnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask):\n",
    "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_qk = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_qk += mask\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_qk)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = tf.keras.layers.Dense(hidden)\n",
    "        self.linear2 = tf.keras.layers.Dense(d_model)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostionalEmbedding(tf.keras.Model):\n",
    "    def __init__(self,vocab_size=num_classes,embedding_dim=64):\n",
    "        super(PostionalEmbedding,self).__init__()\n",
    "        self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=30)\n",
    "    \n",
    "    def call(self,input,training=None):\n",
    "        if input.shape == (100,30,64):\n",
    "            return input\n",
    "        else:\n",
    "            output = self.embedding(input)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = tf.keras.layers.Dense(3 * d_model, use_bias=False)\n",
    "        self.linear_layer = tf.keras.layers.Dense(d_model, activation='relu')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        if len(x.shape) == 2:\n",
    "            x = tf.expand_dims(tf.expand_dims(x, axis=0), axis=1)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        batch_size, _, _ = x.shape\n",
    "\n",
    "        qkv = self.qkv_layer(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
    "        values = tf.reshape(values, (batch_size, -1, self.num_heads * self.head_dim))\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticrossHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MulticrossHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = tf.keras.layers.Dense(3 * d_model, use_bias=False)\n",
    "        self.linear_layer = tf.keras.layers.Dense(d_model, activation='relu')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        if len(x.shape) == 2:\n",
    "            x = tf.expand_dims(tf.expand_dims(x, axis=0), axis=1)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x,encoder_output,  mask):\n",
    "        batch_size, _, _ = x.shape\n",
    "\n",
    "        qkv = self.qkv_layer(x)\n",
    "        q, k, _ = tf.split(qkv, 3, axis=-1)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "\n",
    "        qkv = self.qkv_layer(encoder_output)\n",
    "        _, _, v = tf.split(qkv, 3, axis=-1)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
    "        values = tf.reshape(values, (batch_size, -1, self.num_heads * self.head_dim))\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, hidden, dropout_rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.cnn_layer = CNN()\n",
    "        self.multihead_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feedforward = PositionwiseFeedForward(d_model, hidden, drop_prob=dropout_rate)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "\n",
    "    def call(self, x, training=None, mask=None):\n",
    "        x = self.cnn_layer(x)\n",
    "        x_att = self.multihead_attention(x, mask)\n",
    "        x = x + x_att\n",
    "        x = self.dropout(x)\n",
    "        x = self.feedforward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, hidden, dropout_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden = hidden\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Define layers\n",
    "        self.encoder_layers = [EncoderLayer(d_model, num_heads, hidden, dropout_rate) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, x, training=None, mask=None):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.encoder_layers[i](x, training=training, mask=mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, vocab_size, hidden, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.embedding = PostionalEmbedding(vocab_size=vocab_size, embedding_dim=64)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attention = MulticrossHeadAttention(d_model, num_heads)\n",
    "        self.feedforward = PositionwiseFeedForward(d_model, hidden, drop_prob=dropout_rate)\n",
    "\n",
    "    def call(self, inputs, encoder_output, training=None, mask=None):\n",
    "        # Self-attention on the decoder side\n",
    "        x = self.embedding(inputs)\n",
    "        x_att_self = self.self_attention(x, mask)\n",
    "        x = x + x_att_self\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Cross-attention with the encoder output\n",
    "        x_att_cross = self.cross_attention(x, encoder_output, mask)\n",
    "        x = x + x_att_cross\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Feedforward layer\n",
    "        x = self.feedforward(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, num_layers, d_model, num_heads, hidden, dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden = hidden\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.flatten = layers.Flatten()\n",
    "        # self.output_layer = layers.Dense(Y_data.shape[-1], activation='softmax')\n",
    "        self.output_layer = layers.Dense(30, activation='softmax')\n",
    "\n",
    "\n",
    "        # Define layers\n",
    "        self.decoder_layers = [DecoderLayer(d_model, num_heads, vocab_size, hidden, dropout_rate) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, x,encoder_output, training=None, mask=None):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.decoder_layers[i](x,encoder_output, training=training, mask=mask)\n",
    "        x = self.flatten(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self ,d_model,num_heads,hidden,num_layers,voacb_size,dropout_rate=0.1):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden = hidden\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.encoder = Encoder(d_model=d_model,num_heads=num_heads,hidden=hidden,num_layers=num_layers)\n",
    "        self.decoder = Decoder(d_model=d_model,num_heads=num_heads,num_layers=num_layers,hidden=hidden,dropout_rate=dropout_rate,vocab_size=voacb_size)\n",
    "\n",
    "    def call(self,x,training=None,mask=None):\n",
    "        encoder_output = self.encoder(x[:,:,:64],training,mask)\n",
    "        decoder_output = self.decoder(x[:,:,64],encoder_output,training,mask)\n",
    "        return decoder_output\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64\n",
    "num_heads = 8\n",
    "hidden = 2048\n",
    "num_layer = 1                                                                                                                                                                                                                                                                   \n",
    "vocab_size = num_classes\n",
    "dropout_rate =0.1\n",
    "transformer = Transformer(d_model=d_model,num_heads=num_heads,hidden=hidden,num_layers=num_layer,voacb_size=vocab_size,dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = keras.metrics.MeanSquaredError()\n",
    "val_acc_metric = keras.metrics.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data_copy = Y_data\n",
    "Y_data_copy = Y_data_copy[:,:,np.newaxis]\n",
    "result = np.concatenate((X_data, Y_data_copy), axis=-1)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.build(input_shape=(100,30,65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nStart of epoch {epoch}\")\n",
    "\n",
    "    # Shuffle the data\n",
    "    np.random.shuffle(result)\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step in range(0, len(result), batch_size):\n",
    "        x_batch_train = result[step:step+batch_size, :, :64]\n",
    "        y_batch_train = result[step:step+batch_size, :, 64]\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Run the forward pass of the layer.\n",
    "            logits = transformer(x_batch_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, transformer.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, transformer.trainable_weights))\n",
    "\n",
    "        # Log every 100 batches.\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Training loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"\n",
    "            )\n",
    "            print(f\"Seen so far: {step} samples\")\n",
    "\n",
    "# Optionally, you can evaluate the model on a validation set after each epoch if you have one.\n",
    "# val_loss = ...\n",
    "# print(f\"Validation loss: {val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
