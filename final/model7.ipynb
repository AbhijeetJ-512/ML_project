{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 14:20:31.714353: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-15 14:20:31.745867: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-15 14:20:31.745904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-15 14:20:31.746602: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-15 14:20:31.752087: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-15 14:20:32.324231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13100, 30, 64)\n"
     ]
    }
   ],
   "source": [
    "X_data = np.load(\"../data/data_64_30.npy\")\n",
    "X_data = np.transpose(X_data, (0, 2, 1))\n",
    "print(X_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13100, 30)\n",
      "14518\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    \"../data/LJSpeech-1.1/metadata.csv\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"ID\", \"Text1\", \"Text2\"],\n",
    ")\n",
    "texts = data[\"Text1\"].to_list()\n",
    "ID = data[\"ID\"].to_list()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "num_classes = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "Y_data = pad_sequences(sequences, padding=\"post\", maxlen=30)\n",
    "print(Y_data.shape)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13100, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_data = np.load(\"../data/padded_y_data_6546.npy\")\n",
    "Y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer(tf.keras.layers.Layer):  # Change the base class to tf.keras.layers.Layer\n",
    "    def __init__(self, input_shape, filters=32, kernel_size=3, **kwargs):\n",
    "        super(ConvolutionalLayer, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Extract the number of filters from the input shape\n",
    "        if isinstance(input_shape, tuple):\n",
    "            self.filters = input_shape[-1]\n",
    "\n",
    "        self.conv1 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\")\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "\n",
    "        self.conv2 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\")\n",
    "        self.batch_norm2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.ReLU()\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        conv1_out = self.relu1(self.batch_norm1(self.conv1(inputs), training=training))\n",
    "        conv2_out = self.relu2(self.batch_norm2(self.conv2(conv1_out), training=training))\n",
    "        # print(\"CNN output shape is \",conv2_out.shape)\n",
    "        return conv2_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn_layer = ConvolutionalLayer(input_shape=(30, 64))  # Adjust input shape\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        cnn_output = self.cnn_layer(inputs, training=training)  # Explicitly call the 'call' method\n",
    "        return cnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask):\n",
    "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_qk = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_qk += mask\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_qk)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = tf.keras.layers.Dense(hidden)\n",
    "        self.linear2 = tf.keras.layers.Dense(d_model)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostionalEmbedding(tf.keras.Model):\n",
    "    def __init__(self,vocab_size=num_classes,embedding_dim=64):\n",
    "        super(PostionalEmbedding,self).__init__()\n",
    "        self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=30)\n",
    "    \n",
    "    def call(self,input,training=None):\n",
    "        if input.shape == (100,30,64):\n",
    "            return input\n",
    "        else:\n",
    "            output = self.embedding(input)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = tf.keras.layers.Dense(3 * d_model, use_bias=False)\n",
    "        self.linear_layer = tf.keras.layers.Dense(d_model, activation='relu')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        if len(x.shape) == 2:\n",
    "            x = tf.expand_dims(tf.expand_dims(x, axis=0), axis=1)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        batch_size, _, _ = x.shape\n",
    "\n",
    "        qkv = self.qkv_layer(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
    "        values = tf.reshape(values, (batch_size, -1, self.num_heads * self.head_dim))\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticrossHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MulticrossHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = tf.keras.layers.Dense(3 * d_model, use_bias=False)\n",
    "        self.linear_layer = tf.keras.layers.Dense(d_model, activation='relu')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        if len(x.shape) == 2:\n",
    "            x = tf.expand_dims(tf.expand_dims(x, axis=0), axis=1)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x,encoder_output,  mask):\n",
    "        batch_size, _, _ = x.shape\n",
    "\n",
    "        qkv = self.qkv_layer(x)\n",
    "        q, k, _ = tf.split(qkv, 3, axis=-1)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "\n",
    "        qkv = self.qkv_layer(encoder_output)\n",
    "        _, _, v = tf.split(qkv, 3, axis=-1)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
    "        values = tf.reshape(values, (batch_size, -1, self.num_heads * self.head_dim))\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, hidden, dropout_rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.cnn_layer = CNN()\n",
    "        self.multihead_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feedforward = PositionwiseFeedForward(d_model, hidden, drop_prob=dropout_rate)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "\n",
    "    def call(self, x, training=None, mask=None):\n",
    "        x = self.cnn_layer(x)\n",
    "        x_att = self.multihead_attention(x, mask)\n",
    "        x = x + x_att\n",
    "        x = self.dropout(x)\n",
    "        x = self.feedforward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, hidden, dropout_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden = hidden\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # self.conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')\n",
    "\n",
    "        # Define layers\n",
    "        self.encoder_layers = [EncoderLayer(d_model, num_heads, hidden, dropout_rate) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, x, training=None, mask=None):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.encoder_layers[i](x, training=training, mask=mask)\n",
    "        # print(x.shape)\n",
    "        # x = self.conv_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, vocab_size, hidden, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.embedding = PostionalEmbedding(vocab_size=vocab_size, embedding_dim=64)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attention = MulticrossHeadAttention(d_model, num_heads)\n",
    "        self.feedforward = PositionwiseFeedForward(d_model, hidden, drop_prob=dropout_rate)\n",
    "\n",
    "    def call(self, inputs, encoder_output, training=None, mask=None):\n",
    "        # Self-attention on the decoder side\n",
    "        x = self.embedding(inputs)\n",
    "        x_att_self = self.self_attention(x, mask)\n",
    "        x = x + x_att_self\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Cross-attention with the encoder output\n",
    "        x_att_cross = self.cross_attention(x, encoder_output, mask)\n",
    "        x = x + x_att_cross\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Feedforward layer\n",
    "        x = self.feedforward(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, num_layers, d_model, num_heads, hidden, dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden = hidden\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.flatten = layers.Flatten()\n",
    "        # self.output_layer = layers.Dense(Y_data.shape[-1], activation='softmax')\n",
    "        self.output_layer = layers.Dense(30, activation='relu')\n",
    "        # self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "        # Modify the output layer in the Decoder class\n",
    "        # self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "\n",
    "\n",
    "        # Define layers\n",
    "        self.decoder_layers = [DecoderLayer(d_model, num_heads, vocab_size, hidden, dropout_rate) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, x,encoder_output, training=None, mask=None):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.decoder_layers[i](x,encoder_output, training=training, mask=mask)\n",
    "        x = self.flatten(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self ,d_model,num_heads,hidden,num_layers,voacb_size,dropout_rate=0.1):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden = hidden\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.encoder = Encoder(d_model=d_model,num_heads=num_heads,hidden=hidden,num_layers=num_layers)\n",
    "        self.decoder = Decoder(d_model=d_model,num_heads=num_heads,num_layers=num_layers,hidden=hidden,dropout_rate=dropout_rate,vocab_size=voacb_size)\n",
    "\n",
    "    def call(self,x,training=None,mask=None):\n",
    "        encoder_output = self.encoder(x[:,:,:64],training,mask)\n",
    "        decoder_output = self.decoder(x[:,:,64],encoder_output,training,mask)\n",
    "        return decoder_output\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 14:20:34.717864: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 14:20:34.741979: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 14:20:34.742157: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 14:20:34.743183: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 14:20:34.743335: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 14:20:34.743420: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 14:20:34.784759: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 14:20:34.784897: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 14:20:34.785004: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-15 14:20:34.785104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2796 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "d_model = 64\n",
    "num_heads = 8\n",
    "hidden = 2048\n",
    "num_layer = 1                                                                                                                                                                                                                                                              \n",
    "vocab_size = num_classes\n",
    "dropout_rate =0.1\n",
    "transformer = Transformer(d_model=d_model,num_heads=num_heads,hidden=hidden,num_layers=num_layer,voacb_size=vocab_size,dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "# loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "# loss_fn = keras.losses.MeanSquaredError()\n",
    "loss_fn = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "# Prepare the metrics. 672.3503\n",
    "train_acc_metric = keras.metrics.MeanSquaredError()\n",
    "val_acc_metric = keras.metrics.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13100, 30, 65)\n"
     ]
    }
   ],
   "source": [
    "output_array_shifted = np.roll(Y_data, shift=1, axis=1)\n",
    "output_array_shifted = output_array_shifted[:,:,np.newaxis]\n",
    "result = np.concatenate((X_data, output_array_shifted), axis=-1)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer.build(input_shape=(100,30,65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "Training loss at step 0: 7.0004\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 8.6205\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 13.0166\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 16.8441\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 23.7173\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 27.2400\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 23.6772\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 21.0977\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 23.1778\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 25.4248\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 23.0660\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 25.8496\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 23.2840\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 27.5746\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 27.5266\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 26.8708\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 25.5664\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 22.7484\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 21.5400\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 18.3998\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 19.1465\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 18.3880\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 19.3078\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 19.2444\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 19.6164\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 20.0932\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 21.5307\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 21.9416\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 21.4907\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 20.8820\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 26.4886\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 24.9873\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 21.3599\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 24.9003\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 20.4936\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 20.8840\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 17.2499\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 19.0938\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 22.1214\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 17.4344\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 20.9219\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 20.3512\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 15.7387\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 19.6372\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 18.7245\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 17.3906\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 18.6004\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 16.7593\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 18.1623\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 18.5367\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 17.5241\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 20.0612\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 16.8915\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 17.5908\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 18.8410\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 16.2795\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 17.7852\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 18.3874\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 14.8573\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 19.3408\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 16.8516\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 17.7975\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 14.4429\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 15.1874\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 13.8432\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 13.4673\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 14.4938\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 15.7991\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 17.6052\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 18.4170\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 18.7965\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 18.3224\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 16.4764\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 15.7017\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 17.3099\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 20.3058\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 15.5650\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 21.5726\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 25.2439\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 25.9266\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 23.9826\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 23.6182\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 25.3468\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 22.7278\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 22.9994\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 24.6108\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 24.4852\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 23.7455\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 29.5421\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 23.5847\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 22.9575\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 21.1413\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 22.8213\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 21.6720\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 23.7076\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 26.4734\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 26.7432\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 24.0167\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 24.5608\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 22.5529\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 25.6611\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 23.5963\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 23.2110\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 23.1963\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 23.7602\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 22.4946\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 26.5658\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 28.6291\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 22.7217\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 27.0087\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 24.7173\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 22.3445\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 24.9008\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 27.2516\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 25.5136\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 34.0119\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 26.6442\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 26.2723\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 27.7678\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 19.9991\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 30.8084\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 29.3691\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 25.9825\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 29.3424\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 28.2090\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 22.2417\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 32.1752\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 25.3763\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 21.5253\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 31.9112\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 22.3272\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 1: 21.7651\n",
      "\n",
      "Epoch 1 ended\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss at step 0: 5.9825\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 7.5995\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 11.7558\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 12.9069\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 19.1214\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 23.5456\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 20.3781\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 19.8388\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 19.5084\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 21.8807\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 20.4932\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 26.1119\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 21.1336\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 22.6594\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 23.3391\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 22.5830\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 23.8310\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 21.3628\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 20.0275\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 18.1652\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 18.3759\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 16.0157\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 15.7781\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 16.2640\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 18.1245\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 17.5885\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 19.7507\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 19.8096\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 19.7217\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 19.4380\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 24.2458\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 24.4888\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 33.7374\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 29.1905\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 17.6275\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 20.8244\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 18.6854\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 16.9127\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 17.5844\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 15.6680\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 18.0312\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 16.8132\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 15.0269\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 20.9910\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 16.5252\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 15.9551\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 17.4598\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 14.6925\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 15.5334\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 17.7567\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 16.1166\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 16.8630\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 16.2477\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 16.6468\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 15.2893\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 15.5855\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 16.0302\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 15.5221\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 14.5846\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 18.9770\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 14.6875\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 14.9208\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 14.4045\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 13.7609\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 13.3388\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 13.5196\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 13.9512\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 16.7479\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 16.7393\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 17.2370\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 17.8187\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 16.4960\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 15.0078\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 14.8500\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 15.7852\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 20.0376\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 14.1672\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 18.4312\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 23.6905\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 22.3437\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 23.0382\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 22.9279\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 24.4279\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 24.0731\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 20.6527\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 24.7582\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 22.9167\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 25.4161\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 28.7287\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 22.9308\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 23.6327\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 21.9825\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 25.7793\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 21.6363\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 21.9265\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 26.6867\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 26.1968\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 24.6279\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 24.8194\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 22.8350\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 25.0444\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 22.0638\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 21.9324\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 21.7059\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 20.9402\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 20.5314\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 22.3921\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 22.1288\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 24.1432\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 21.3672\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 20.6642\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 21.7077\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 21.5499\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 24.5275\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 24.9544\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 28.4648\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 22.6034\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 23.1917\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 21.5729\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 16.9074\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 25.2376\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 23.4351\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 25.4252\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 23.6202\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 23.3852\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 20.6382\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 24.0108\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 20.5982\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 19.2847\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 21.2195\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 17.9047\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 2: 19.9526\n",
      "\n",
      "Epoch 2 ended\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss at step 0: 5.7793\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 6.5819\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 8.7399\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 10.0576\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 12.6606\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 16.3175\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 13.9064\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 13.4076\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 14.0002\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 16.4310\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 14.7457\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 19.2364\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 14.9476\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 16.9983\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 16.7319\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 16.3291\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 16.4563\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 14.9882\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 16.0549\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 13.4933\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 14.6863\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 12.5933\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 12.6155\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 13.5933\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 14.9352\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 15.6506\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 16.6493\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 16.8725\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 16.7807\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 16.1896\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 18.7549\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 18.3695\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 17.5804\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 18.4704\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 15.1110\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 17.0429\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 15.0939\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 14.8228\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 16.2764\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 15.1056\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 14.7629\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 12.9636\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 13.3316\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 17.6190\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 14.6131\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 13.3309\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 15.1774\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 12.7569\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 14.3811\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 16.0073\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 13.6001\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 14.9166\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 15.3589\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 15.2415\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 14.1130\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 13.6147\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 13.9730\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 15.5402\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 12.4996\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 18.0876\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 13.7766\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 13.8170\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 13.9265\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 13.5032\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 12.6768\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 12.2314\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 12.7417\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 15.4750\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 16.5400\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 17.6966\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 18.4731\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 16.2465\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 16.3088\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 15.5533\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 16.3533\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 18.9644\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 15.2095\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 18.1974\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 23.5147\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 20.8339\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 21.5740\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 21.5233\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 22.4667\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 22.5576\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 20.3305\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 23.1575\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 23.1320\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 24.7577\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 30.0665\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 24.5830\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 22.3771\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 22.0436\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 23.6927\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 24.4107\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 22.6422\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 27.8062\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 26.9051\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 24.8884\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 26.0541\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 23.3108\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 24.5456\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 22.4872\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 22.3618\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 20.6289\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 19.8699\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 21.7260\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 22.0623\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 24.3006\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 23.1888\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 22.1372\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 23.6871\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 21.5759\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 23.0487\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 25.5246\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 24.1347\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 30.5186\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 23.1333\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 24.8339\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 24.1164\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 17.7907\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 28.0752\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 24.4226\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 26.8662\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 26.1716\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 22.7008\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 22.8008\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 23.7882\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 19.4050\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 19.2721\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 19.1002\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 18.2907\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 3: 18.3193\n",
      "\n",
      "Epoch 3 ended\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss at step 0: 6.4233\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 6.3051\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 8.0272\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 8.8975\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 10.6933\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 13.8807\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 12.6502\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 12.3305\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 12.6573\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 15.4393\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 14.1604\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 17.5858\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 13.3336\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 15.1013\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 18.0932\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 14.8507\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 14.9146\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 14.2185\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 14.5346\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 12.7670\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 13.1267\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 12.2641\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 12.9223\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 12.7772\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 13.4511\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 14.3196\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 15.3616\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 15.2462\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 15.7225\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 15.2963\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 17.3224\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 17.8925\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 16.2582\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 16.5398\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 13.3540\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 15.7452\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 13.6800\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 14.2413\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 16.0255\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 14.5451\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 13.7288\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 13.7617\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 11.4213\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 16.3161\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 13.1993\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 12.5293\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 14.5976\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 13.2625\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 13.7869\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 15.0313\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 13.4437\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 15.4668\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 14.0582\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 13.8811\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 14.2462\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 13.1105\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 13.4766\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 14.4376\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 11.1249\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 17.6189\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 14.0383\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 14.9719\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 13.1092\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 13.1561\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 13.0121\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 12.2690\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 13.0367\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 14.3983\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 16.3073\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 15.9984\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 16.7558\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 17.0694\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 14.2669\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 14.8712\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 16.9896\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 18.2133\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 14.7558\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 19.4837\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 23.4489\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 22.5676\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 22.3253\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 20.5357\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 22.9942\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 20.6384\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 19.5810\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 21.8172\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 22.9052\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 21.2268\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 27.7843\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 21.4097\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 20.0077\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 22.4599\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 21.8717\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 21.6921\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 23.9048\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 25.5604\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 29.5179\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 24.5297\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 29.8457\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 23.4566\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 27.0068\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 22.7121\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 22.0780\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 23.9284\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 20.4687\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 22.9567\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 21.8305\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 22.0501\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 25.3639\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 19.5445\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 23.7699\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 25.7645\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 19.0343\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 28.3695\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 26.4105\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 27.4262\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 25.6152\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 21.8355\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 21.6542\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 19.7051\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 24.1565\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 26.3423\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 26.6418\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 24.9645\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 27.0129\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 18.7221\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 26.8734\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 24.0217\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 19.3761\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 24.4675\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 18.7155\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 4: 17.8859\n",
      "\n",
      "Epoch 4 ended\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss at step 0: 6.1267\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 6.6112\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 8.5597\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 9.5003\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 11.8410\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 15.3236\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 14.3659\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 13.8372\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 13.9565\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 16.1385\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 15.7424\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 19.0092\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 15.8208\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 16.6867\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 18.6933\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 17.4054\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 16.7140\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 16.1545\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 15.7882\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 13.7513\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 14.4703\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 13.4052\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 13.5551\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 14.7534\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 15.4238\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 15.7053\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 16.0983\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 16.0457\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 15.8367\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 15.8269\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 18.8044\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 21.2671\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 17.4818\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 18.8815\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 14.9720\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 16.8903\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 14.6075\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 14.4179\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 16.3470\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 16.1884\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 16.1289\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 14.9796\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 13.3924\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 17.9002\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 14.7265\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 14.7736\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 16.9242\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 14.0419\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 15.7181\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 16.7445\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 13.8429\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 15.6027\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 14.9934\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 13.6267\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 14.9909\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 14.0069\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 14.0359\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 15.2881\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 12.7048\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 17.0044\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 13.3753\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 15.7858\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 13.4150\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 13.4279\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 13.3579\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 12.1538\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 12.0886\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 14.6130\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 16.6780\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 17.1333\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 17.7473\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 16.0250\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 15.5423\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 15.6278\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 16.2734\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 18.6158\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 14.5468\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 18.5798\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 23.2847\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 21.0007\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 24.7885\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 22.9065\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 27.4367\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 22.3042\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 21.2091\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 22.1951\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 22.9235\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 22.7997\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 25.6016\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 24.2657\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 20.4190\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 22.6852\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 22.3940\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 22.7514\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 21.3688\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 27.6043\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 25.9917\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 30.2741\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 27.2728\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 24.4809\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 31.4438\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 22.0846\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 28.3179\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 21.8023\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 20.9121\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 21.7665\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 19.5641\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 21.7045\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 21.7419\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 19.1386\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 19.5250\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 20.0114\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 19.4522\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 22.5502\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 23.6090\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 25.7776\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 21.5781\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 22.9893\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 17.7526\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 19.1638\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 24.1852\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 24.4512\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 28.5452\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 22.2861\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 25.9621\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 18.7196\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 24.8204\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 25.4641\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 17.4014\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 23.3823\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 20.9269\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 5: 18.3390\n",
      "\n",
      "Epoch 5 ended\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss at step 0: 5.7458\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 6.8867\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 9.0592\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 9.2466\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 12.4931\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 16.9610\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 16.4067\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 15.5801\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 14.5241\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 17.7768\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 17.3464\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 20.7979\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 18.1807\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 20.5578\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 20.6131\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 19.7262\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 19.6994\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 18.1053\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 16.3335\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 15.2155\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 16.1589\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 13.3869\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 13.4875\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 13.5377\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 16.0533\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 16.5045\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 18.1999\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 17.2080\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 16.6821\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 17.5629\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 21.4302\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 20.7304\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 18.5517\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 19.9832\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 16.4283\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 16.5952\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 15.0775\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 14.5977\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 15.9544\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 15.1807\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 14.4538\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 14.6016\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 14.3074\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 17.8499\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 15.5033\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 14.4658\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 15.6183\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 13.3110\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 15.0540\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 16.7108\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 14.0567\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 17.1260\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 15.2927\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 14.8729\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 15.2693\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 14.4551\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 14.7211\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 15.8346\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 12.5046\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 18.1134\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 15.2341\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 14.7031\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 14.0283\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 13.0325\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 13.6847\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 13.5161\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 13.3985\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 14.3966\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 16.4136\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 18.1602\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 19.1909\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 17.4285\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 15.7655\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 15.5750\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 16.8693\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 21.4539\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 14.5532\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 18.5486\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 21.0327\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 20.6712\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 21.4874\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 21.3904\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 22.2024\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 23.0431\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 21.3024\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 23.8547\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 30.5592\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 22.7517\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 34.3772\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 25.5289\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 27.2156\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 30.9110\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 21.6332\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 31.6116\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 26.0452\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 27.8434\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 28.0875\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 24.8672\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 26.7933\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 22.9423\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 24.1219\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 21.2162\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 22.1979\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 21.1301\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 20.1668\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 19.7171\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 21.5877\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 20.7731\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 22.3639\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 20.0635\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 18.0007\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 19.5065\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 17.5049\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 20.0582\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 23.4593\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 24.9444\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 21.5216\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 20.4878\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 18.6112\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 16.5366\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 22.7775\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 20.2376\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 26.0392\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 20.5700\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 23.1565\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 19.6986\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 19.8365\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 23.6383\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 17.0622\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 20.2676\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 19.7766\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 6: 18.6093\n",
      "\n",
      "Epoch 6 ended\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss at step 0: 6.0271\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 6.2421\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 8.2639\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 9.5321\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 11.1826\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 14.2230\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 12.6275\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 12.7718\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 12.8294\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 16.2537\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 14.3071\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 17.6471\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 14.4725\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 16.3443\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 15.7633\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 15.4148\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 15.1215\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 14.4865\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 15.0304\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 13.3521\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 13.6835\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 12.8064\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 12.6975\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 12.9525\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 14.0789\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 14.5718\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 16.3724\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 15.0267\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 15.7251\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 15.3206\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 18.8946\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 18.7696\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 18.4052\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 17.3509\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 13.7113\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 15.6610\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 13.8625\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 14.2713\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 15.9118\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 14.1703\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 14.8852\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 13.8215\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 11.7510\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 15.7241\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 14.0718\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 12.9362\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 14.3904\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 12.3625\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 13.7471\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 14.8499\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 13.4521\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 14.6699\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 14.0752\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 13.5399\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 13.5378\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 12.6409\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 12.3076\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 14.4668\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 11.6277\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 16.1800\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 13.4085\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 13.5746\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 14.0055\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 13.8356\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 12.7189\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 12.6430\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 12.8816\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 13.5103\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 15.8888\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 17.1028\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 17.0353\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 16.3982\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 15.5462\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 14.8429\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 16.0862\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 19.3250\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 15.5695\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 17.7003\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 22.8710\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 21.5508\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 22.7544\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 20.2351\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 22.2867\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 20.7961\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 19.7472\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 21.4482\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 21.8762\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 22.0589\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 27.5756\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 23.4463\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 20.4065\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 22.7832\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 23.3214\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 20.5314\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 25.0172\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 25.9946\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 28.2494\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 24.7593\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 23.6980\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 22.1246\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 23.2320\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 21.9621\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 21.9049\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 22.2624\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 18.9087\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 19.9169\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 20.7868\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 20.0912\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 22.2758\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 18.6539\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 18.7780\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 20.3561\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 18.7091\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 20.9347\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 22.0310\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 24.6897\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 21.1544\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 22.1659\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 17.8637\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 15.7189\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 22.5901\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 19.9004\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 22.7994\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 21.4378\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 20.3920\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 19.7043\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 19.8985\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 19.2355\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 16.3721\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 18.7358\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 17.8514\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 7: 17.1610\n",
      "\n",
      "Epoch 7 ended\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss at step 0: 5.8220\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 6.2318\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 8.1694\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 9.1321\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 9.8579\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 13.3669\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 11.4431\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 11.5801\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 12.1844\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 14.0186\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 12.7809\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 15.4301\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 12.6967\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 13.7938\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 13.7981\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 13.8207\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 13.5998\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 13.7691\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 13.5739\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 11.8995\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 12.7182\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 11.8911\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 12.0517\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 12.1852\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 13.5957\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 13.9268\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 14.1694\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 13.7593\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 13.8238\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 14.8001\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 15.8974\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 17.4562\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 16.9456\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 15.9346\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 13.6812\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 16.1069\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 14.0118\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 13.9991\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 15.4650\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 14.6915\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 14.7297\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 13.9169\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 11.8035\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 15.4118\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 13.2071\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 13.5534\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 14.2057\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 12.3879\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 14.3715\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 15.5602\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 13.9543\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 14.3117\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 14.3000\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 13.9978\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 13.6965\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 12.1488\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 13.4989\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 14.1756\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 11.5059\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 17.4638\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 13.4198\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 14.0373\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 13.8079\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 12.1463\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 13.5359\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 13.1202\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 12.5718\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 14.3011\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 15.8048\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 16.5881\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 17.9831\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 15.0628\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 16.0342\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 15.7598\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 15.7154\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 21.1753\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 16.4181\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 18.3536\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 25.4887\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 21.5962\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 24.2541\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 21.4200\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 25.9891\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 20.3710\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 20.6287\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 21.8470\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 22.9745\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 21.8141\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 28.2010\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 21.3431\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 20.2771\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 21.1695\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 21.7896\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 22.2821\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 20.5117\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 29.4202\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 26.6179\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 30.9350\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 28.8783\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 28.1287\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 32.3183\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 21.1132\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 32.3418\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 25.8657\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 20.7136\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 26.6391\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 23.0444\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 24.0133\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 27.7815\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 19.4813\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 21.5664\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 24.6892\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 18.0834\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 24.3448\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 23.6394\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 26.9726\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 21.5552\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 22.0163\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 18.5479\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 16.3352\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 22.7459\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 19.7328\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 22.9247\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 20.0368\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 21.1764\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 19.4271\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 20.6217\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 19.3807\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 16.7203\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 18.4909\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 19.5206\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 8: 17.5259\n",
      "\n",
      "Epoch 8 ended\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss at step 0: 5.8940\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 5.6707\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 8.0746\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 8.7323\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 9.4342\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 12.2766\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 11.2038\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 11.3388\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 11.9116\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 13.7366\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 11.9832\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 15.2060\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 11.6841\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 14.3654\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 13.6078\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 14.1302\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 13.8770\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 12.9862\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 13.7350\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 12.0723\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 12.5088\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 11.8146\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 11.0546\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 12.6075\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 12.4830\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 12.6304\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 13.7146\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 13.7521\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 14.1563\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 13.8090\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 17.5230\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 17.4275\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 15.0216\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 15.5933\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 12.3296\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 14.1140\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 13.5955\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 13.2523\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 15.2102\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 13.1384\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 13.5331\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 13.2207\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 11.4042\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 15.5043\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 12.9756\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 12.5905\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 14.7142\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 12.7017\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 14.1366\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 15.3684\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 14.3597\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 13.8367\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 14.2732\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 13.9408\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 13.1160\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 13.6316\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 12.8389\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 14.4106\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 11.8858\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 16.8117\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 14.3089\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 15.9966\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 14.1271\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 14.0953\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 13.2391\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 11.9556\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 12.5565\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 15.6508\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 15.8511\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 18.3542\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 17.3105\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 15.7870\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 15.5465\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 15.3741\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 14.8282\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 19.4103\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 16.7838\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 19.3449\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 22.2546\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 24.3219\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 22.7137\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 25.0644\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 21.5275\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 27.2877\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 19.8299\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 25.8161\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 24.0993\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 22.2564\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 32.5664\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 22.4275\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 24.7805\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 24.6264\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 21.9908\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 25.6729\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 21.0933\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 28.7256\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 26.3184\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 25.5594\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 28.0450\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 25.3567\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 28.3411\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 24.9503\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 23.5376\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 25.8578\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 22.9250\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 21.1771\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 25.4571\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 22.0559\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 23.4077\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 23.6857\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 18.5246\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 22.8158\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 19.9104\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 21.1696\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 26.8452\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 25.0327\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 21.7081\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 23.5414\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 19.5028\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 17.9339\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 25.7416\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 22.2499\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 26.7604\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 21.7771\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 23.7633\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 18.2602\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 26.5135\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 20.2502\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 21.1403\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 23.6476\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 17.7941\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 9: 17.6492\n",
      "\n",
      "Epoch 9 ended\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss at step 0: 6.2555\n",
      "Seen so far: 0 samples\n",
      "Training loss at step 100: 6.3896\n",
      "Seen so far: 100 samples\n",
      "Training loss at step 200: 8.3223\n",
      "Seen so far: 200 samples\n",
      "Training loss at step 300: 9.8347\n",
      "Seen so far: 300 samples\n",
      "Training loss at step 400: 10.9560\n",
      "Seen so far: 400 samples\n",
      "Training loss at step 500: 14.1613\n",
      "Seen so far: 500 samples\n",
      "Training loss at step 600: 12.0455\n",
      "Seen so far: 600 samples\n",
      "Training loss at step 700: 11.9224\n",
      "Seen so far: 700 samples\n",
      "Training loss at step 800: 13.0921\n",
      "Seen so far: 800 samples\n",
      "Training loss at step 900: 15.1758\n",
      "Seen so far: 900 samples\n",
      "Training loss at step 1000: 12.8761\n",
      "Seen so far: 1000 samples\n",
      "Training loss at step 1100: 17.6864\n",
      "Seen so far: 1100 samples\n",
      "Training loss at step 1200: 13.6812\n",
      "Seen so far: 1200 samples\n",
      "Training loss at step 1300: 16.3671\n",
      "Seen so far: 1300 samples\n",
      "Training loss at step 1400: 16.4625\n",
      "Seen so far: 1400 samples\n",
      "Training loss at step 1500: 16.3127\n",
      "Seen so far: 1500 samples\n",
      "Training loss at step 1600: 15.9777\n",
      "Seen so far: 1600 samples\n",
      "Training loss at step 1700: 15.2049\n",
      "Seen so far: 1700 samples\n",
      "Training loss at step 1800: 15.0633\n",
      "Seen so far: 1800 samples\n",
      "Training loss at step 1900: 12.9256\n",
      "Seen so far: 1900 samples\n",
      "Training loss at step 2000: 14.4645\n",
      "Seen so far: 2000 samples\n",
      "Training loss at step 2100: 13.0986\n",
      "Seen so far: 2100 samples\n",
      "Training loss at step 2200: 14.1320\n",
      "Seen so far: 2200 samples\n",
      "Training loss at step 2300: 12.9736\n",
      "Seen so far: 2300 samples\n",
      "Training loss at step 2400: 13.6448\n",
      "Seen so far: 2400 samples\n",
      "Training loss at step 2500: 15.2139\n",
      "Seen so far: 2500 samples\n",
      "Training loss at step 2600: 14.5634\n",
      "Seen so far: 2600 samples\n",
      "Training loss at step 2700: 15.7049\n",
      "Seen so far: 2700 samples\n",
      "Training loss at step 2800: 15.5732\n",
      "Seen so far: 2800 samples\n",
      "Training loss at step 2900: 15.9377\n",
      "Seen so far: 2900 samples\n",
      "Training loss at step 3000: 16.9096\n",
      "Seen so far: 3000 samples\n",
      "Training loss at step 3100: 16.9242\n",
      "Seen so far: 3100 samples\n",
      "Training loss at step 3200: 16.9952\n",
      "Seen so far: 3200 samples\n",
      "Training loss at step 3300: 16.0459\n",
      "Seen so far: 3300 samples\n",
      "Training loss at step 3400: 13.0302\n",
      "Seen so far: 3400 samples\n",
      "Training loss at step 3500: 15.5173\n",
      "Seen so far: 3500 samples\n",
      "Training loss at step 3600: 14.5160\n",
      "Seen so far: 3600 samples\n",
      "Training loss at step 3700: 14.3203\n",
      "Seen so far: 3700 samples\n",
      "Training loss at step 3800: 16.2281\n",
      "Seen so far: 3800 samples\n",
      "Training loss at step 3900: 14.7590\n",
      "Seen so far: 3900 samples\n",
      "Training loss at step 4000: 15.1429\n",
      "Seen so far: 4000 samples\n",
      "Training loss at step 4100: 13.1783\n",
      "Seen so far: 4100 samples\n",
      "Training loss at step 4200: 12.1832\n",
      "Seen so far: 4200 samples\n",
      "Training loss at step 4300: 15.3176\n",
      "Seen so far: 4300 samples\n",
      "Training loss at step 4400: 13.5163\n",
      "Seen so far: 4400 samples\n",
      "Training loss at step 4500: 13.5468\n",
      "Seen so far: 4500 samples\n",
      "Training loss at step 4600: 15.0277\n",
      "Seen so far: 4600 samples\n",
      "Training loss at step 4700: 12.2633\n",
      "Seen so far: 4700 samples\n",
      "Training loss at step 4800: 13.2444\n",
      "Seen so far: 4800 samples\n",
      "Training loss at step 4900: 14.9145\n",
      "Seen so far: 4900 samples\n",
      "Training loss at step 5000: 12.4614\n",
      "Seen so far: 5000 samples\n",
      "Training loss at step 5100: 14.5261\n",
      "Seen so far: 5100 samples\n",
      "Training loss at step 5200: 14.4226\n",
      "Seen so far: 5200 samples\n",
      "Training loss at step 5300: 13.4199\n",
      "Seen so far: 5300 samples\n",
      "Training loss at step 5400: 13.4567\n",
      "Seen so far: 5400 samples\n",
      "Training loss at step 5500: 12.7490\n",
      "Seen so far: 5500 samples\n",
      "Training loss at step 5600: 13.5957\n",
      "Seen so far: 5600 samples\n",
      "Training loss at step 5700: 13.7984\n",
      "Seen so far: 5700 samples\n",
      "Training loss at step 5800: 12.0107\n",
      "Seen so far: 5800 samples\n",
      "Training loss at step 5900: 17.1519\n",
      "Seen so far: 5900 samples\n",
      "Training loss at step 6000: 14.3254\n",
      "Seen so far: 6000 samples\n",
      "Training loss at step 6100: 14.5720\n",
      "Seen so far: 6100 samples\n",
      "Training loss at step 6200: 12.3730\n",
      "Seen so far: 6200 samples\n",
      "Training loss at step 6300: 13.0192\n",
      "Seen so far: 6300 samples\n",
      "Training loss at step 6400: 12.9945\n",
      "Seen so far: 6400 samples\n",
      "Training loss at step 6500: 12.6291\n",
      "Seen so far: 6500 samples\n",
      "Training loss at step 6600: 12.0621\n",
      "Seen so far: 6600 samples\n",
      "Training loss at step 6700: 13.9679\n",
      "Seen so far: 6700 samples\n",
      "Training loss at step 6800: 16.4721\n",
      "Seen so far: 6800 samples\n",
      "Training loss at step 6900: 18.0908\n",
      "Seen so far: 6900 samples\n",
      "Training loss at step 7000: 17.2841\n",
      "Seen so far: 7000 samples\n",
      "Training loss at step 7100: 15.3980\n",
      "Seen so far: 7100 samples\n",
      "Training loss at step 7200: 14.9608\n",
      "Seen so far: 7200 samples\n",
      "Training loss at step 7300: 14.4124\n",
      "Seen so far: 7300 samples\n",
      "Training loss at step 7400: 14.3504\n",
      "Seen so far: 7400 samples\n",
      "Training loss at step 7500: 18.4075\n",
      "Seen so far: 7500 samples\n",
      "Training loss at step 7600: 14.4015\n",
      "Seen so far: 7600 samples\n",
      "Training loss at step 7700: 17.2823\n",
      "Seen so far: 7700 samples\n",
      "Training loss at step 7800: 20.8297\n",
      "Seen so far: 7800 samples\n",
      "Training loss at step 7900: 20.2930\n",
      "Seen so far: 7900 samples\n",
      "Training loss at step 8000: 20.9682\n",
      "Seen so far: 8000 samples\n",
      "Training loss at step 8100: 20.9679\n",
      "Seen so far: 8100 samples\n",
      "Training loss at step 8200: 22.9330\n",
      "Seen so far: 8200 samples\n",
      "Training loss at step 8300: 21.9229\n",
      "Seen so far: 8300 samples\n",
      "Training loss at step 8400: 21.6484\n",
      "Seen so far: 8400 samples\n",
      "Training loss at step 8500: 22.9752\n",
      "Seen so far: 8500 samples\n",
      "Training loss at step 8600: 29.4214\n",
      "Seen so far: 8600 samples\n",
      "Training loss at step 8700: 21.4121\n",
      "Seen so far: 8700 samples\n",
      "Training loss at step 8800: 34.3765\n",
      "Seen so far: 8800 samples\n",
      "Training loss at step 8900: 28.0222\n",
      "Seen so far: 8900 samples\n",
      "Training loss at step 9000: 21.8357\n",
      "Seen so far: 9000 samples\n",
      "Training loss at step 9100: 32.3678\n",
      "Seen so far: 9100 samples\n",
      "Training loss at step 9200: 26.9987\n",
      "Seen so far: 9200 samples\n",
      "Training loss at step 9300: 22.4952\n",
      "Seen so far: 9300 samples\n",
      "Training loss at step 9400: 31.4063\n",
      "Seen so far: 9400 samples\n",
      "Training loss at step 9500: 26.1097\n",
      "Seen so far: 9500 samples\n",
      "Training loss at step 9600: 33.8950\n",
      "Seen so far: 9600 samples\n",
      "Training loss at step 9700: 28.4828\n",
      "Seen so far: 9700 samples\n",
      "Training loss at step 9800: 26.9612\n",
      "Seen so far: 9800 samples\n",
      "Training loss at step 9900: 25.6260\n",
      "Seen so far: 9900 samples\n",
      "Training loss at step 10000: 25.5139\n",
      "Seen so far: 10000 samples\n",
      "Training loss at step 10100: 23.8491\n",
      "Seen so far: 10100 samples\n",
      "Training loss at step 10200: 22.3097\n",
      "Seen so far: 10200 samples\n",
      "Training loss at step 10300: 23.8695\n",
      "Seen so far: 10300 samples\n",
      "Training loss at step 10400: 21.2648\n",
      "Seen so far: 10400 samples\n",
      "Training loss at step 10500: 20.4623\n",
      "Seen so far: 10500 samples\n",
      "Training loss at step 10600: 20.3708\n",
      "Seen so far: 10600 samples\n",
      "Training loss at step 10700: 21.9700\n",
      "Seen so far: 10700 samples\n",
      "Training loss at step 10800: 21.7958\n",
      "Seen so far: 10800 samples\n",
      "Training loss at step 10900: 19.0665\n",
      "Seen so far: 10900 samples\n",
      "Training loss at step 11000: 21.7156\n",
      "Seen so far: 11000 samples\n",
      "Training loss at step 11100: 21.2399\n",
      "Seen so far: 11100 samples\n",
      "Training loss at step 11200: 20.2126\n",
      "Seen so far: 11200 samples\n",
      "Training loss at step 11300: 24.9050\n",
      "Seen so far: 11300 samples\n",
      "Training loss at step 11400: 22.8506\n",
      "Seen so far: 11400 samples\n",
      "Training loss at step 11500: 27.4425\n",
      "Seen so far: 11500 samples\n",
      "Training loss at step 11600: 21.1143\n",
      "Seen so far: 11600 samples\n",
      "Training loss at step 11700: 21.9725\n",
      "Seen so far: 11700 samples\n",
      "Training loss at step 11800: 21.9840\n",
      "Seen so far: 11800 samples\n",
      "Training loss at step 11900: 16.4377\n",
      "Seen so far: 11900 samples\n",
      "Training loss at step 12000: 24.9193\n",
      "Seen so far: 12000 samples\n",
      "Training loss at step 12100: 23.9964\n",
      "Seen so far: 12100 samples\n",
      "Training loss at step 12200: 24.7399\n",
      "Seen so far: 12200 samples\n",
      "Training loss at step 12300: 24.5028\n",
      "Seen so far: 12300 samples\n",
      "Training loss at step 12400: 20.1696\n",
      "Seen so far: 12400 samples\n",
      "Training loss at step 12500: 21.9020\n",
      "Seen so far: 12500 samples\n",
      "Training loss at step 12600: 22.3306\n",
      "Seen so far: 12600 samples\n",
      "Training loss at step 12700: 20.4188\n",
      "Seen so far: 12700 samples\n",
      "Training loss at step 12800: 17.3414\n",
      "Seen so far: 12800 samples\n",
      "Training loss at step 12900: 17.8511\n",
      "Seen so far: 12900 samples\n",
      "Training loss at step 13000: 19.2925\n",
      "Seen so far: 13000 samples\n",
      "Average training loss for epoch 10: 17.8928\n",
      "\n",
      "Epoch 10 ended\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 100\n",
    "training_loss_values = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nStart of epoch {epoch+1}\")\n",
    "\n",
    "    # Initialize variables to accumulate loss\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step in range(0, len(result), batch_size):\n",
    "        x_batch_train = result[step:step+batch_size, :, :]\n",
    "        y_batch_train = result[step:step+batch_size, :, 64]\n",
    "        # y_batch_train_one_hot = keras.utils.to_categorical(y_batch_train, num_classes)\n",
    "        # print(y_batch_train_one_hot.shape)\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Run the forward pass of the layer.\n",
    "            logits = transformer(x_batch_train, training=True)  # Logits for this minibatch\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, transformer.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, transformer.trainable_weights))\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss_value\n",
    "        num_batches += 1\n",
    "\n",
    "        # Log every 100 batches.\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Training loss at step {step}: {float(loss_value):.4f}\"\n",
    "            )\n",
    "            print(f\"Seen so far: {step} samples\")\n",
    "\n",
    "\n",
    "    # Calculate and print the average loss for the epoch\n",
    "    avg_loss = total_loss / num_batches\n",
    "    training_loss_values.append(avg_loss)\n",
    "    print(f\"Average training loss for epoch {epoch + 1}: {float(avg_loss):.4f}\")\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1} ended\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training loss\n",
    "plt.plot(range(1, epochs + 1), training_loss_values, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('Average Loss vs Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "Shape of Predictions: (100, 30)\n"
     ]
    }
   ],
   "source": [
    "predictions = transformer.predict(result[:100, :, :], batch_size=batch_size)\n",
    "\n",
    "# 'predictions' will contain the model's predictions for the test_data\n",
    "print(\"Shape of Predictions:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        16.987024  14.390427\n",
      " 12.316244   0.        26.973192  31.205574  34.63744   11.076159\n",
      " 15.968062  18.758791   0.         2.2694092 25.526825   0.\n",
      "  0.         0.        33.02851    0.         0.         0.       ]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 14. 18.  4. 19. 20. 21.  3.  4. 22. 23.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(result[0,:,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Encoder)           multiple                  305920    \n",
      "                                                                 \n",
      " decoder (Decoder)           multiple                  1283934   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1589854 (6.06 MB)\n",
      "Trainable params: 1589598 (6.06 MB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer.save_weights(\"model7_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  3 24 25 26 23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0]\n",
      "[ 0  1  3 24 25 26 23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(Y_data[1])\n",
    "# Assuming your output array is named 'output_array'\n",
    "output_array_shifted = np.roll(Y_data, shift=1, axis=1)\n",
    "print(output_array_shifted[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 21.3 GiB for an array with shape (393000, 14518) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_batch_train_one_hot \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_batch_train_one_hot)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/np_utils.py:73\u001b[0m, in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     71\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(y) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     72\u001b[0m n \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 73\u001b[0m categorical \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m categorical[np\u001b[38;5;241m.\u001b[39marange(n), y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     75\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 21.3 GiB for an array with shape (393000, 14518) and data type float32"
     ]
    }
   ],
   "source": [
    "y_batch_train_one_hot = keras.utils.to_categorical(Y_data, num_classes)\n",
    "print(y_batch_train_one_hot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
