{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"E:\\\\ML\\\\data\\\\LJSpeech-1.1\\\\metadata.csv\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"ID\", \"Text1\", \"Text2\"],\n",
    ")\n",
    "texts = data[\"Text1\"].to_list()\n",
    "ID = data[\"ID\"].to_list()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "test = tokenizer.word_index\n",
    "print(sequences[:10])\n",
    "X_test = pad_sequences(sequences, padding=\"post\", maxlen=30)\n",
    "y_text = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract MFCC features from audio\n",
    "\n",
    "\n",
    "def feature_extraction(audio_path, desired_frames):\n",
    "    x, sample_rate = librosa.load(audio_path, res_type=\"kaiser_fast\")\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=x,  # Audio signal\n",
    "        sr=sample_rate,  # Sampling rate\n",
    "        n_mfcc=20,  # Number of MFCC coefficients to compute\n",
    "        n_fft=2048,  # FFT window size\n",
    "        hop_length=512,  # Number of samples between successive frames\n",
    "        n_mels=128,  # Number of Mel bands to generate\n",
    "        htk=True,  # Use HTK formula for Mel filter banks\n",
    "        norm=\"ortho\",  # Normalization for Mel spectrogram\n",
    "        center=False,  # Do not center the frame\n",
    "        pad_mode=\"constant\",  # Padding mode\n",
    "    )\n",
    "    if mfcc.shape[1] < desired_frames:\n",
    "        pad_width = desired_frames - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)))\n",
    "\n",
    "    # If the number of frames is greater than desired_frames, truncate\n",
    "    elif mfcc.shape[1] > desired_frames:\n",
    "        mfcc = mfcc[:, :desired_frames]\n",
    "    return mfcc\n",
    "\n",
    "\n",
    "# Load your dataset\n",
    "dataset_directory = \"E:\\\\ML\\\\data\\\\LJSpeech-1.1\\\\wavs\\\\\"\n",
    "audio_files = os.listdir(dataset_directory)\n",
    "\n",
    "# Lists to store features and transcriptions\n",
    "mfcc_features_list = []\n",
    "transcription_sequences = []\n",
    "i = 0\n",
    "# Process each audio file\n",
    "for audio_file in audio_files:\n",
    "    audio_path = os.path.join(dataset_directory, audio_file)\n",
    "\n",
    "    # Extract MFCC features\n",
    "    mfcc_features = feature_extraction(audio_path, 500)\n",
    "    mfcc_features_list.append(mfcc_features)\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "X_mfcc = np.array(mfcc_features_list)\n",
    "# np.save(\"E:\\\\ML\\\\data\", X_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "    x = inputs\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return res + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer_stt_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0.1,\n",
    "):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_block(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    # Output layer for sequence data\n",
    "    outputs = layers.Dense(vocab_size)(x)  # Adjust vocab_size based on your task\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your features (X) and labels (y) ready\n",
    "# X_mfcc = np.load(\"../data/data_mfcc.npy\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mfcc, y_text, test_size=0.2, random_state=42\n",
    ")\n",
    "print(X_mfcc.shape)\n",
    "print(y_text.shape)\n",
    "print(X_train.shape)\n",
    "print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "head_size = 256\n",
    "num_heads = 4\n",
    "ff_dim = 4\n",
    "num_transformer_blocks = 4\n",
    "mlp_units = [128]\n",
    "\n",
    "# Assume vocab_size is the number of unique characters in your transcriptions\n",
    "vocab_size = 30  # Replace with the actual size of your vocabulary\n",
    "\n",
    "model = build_transformer_stt_model(\n",
    "    input_shape=(X_mfcc.shape[1], X_mfcc.shape[2]),\n",
    "    head_size=head_size,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    num_transformer_blocks=num_transformer_blocks,\n",
    "    mlp_units=mlp_units,\n",
    ")\n",
    "\n",
    "# Compile the model with appropriate loss and optimizer\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=\"mae\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "predicted = model.predict(X_train_reshaped[1])\n",
    "print(predicted, y_train[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
