{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 19:25:06.106679: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-04 19:25:06.142171: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-04 19:25:06.142200: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-04 19:25:06.143052: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-04 19:25:06.148904: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-04 19:25:06.815272: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import nn\n",
    "from tensorflow.keras.activations import softmax\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import Dense,LayerNormalization ## alternative for nn.linear\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 500, 20)\n",
      "14518\n",
      "(100, 30)\n"
     ]
    }
   ],
   "source": [
    "X_data = np.load(\"../data/data_mfcc.npy\")\n",
    "X_data = np.transpose(X_data, (0, 2, 1))\n",
    "X_data=X_data[:100]\n",
    "print(X_data.shape)\n",
    "data = pd.read_csv(\n",
    "    \"../data/LJSpeech-1.1/metadata.csv\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"ID\", \"Text1\", \"Text2\"],\n",
    ")\n",
    "texts = data[\"Text1\"].to_list()\n",
    "ID = data[\"ID\"].to_list()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "num_classes = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "Y_data = pad_sequences(sequences, padding=\"post\", maxlen=30)\n",
    "Y_data=Y_data[:100]\n",
    "print(num_classes)\n",
    "print(Y_data.shape)\n",
    "# print(X_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 500, 30)\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train is your original array with shape (100, 500, 20)\n",
    "original_shape = X_data.shape\n",
    "target_shape = (100, 500, 30)\n",
    "\n",
    "# Calculate the padding needed for each dimension\n",
    "pad_width = [(0, 0)] * len(original_shape)  # Initialize with zero padding for existing dimensions\n",
    "\n",
    "for i in range(len(original_shape)):\n",
    "    pad_width[i] = (0, target_shape[i] - original_shape[i])\n",
    "\n",
    "# Pad the array\n",
    "X_data = np.pad(X_data, pad_width=pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "# Now, X_train_padded will have the shape (100, 500, 30)\n",
    "print(X_data.shape)\n",
    "# print(X_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_self_attention_mask(sequence_length):\n",
    "    # Create a lower triangular matrix with ones\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((sequence_length, sequence_length)), -1, 0)\n",
    "    # Add a large negative value to the upper triangle\n",
    "    mask = mask * -1e9\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask):\n",
    "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_qk = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_qk += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        even_i = tf.range(0, self.d_model, 2, dtype=tf.float32)\n",
    "        denominator = tf.pow(10000.0, even_i / self.d_model)\n",
    "        position = tf.reshape(\n",
    "            tf.range(self.max_sequence_length, dtype=tf.float32),\n",
    "            (1, self.max_sequence_length, 1),\n",
    "        )\n",
    "        even_PE = tf.sin(position / denominator)\n",
    "        odd_PE = tf.cos(position / denominator)\n",
    "        stacked = tf.stack([even_PE, odd_PE], axis=2)\n",
    "        PE = tf.reshape(stacked, (1, self.max_sequence_length, -1))\n",
    "        return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer():\n",
    "    def __init__(self, input_shape, filters=32, kernel_size=3, **kwargs):\n",
    "        super(ConvolutionalLayer, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Extract the number of filters from the input shape\n",
    "        if isinstance(input_shape, tuple):\n",
    "            self.filters = input_shape[-1]\n",
    "\n",
    "        self.conv1 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\")\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "\n",
    "        self.conv2 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\")\n",
    "        self.batch_norm2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.ReLU()\n",
    "\n",
    "    \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        conv1_out = self.relu1(self.batch_norm1(self.conv1(inputs), training=training))\n",
    "        conv2_out = self.relu2(self.batch_norm2(self.conv2(conv1_out), training=training))\n",
    "        print(\"CNN output shape is \",gap_out.shape)\n",
    "        return gap_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = tf.keras.layers.Dense(3 * d_model, use_bias=False)\n",
    "        self.linear_layer = tf.keras.layers.Dense(d_model, activation='relu')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        if len(x.shape) == 2:\n",
    "            # Expand dimensions to simulate batch_size=1 and sequence_length=30\n",
    "            x = tf.expand_dims(tf.expand_dims(x, axis=0), axis=1)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        print(\"MultiHeadAttention input shape\",x.shape)\n",
    "        batch_size, _, _ = x.shape\n",
    "\n",
    "        qkv = self.qkv_layer(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
    "        values = tf.reshape(values, (batch_size, -1, self.num_heads * self.head_dim))\n",
    "        out = self.linear_layer(values)\n",
    "        print(\"MultiHeadAttention output shape is \",out.shape)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = tf.keras.layers.Dense(hidden)\n",
    "        self.linear2 = tf.keras.layers.Dense(d_model)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        print(\"Input shape for positonal encoding\",x.shape)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        print(\"output shape from postional encoding\",x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadCrossAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.kv_layer = tf.keras.layers.Dense(2 * d_model)\n",
    "        self.q_layer = tf.keras.layers.Dense(d_model)\n",
    "        self.linear_layer = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    \n",
    "    def call(self, x, y, mask):\n",
    "        batch_size, sequence_length, d_model = tf.shape(x)\n",
    "        kv = self.kv_layer(x)\n",
    "        q = self.q_layer(y)\n",
    "        kv = tf.reshape(kv, (batch_size, sequence_length, self.num_heads, 2 * self.head_dim))\n",
    "        q = tf.reshape(q, (batch_size, sequence_length, self.num_heads, self.head_dim))\n",
    "        kv = tf.transpose(kv, perm=[0, 2, 1, 3])\n",
    "        q = tf.transpose(q, perm=[0, 2, 1, 3])\n",
    "        k, v = tf.split(kv, 2, axis=-1)\n",
    "        \n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
    "        values = tf.reshape(values, (batch_size, sequence_length, d_model))\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-5)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-5)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "\n",
    "    \n",
    "    def call(self, x, self_attention_mask, training=None):\n",
    "        residual_x = x\n",
    "        x = self.attention(x, mask=self_attention_mask)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.norm1(x + residual_x)\n",
    "\n",
    "        residual_x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.norm2(x + residual_x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length):\n",
    "        super(SequentialEncoder, self).__init__()\n",
    "        self.layers = [EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)]\n",
    "\n",
    "    \n",
    "    def call(self, x, training=True, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, training, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.convolutional_layer = ConvolutionalLayer(input_shape=(max_sequence_length, 30))  # Assuming input shape (max_sequence_length, 20)\n",
    "        self.positional_encoding = PositionalEncoding(d_model=d_model, max_sequence_length=max_sequence_length)\n",
    "        self.layers = SequentialEncoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length)\n",
    "    \n",
    "    \n",
    "    def call(self, x, self_attention_mask):\n",
    "        # Assuming x is the output from the convolutional layer\n",
    "        print(\"Encoder input shape is\",x.shape)\n",
    "        x = self.convolutional_layer.call(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.layers(x, self_attention_mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.layer_norm1 = LayerNormalization(epsilon=1e-5)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "\n",
    "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.layer_norm2 = LayerNormalization(epsilon=1e-5)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.layer_norm3 = LayerNormalization(epsilon=1e-5)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "\n",
    "    \n",
    "    def call(self, x, y, self_attention_mask, cross_attention_mask, training=None):\n",
    "        print(\"Shape of x before encoder_decoder_attention:\", x.shape)\n",
    "        print(\"Shape of y before encoder_decoder_attention:\", y.shape)\n",
    "\n",
    "        _y = y\n",
    "        y = self.self_attention(y, mask=self_attention_mask)\n",
    "        y = self.dropout1(y, training=training)\n",
    "        y = self.layer_norm1(y + _y)\n",
    "\n",
    "        _y = y\n",
    "        print(\"Shape of x before encoder_decoder_attention:\", x.shape)\n",
    "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
    "        y = self.dropout2(y, training=training)\n",
    "        y = self.layer_norm2(y + _y)\n",
    "\n",
    "        _y = y\n",
    "        y = self.ffn(y)\n",
    "        y = self.dropout3(y, training=training)\n",
    "        y = self.layer_norm3(y + _y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, layers):\n",
    "        super(SequentialDecoder, self).__init__()\n",
    "        self.layers = layers\n",
    "    \n",
    "    def call(self, x, y, self_attention_mask, cross_attention_mask, training=None):\n",
    "        for layer in self.layers:\n",
    "            y = layer(x, y, self_attention_mask, cross_attention_mask, training=training)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length)\n",
    "        self.decoder = SequentialDecoder([DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "        self.final_layer = tf.keras.layers.Dense(units=max_sequence_length, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs['input_x']\n",
    "        y = inputs['input_y']\n",
    "        self_attention_mask_encoder = inputs['self_attention_mask_encoder']\n",
    "        self_attention_mask_decoder = inputs['self_attention_mask_decoder']\n",
    "        encoder_decoder_attention_mask = inputs['encoder_decoder_attention_mask']\n",
    "\n",
    "        print(\"Input shape:\", x.shape)\n",
    "\n",
    "        encoder_output = self.encoder(x, self_attention_mask_encoder)\n",
    "\n",
    "        print(\"Encoder output shape:\", encoder_output.shape)\n",
    "\n",
    "        decoder_output = self.decoder(encoder_output, y, self_attention_mask_decoder, encoder_decoder_attention_mask, training=training)\n",
    "\n",
    "        print(\"Decoder output shape:\", decoder_output.shape)\n",
    "\n",
    "        output = self.final_layer(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 500, 30)\n",
      "(20, 500, 30)\n",
      "(80, 30)\n",
      "(20, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_data, Y_data, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 19:25:09.482781: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-12-04 19:25:09.482822: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: Pc\n",
      "2023-12-04 19:25:09.482827: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: Pc\n",
      "2023-12-04 19:25:09.482994: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 535.129.3\n",
      "2023-12-04 19:25:09.483009: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 535.129.3\n",
      "2023-12-04 19:25:09.483012: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 535.129.3\n"
     ]
    }
   ],
   "source": [
    "d_model = 30\n",
    "ffn_hidden = 120\n",
    "num_heads = 1\n",
    "drop_prob = 0.1\n",
    "num_layers = 2\n",
    "max_sequence_length = 500\n",
    "transformer_model = Transformer(\n",
    "    d_model=d_model,\n",
    "    ffn_hidden=ffn_hidden,\n",
    "    num_heads=num_heads,\n",
    "    drop_prob=drop_prob,\n",
    "    num_layers=num_layers,\n",
    "    max_sequence_length=max_sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# transformer_model.build(input_shape=(100,500,30))  # Replace your_input_shape with the actual input shape\n",
    "# transformer_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 500, 500)\n",
      "(80, 500, 500)\n",
      "(80, 500, 500)\n"
     ]
    }
   ],
   "source": [
    "# Assuming max_sequence_length is the actual length of your input sequences\n",
    "max_sequence_length = X_train.shape[1]\n",
    "\n",
    "# Create self-attention mask for encoder\n",
    "self_attention_mask_encoder = create_self_attention_mask(max_sequence_length)\n",
    "self_attention_mask_encoder = tf.expand_dims(self_attention_mask_encoder, axis=0)  # Add batch dimension\n",
    "self_attention_mask_encoder = tf.tile(self_attention_mask_encoder, [X_train.shape[0], 1, 1])  # Tile to match the number of samples\n",
    "\n",
    "# Create self-attention mask for decoder\n",
    "self_attention_mask_decoder = create_self_attention_mask(500)  # Assuming 30 is the length of your target sequence\n",
    "self_attention_mask_decoder = tf.expand_dims(self_attention_mask_decoder, axis=0)  # Add batch dimension\n",
    "self_attention_mask_decoder = tf.tile(self_attention_mask_decoder, [X_train.shape[0],1, 1])  # Tile to match the number of samples\n",
    "\n",
    "# Create encoder-decoder attention mask\n",
    "encoder_decoder_attention_mask = create_self_attention_mask(500)  # Assuming 30 is the length of your target sequence\n",
    "encoder_decoder_attention_mask = tf.expand_dims(encoder_decoder_attention_mask, axis=0)  # Add batch dimension\n",
    "encoder_decoder_attention_mask = tf.tile(encoder_decoder_attention_mask, [X_train.shape[0],1, 1])  # Tile to match the number of samples\n",
    "print(self_attention_mask_encoder.shape)\n",
    "print(self_attention_mask_decoder.shape)\n",
    "print(encoder_decoder_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (None, 500, 30)\n",
      "Encoder input shape is (None, 500, 30)\n",
      "CNN output shape is  (None, 30)\n",
      "MultiHeadAttention input shape (1, 500, 30)\n",
      "MultiHeadAttention output shape is  (1, None, 30)\n",
      "Input shape for positonal encoding (1, 500, 30)\n",
      "output shape from postional encoding (1, 500, 30)\n",
      "MultiHeadAttention input shape (1, 500, 30)\n",
      "MultiHeadAttention output shape is  (1, None, 30)\n",
      "Input shape for positonal encoding (1, 500, 30)\n",
      "output shape from postional encoding (1, 500, 30)\n",
      "Encoder output shape: (1, 500, 30)\n",
      "Shape of x before encoder_decoder_attention: (1, 500, 30)\n",
      "Shape of y before encoder_decoder_attention: (None, 30)\n",
      "MultiHeadAttention input shape (None, 30)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filed0t3bt1m.py\", line 18, in tf__call\n        decoder_output = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(encoder_output), ag__.ld(y), ag__.ld(self_attention_mask_decoder), ag__.ld(encoder_decoder_attention_mask)), dict(training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_fileabcr8nya.py\", line 23, in tf__call\n        ag__.for_stmt(ag__.ld(self).layers, None, loop_body, get_state, set_state, ('y',), {'iterate_names': 'layer'})\n    File \"/tmp/__autograph_generated_fileabcr8nya.py\", line 21, in loop_body\n        y = ag__.converted_call(ag__.ld(layer), (ag__.ld(x), ag__.ld(y), ag__.ld(self_attention_mask), ag__.ld(cross_attention_mask)), dict(training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_file0o2eepzw.py\", line 13, in tf__call\n        y = ag__.converted_call(ag__.ld(self).self_attention, (ag__.ld(y),), dict(mask=ag__.ld(self_attention_mask)), fscope)\n    File \"/tmp/__autograph_generated_filev5lv2sn9.py\", line 11, in tf__call\n        (batch_size, _, _) = ag__.ld(x).shape\n\n    ValueError: Exception encountered when calling layer 'transformer' (type Transformer).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_38863/2657846362.py\", line 28, in call  *\n            decoder_output = self.decoder(encoder_output, y, self_attention_mask_decoder, encoder_decoder_attention_mask, training=training)\n        File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_fileabcr8nya.py\", line 23, in tf__call\n            ag__.for_stmt(ag__.ld(self).layers, None, loop_body, get_state, set_state, ('y',), {'iterate_names': 'layer'})\n        File \"/tmp/__autograph_generated_fileabcr8nya.py\", line 21, in loop_body\n            y = ag__.converted_call(ag__.ld(layer), (ag__.ld(x), ag__.ld(y), ag__.ld(self_attention_mask), ag__.ld(cross_attention_mask)), dict(training=ag__.ld(training)), fscope)\n        File \"/tmp/__autograph_generated_file0o2eepzw.py\", line 13, in tf__call\n            y = ag__.converted_call(ag__.ld(self).self_attention, (ag__.ld(y),), dict(mask=ag__.ld(self_attention_mask)), fscope)\n        File \"/tmp/__autograph_generated_filev5lv2sn9.py\", line 11, in tf__call\n            (batch_size, _, _) = ag__.ld(x).shape\n    \n        ValueError: Exception encountered when calling layer 'sequential_decoder' (type SequentialDecoder).\n        \n        in user code:\n        \n            File \"/tmp/ipykernel_38863/576445799.py\", line 8, in call  *\n                y = layer(x, y, self_attention_mask, cross_attention_mask, training=training)\n            File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/tmp/__autograph_generated_file0o2eepzw.py\", line 13, in tf__call\n                y = ag__.converted_call(ag__.ld(self).self_attention, (ag__.ld(y),), dict(mask=ag__.ld(self_attention_mask)), fscope)\n            File \"/tmp/__autograph_generated_filev5lv2sn9.py\", line 11, in tf__call\n                (batch_size, _, _) = ag__.ld(x).shape\n        \n            ValueError: Exception encountered when calling layer 'decoder_layer' (type DecoderLayer).\n            \n            in user code:\n            \n                File \"/tmp/ipykernel_38863/2717270461.py\", line 22, in call  *\n                    y = self.self_attention(y, mask=self_attention_mask)\n                File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"/tmp/__autograph_generated_filev5lv2sn9.py\", line 11, in tf__call\n                    (batch_size, _, _) = ag__.ld(x).shape\n            \n                ValueError: Exception encountered when calling layer 'multi_head_attention_2' (type MultiHeadAttention).\n                \n                in user code:\n                \n                    File \"/tmp/ipykernel_38863/2633592716.py\", line 19, in call  *\n                        batch_size, _, _ = x.shape\n                \n                    ValueError: not enough values to unpack (expected 3, got 2)\n                \n                \n                Call arguments received by layer 'multi_head_attention_2' (type MultiHeadAttention):\n                  • x=tf.Tensor(shape=(None, 30), dtype=int32)\n                  • mask=tf.Tensor(shape=(None, 500, 500), dtype=float32)\n            \n            \n            Call arguments received by layer 'decoder_layer' (type DecoderLayer):\n              • x=tf.Tensor(shape=(1, 500, 30), dtype=float32)\n              • y=tf.Tensor(shape=(None, 30), dtype=int32)\n              • self_attention_mask=tf.Tensor(shape=(None, 500, 500), dtype=float32)\n              • cross_attention_mask=tf.Tensor(shape=(None, 500, 500), dtype=float32)\n              • training=True\n        \n        \n        Call arguments received by layer 'sequential_decoder' (type SequentialDecoder):\n          • x=tf.Tensor(shape=(1, 500, 30), dtype=float32)\n          • y=tf.Tensor(shape=(None, 30), dtype=int32)\n          • self_attention_mask=tf.Tensor(shape=(None, 500, 500), dtype=float32)\n          • cross_attention_mask=tf.Tensor(shape=(None, 500, 500), dtype=float32)\n          • training=True\n    \n    \n    Call arguments received by layer 'transformer' (type Transformer):\n      • inputs={'input_x': 'tf.Tensor(shape=(None, 500, 30), dtype=float32)', 'input_y': 'tf.Tensor(shape=(None, 30), dtype=int32)', 'self_attention_mask_encoder': 'tf.Tensor(shape=(None, 500, 500), dtype=float32)', 'self_attention_mask_decoder': 'tf.Tensor(shape=(None, 500, 500), dtype=float32)', 'encoder_decoder_attention_mask': 'tf.Tensor(shape=(None, 500, 500), dtype=float32)'}\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/abhi/ML/ML_project/final/transformer.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/transformer.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Assuming you have modified your Transformer model to accept these masks separately\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/transformer.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m transformer_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/transformer.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     {\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/transformer.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39minput_x\u001b[39;49m\u001b[39m'\u001b[39;49m: X_train,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/transformer.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39minput_y\u001b[39;49m\u001b[39m'\u001b[39;49m: Y_train,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/transformer.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mself_attention_mask_encoder\u001b[39;49m\u001b[39m'\u001b[39;49m: self_attention_mask_encoder,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/transformer.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mself_attention_mask_decoder\u001b[39;49m\u001b[39m'\u001b[39;49m: self_attention_mask_decoder,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/transformer.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mencoder_decoder_attention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m: encoder_decoder_attention_mask\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/transformer.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     },\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/transformer.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     Y_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/transformer.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/transformer.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/transformer.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileao72r9gf.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filed0t3bt1m.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     16\u001b[0m encoder_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mencoder, (ag__\u001b[39m.\u001b[39mld(x), ag__\u001b[39m.\u001b[39mld(self_attention_mask_encoder)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     17\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39m'\u001b[39m\u001b[39mEncoder output shape:\u001b[39m\u001b[39m'\u001b[39m, ag__\u001b[39m.\u001b[39mld(encoder_output)\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 18\u001b[0m decoder_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdecoder, (ag__\u001b[39m.\u001b[39mld(encoder_output), ag__\u001b[39m.\u001b[39mld(y), ag__\u001b[39m.\u001b[39mld(self_attention_mask_decoder), ag__\u001b[39m.\u001b[39mld(encoder_decoder_attention_mask)), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     19\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39m'\u001b[39m\u001b[39mDecoder output shape:\u001b[39m\u001b[39m'\u001b[39m, ag__\u001b[39m.\u001b[39mld(decoder_output)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     20\u001b[0m output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfinal_layer, (ag__\u001b[39m.\u001b[39mld(decoder_output),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileabcr8nya.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, x, y, self_attention_mask, cross_attention_mask, training)\u001b[0m\n\u001b[1;32m     21\u001b[0m     y \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(layer), (ag__\u001b[39m.\u001b[39mld(x), ag__\u001b[39m.\u001b[39mld(y), ag__\u001b[39m.\u001b[39mld(self_attention_mask), ag__\u001b[39m.\u001b[39mld(cross_attention_mask)), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     22\u001b[0m layer \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mlayer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m ag__\u001b[39m.\u001b[39mfor_stmt(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mlayers, \u001b[39mNone\u001b[39;00m, loop_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m,), {\u001b[39m'\u001b[39m\u001b[39miterate_names\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mlayer\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileabcr8nya.py:21\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mnonlocal\u001b[39;00m y\n\u001b[1;32m     20\u001b[0m layer \u001b[39m=\u001b[39m itr\n\u001b[0;32m---> 21\u001b[0m y \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(layer), (ag__\u001b[39m.\u001b[39;49mld(x), ag__\u001b[39m.\u001b[39;49mld(y), ag__\u001b[39m.\u001b[39;49mld(self_attention_mask), ag__\u001b[39m.\u001b[39;49mld(cross_attention_mask)), \u001b[39mdict\u001b[39;49m(training\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(training)), fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file0o2eepzw.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, x, y, self_attention_mask, cross_attention_mask, training)\u001b[0m\n\u001b[1;32m     11\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39m'\u001b[39m\u001b[39mShape of y before encoder_decoder_attention:\u001b[39m\u001b[39m'\u001b[39m, ag__\u001b[39m.\u001b[39mld(y)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m _y \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(y)\n\u001b[0;32m---> 13\u001b[0m y \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mself_attention, (ag__\u001b[39m.\u001b[39mld(y),), \u001b[39mdict\u001b[39m(mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(self_attention_mask)), fscope)\n\u001b[1;32m     14\u001b[0m y \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdropout1, (ag__\u001b[39m.\u001b[39mld(y),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     15\u001b[0m y \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mlayer_norm1, (ag__\u001b[39m.\u001b[39mld(y) \u001b[39m+\u001b[39m ag__\u001b[39m.\u001b[39mld(_y),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filev5lv2sn9.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39m'\u001b[39m\u001b[39mMultiHeadAttention input shape\u001b[39m\u001b[39m'\u001b[39m, ag__\u001b[39m.\u001b[39mld(x)\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 11\u001b[0m (batch_size, _, _) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(x)\u001b[39m.\u001b[39mshape\n\u001b[1;32m     12\u001b[0m qkv \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mqkv_layer, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m (q, k, v) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39msplit, (ag__\u001b[39m.\u001b[39mld(qkv), \u001b[39m3\u001b[39m), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filed0t3bt1m.py\", line 18, in tf__call\n        decoder_output = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(encoder_output), ag__.ld(y), ag__.ld(self_attention_mask_decoder), ag__.ld(encoder_decoder_attention_mask)), dict(training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_fileabcr8nya.py\", line 23, in tf__call\n        ag__.for_stmt(ag__.ld(self).layers, None, loop_body, get_state, set_state, ('y',), {'iterate_names': 'layer'})\n    File \"/tmp/__autograph_generated_fileabcr8nya.py\", line 21, in loop_body\n        y = ag__.converted_call(ag__.ld(layer), (ag__.ld(x), ag__.ld(y), ag__.ld(self_attention_mask), ag__.ld(cross_attention_mask)), dict(training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_file0o2eepzw.py\", line 13, in tf__call\n        y = ag__.converted_call(ag__.ld(self).self_attention, (ag__.ld(y),), dict(mask=ag__.ld(self_attention_mask)), fscope)\n    File \"/tmp/__autograph_generated_filev5lv2sn9.py\", line 11, in tf__call\n        (batch_size, _, _) = ag__.ld(x).shape\n\n    ValueError: Exception encountered when calling layer 'transformer' (type Transformer).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_38863/2657846362.py\", line 28, in call  *\n            decoder_output = self.decoder(encoder_output, y, self_attention_mask_decoder, encoder_decoder_attention_mask, training=training)\n        File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_fileabcr8nya.py\", line 23, in tf__call\n            ag__.for_stmt(ag__.ld(self).layers, None, loop_body, get_state, set_state, ('y',), {'iterate_names': 'layer'})\n        File \"/tmp/__autograph_generated_fileabcr8nya.py\", line 21, in loop_body\n            y = ag__.converted_call(ag__.ld(layer), (ag__.ld(x), ag__.ld(y), ag__.ld(self_attention_mask), ag__.ld(cross_attention_mask)), dict(training=ag__.ld(training)), fscope)\n        File \"/tmp/__autograph_generated_file0o2eepzw.py\", line 13, in tf__call\n            y = ag__.converted_call(ag__.ld(self).self_attention, (ag__.ld(y),), dict(mask=ag__.ld(self_attention_mask)), fscope)\n        File \"/tmp/__autograph_generated_filev5lv2sn9.py\", line 11, in tf__call\n            (batch_size, _, _) = ag__.ld(x).shape\n    \n        ValueError: Exception encountered when calling layer 'sequential_decoder' (type SequentialDecoder).\n        \n        in user code:\n        \n            File \"/tmp/ipykernel_38863/576445799.py\", line 8, in call  *\n                y = layer(x, y, self_attention_mask, cross_attention_mask, training=training)\n            File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/tmp/__autograph_generated_file0o2eepzw.py\", line 13, in tf__call\n                y = ag__.converted_call(ag__.ld(self).self_attention, (ag__.ld(y),), dict(mask=ag__.ld(self_attention_mask)), fscope)\n            File \"/tmp/__autograph_generated_filev5lv2sn9.py\", line 11, in tf__call\n                (batch_size, _, _) = ag__.ld(x).shape\n        \n            ValueError: Exception encountered when calling layer 'decoder_layer' (type DecoderLayer).\n            \n            in user code:\n            \n                File \"/tmp/ipykernel_38863/2717270461.py\", line 22, in call  *\n                    y = self.self_attention(y, mask=self_attention_mask)\n                File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"/tmp/__autograph_generated_filev5lv2sn9.py\", line 11, in tf__call\n                    (batch_size, _, _) = ag__.ld(x).shape\n            \n                ValueError: Exception encountered when calling layer 'multi_head_attention_2' (type MultiHeadAttention).\n                \n                in user code:\n                \n                    File \"/tmp/ipykernel_38863/2633592716.py\", line 19, in call  *\n                        batch_size, _, _ = x.shape\n                \n                    ValueError: not enough values to unpack (expected 3, got 2)\n                \n                \n                Call arguments received by layer 'multi_head_attention_2' (type MultiHeadAttention):\n                  • x=tf.Tensor(shape=(None, 30), dtype=int32)\n                  • mask=tf.Tensor(shape=(None, 500, 500), dtype=float32)\n            \n            \n            Call arguments received by layer 'decoder_layer' (type DecoderLayer):\n              • x=tf.Tensor(shape=(1, 500, 30), dtype=float32)\n              • y=tf.Tensor(shape=(None, 30), dtype=int32)\n              • self_attention_mask=tf.Tensor(shape=(None, 500, 500), dtype=float32)\n              • cross_attention_mask=tf.Tensor(shape=(None, 500, 500), dtype=float32)\n              • training=True\n        \n        \n        Call arguments received by layer 'sequential_decoder' (type SequentialDecoder):\n          • x=tf.Tensor(shape=(1, 500, 30), dtype=float32)\n          • y=tf.Tensor(shape=(None, 30), dtype=int32)\n          • self_attention_mask=tf.Tensor(shape=(None, 500, 500), dtype=float32)\n          • cross_attention_mask=tf.Tensor(shape=(None, 500, 500), dtype=float32)\n          • training=True\n    \n    \n    Call arguments received by layer 'transformer' (type Transformer):\n      • inputs={'input_x': 'tf.Tensor(shape=(None, 500, 30), dtype=float32)', 'input_y': 'tf.Tensor(shape=(None, 30), dtype=int32)', 'self_attention_mask_encoder': 'tf.Tensor(shape=(None, 500, 500), dtype=float32)', 'self_attention_mask_decoder': 'tf.Tensor(shape=(None, 500, 500), dtype=float32)', 'encoder_decoder_attention_mask': 'tf.Tensor(shape=(None, 500, 500), dtype=float32)'}\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming you have modified your Transformer model to accept these masks separately\n",
    "transformer_model.fit(\n",
    "    {\n",
    "        'input_x': X_train,\n",
    "        'input_y': Y_train,\n",
    "        'self_attention_mask_encoder': self_attention_mask_encoder,\n",
    "        'self_attention_mask_decoder': self_attention_mask_decoder,\n",
    "        'encoder_decoder_attention_mask': encoder_decoder_attention_mask\n",
    "    },\n",
    "    Y_train,\n",
    "    epochs=1,\n",
    "    batch_size=30\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
