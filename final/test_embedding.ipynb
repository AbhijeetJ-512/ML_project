{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras_nlp.layers import PositionEmbedding\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14518\n",
      "(393000, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.01198915],\n",
       "       [-0.0633029 ],\n",
       "       [ 0.0093736 ],\n",
       "       [ 0.04419341],\n",
       "       [-0.01661698],\n",
       "       [-0.05808525],\n",
       "       [-0.00037932],\n",
       "       [-0.00964375],\n",
       "       [ 0.02145249],\n",
       "       [-0.03655505],\n",
       "       [-0.09183893],\n",
       "       [-0.02409197],\n",
       "       [-0.08685187],\n",
       "       [ 0.02324911],\n",
       "       [ 0.06045104],\n",
       "       [-0.03361482],\n",
       "       [-0.08698981],\n",
       "       [ 0.0050036 ],\n",
       "       [ 0.0931929 ],\n",
       "       [-0.05324392],\n",
       "       [-0.03192737],\n",
       "       [-0.0963658 ],\n",
       "       [-0.07770997],\n",
       "       [-0.00902212],\n",
       "       [ 0.02158431],\n",
       "       [-0.03013596],\n",
       "       [-0.05272006],\n",
       "       [-0.08228543],\n",
       "       [ 0.02043069],\n",
       "       [-0.07138651]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Your input data\n",
    "data = pd.read_csv(\n",
    "    \"../data/LJSpeech-1.1/metadata.csv\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"ID\", \"Text1\", \"Text2\"],\n",
    ")\n",
    "texts = data[\"Text1\"].to_list()\n",
    "ID = data[\"ID\"].to_list()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "num_classes = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X_data = pad_sequences(sequences, padding=\"post\", maxlen=30)\n",
    "print(num_classes)\n",
    "\n",
    "# Reshape to (13100 * 30, 1)\n",
    "reshaped_input = X_data.reshape((-1, 1))\n",
    "\n",
    "# Positional embedding layer\n",
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, **kwargs):\n",
    "        super(PositionEmbedding, self).__init__(**kwargs)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_length = input_shape[-1]\n",
    "        self.position_embeddings = self.add_weight(\n",
    "            shape=(self.sequence_length, feature_length),\n",
    "            initializer=tf.keras.initializers.RandomNormal(),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        start_index = 0\n",
    "        sequence_length = tf.shape(self.position_embeddings)[0]\n",
    "        feature_length = tf.shape(self.position_embeddings)[-1]\n",
    "\n",
    "        position_embeddings = tf.tile(\n",
    "            tf.slice(\n",
    "                self.position_embeddings,\n",
    "                (start_index, 0),\n",
    "                (sequence_length, feature_length),\n",
    "            ),\n",
    "            [tf.shape(inputs)[0] // sequence_length + 1, 1],\n",
    "        )\n",
    "\n",
    "        return tf.slice(position_embeddings, (0, 0), tf.shape(inputs))\n",
    "\n",
    "# Example usage\n",
    "position_embedding_layer = PositionEmbedding(sequence_length=30)\n",
    "embedded_output = position_embedding_layer(reshaped_input)\n",
    "embedded_output = embedded_output.numpy()  # Convert to NumPy array for printing\n",
    "print(embedded_output.shape)\n",
    "embedded_output = embedded_output.reshape((13100, 30, -1))\n",
    "embedded_output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, **kwargs):\n",
    "        super(PositionEmbedding, self).__init__(**kwargs)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[-1]\n",
    "        self.position_embeddings = self.add_weight(\n",
    "            shape=(self.sequence_length, feature_dim),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        start_index = 0\n",
    "        sequence_length = self.sequence_length\n",
    "        feature_length = tf.shape(self.position_embeddings)[-1]\n",
    "\n",
    "        position_embeddings = tf.slice(\n",
    "            self.position_embeddings,\n",
    "            (start_index, 0),\n",
    "            (sequence_length, feature_length),\n",
    "        )\n",
    "        return tf.broadcast_to(position_embeddings, tf.shape(inputs))\n",
    "\n",
    "# Your input data\n",
    "data = pd.read_csv(\n",
    "    \"../data/LJSpeech-1.1/metadata.csv\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"ID\", \"Text1\", \"Text2\"],\n",
    ")\n",
    "texts = data[\"Text1\"].to_list()\n",
    "ID = data[\"ID\"].to_list()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "num_classes = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X_data = pad_sequences(sequences, padding=\"post\", maxlen=30)\n",
    "\n",
    "# Reshape to (13100 * 30, 1)\n",
    "reshaped_input = X_data.reshape((-1, 1))\n",
    "\n",
    "# Positional embedding layer\n",
    "position_embedding_layer = PositionEmbedding(sequence_length=30)\n",
    "\n",
    "# Apply positional embedding\n",
    "embedded_output = position_embedding_layer(reshaped_input)\n",
    "\n",
    "# Reshape back to (13100, 30, embedding_dim)\n",
    "embedded_output = embedded_output.numpy().reshape((13100, 30, -1))\n",
    "\n",
    "# Display the shape of the output\n",
    "print(embedded_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example input data\n",
    "batch_size = 13100\n",
    "sequence_length = 30\n",
    "\n",
    "# Create a sample input tensor with sequential positions\n",
    "input_positions = np.tile(np.arange(sequence_length), (batch_size, 1))\n",
    "\n",
    "# Display the shape of the input tensor\n",
    "print(input_positions.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
