{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import nn\n",
    "import keras\n",
    "from tensorflow.keras.activations import softmax\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import Dense,LayerNormalization ,Flatten,MultiHeadAttention\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data = np.load(\"../../data/data_mfcc.npy\")\n",
    "X_data = np.load(\"../data/data_64_30.npy\")\n",
    "X_data = np.transpose(X_data, (0, 2, 1))\n",
    "# X_data=X_data[:100]\n",
    "print(X_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"../data/LJSpeech-1.1/metadata.csv\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"ID\", \"Text1\", \"Text2\"],\n",
    ")\n",
    "texts = data[\"Text1\"].to_list()\n",
    "ID = data[\"ID\"].to_list()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "num_classes = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "Y_data = pad_sequences(sequences, padding=\"post\", maxlen=30)\n",
    "# Y_data=Y_data[:100]\n",
    "print(num_classes)\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_length, embedding_dim=256):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.max_length = max_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pos_enc = self.add_positional_encoding()\n",
    "\n",
    "    def add_positional_encoding(self):\n",
    "        position = np.arange(self.max_length)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, self.embedding_dim, 2) * -(np.log(10000.0) / self.embedding_dim))\n",
    "        pos_enc = np.zeros((self.max_length, self.embedding_dim))\n",
    "        pos_enc[:, 0::2] = np.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = np.cos(position * div_term)\n",
    "        return tf.convert_to_tensor(pos_enc, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_length, embedding_dim=256):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.max_length = max_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pos_enc = self.add_positional_encoding()\n",
    "\n",
    "    def add_positional_encoding(self):\n",
    "        position = np.arange(self.max_length)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, self.embedding_dim, 2) * -(np.log(10000.0) / self.embedding_dim))\n",
    "        pos_enc = np.zeros((self.max_length, self.embedding_dim))\n",
    "        pos_enc[:, 0::2] = np.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = np.cos(position * div_term)\n",
    "        return tf.convert_to_tensor(pos_enc, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Expand dimensions to match the shape of the CNN output\n",
    "        pos_enc_expanded = tf.expand_dims(self.pos_enc, axis=0)\n",
    "        pos_enc_tiled = tf.tile(pos_enc_expanded, [tf.shape(inputs)[0], 1, 1])\n",
    "        return inputs + pos_enc_tiled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_shape, filters=64, kernel_size=3, **kwargs):\n",
    "        super(ConvolutionalLayer, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.conv1 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\", trainable=True)\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "\n",
    "        self.conv2 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\",trainable=True)\n",
    "        self.batch_norm2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.ReLU()\n",
    "\n",
    "    \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        conv1_out = self.relu1(self.batch_norm1(self.conv1(inputs), training=training))\n",
    "        conv2_out = self.relu2(self.batch_norm2(self.conv2(conv1_out), training=training))\n",
    "        print(\"CNN output shape is  \", conv2_out.shape)\n",
    "        return conv2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_self_attention_mask(sequence_length):\n",
    "    mask = np.tril(np.ones((sequence_length,sequence_length)))\n",
    "    mask[mask==0]=-np.inf\n",
    "    mask[mask==1]=0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask):\n",
    "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_qk = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_qk += mask\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_qk)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "#     def __init__(self, d_model, num_heads):\n",
    "#         super(MultiHeadAttention, self).__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = d_model // num_heads\n",
    "#         self.qkv_layer = tf.keras.layers.Dense(3 * d_model, use_bias=False)\n",
    "#         self.linear_layer = tf.keras.layers.Dense(d_model, activation='relu')\n",
    "\n",
    "#     def split_heads(self, x, batch_size):\n",
    "#         if len(x.shape) == 2:\n",
    "#             x = tf.expand_dims(tf.expand_dims(x, axis=0), axis=1)\n",
    "#         x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
    "#         return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "#     def call(self, x, mask):\n",
    "#         batch_size, _, _ = x.shape\n",
    "\n",
    "#         qkv = self.qkv_layer(x)\n",
    "#         q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "#         q = self.split_heads(q, batch_size)\n",
    "#         k = self.split_heads(k, batch_size)\n",
    "#         v = self.split_heads(v, batch_size)\n",
    "#         values, attention = scaled_dot_product(q, k, v, mask)\n",
    "#         print(\"test\")\n",
    "#         values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
    "#         values = tf.reshape(values, (batch_size, -1, self.num_heads * self.head_dim))\n",
    "#         out = self.linear_layer(values)\n",
    "#         print(\"MultiHeadAttention output shape is \",out.shape)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = tf.keras.layers.Dense(hidden)\n",
    "        self.linear2 = tf.keras.layers.Dense(d_model)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        print(\"Input shape for positonal encoding\",x.shape)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        print(\"output shape from postional encoding\",x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-5)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-5)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "\n",
    "    \n",
    "    def call(self, x, self_attention_mask, training=None):\n",
    "        residual_x = x\n",
    "        print(\"input shape for multihead \",x.shape)\n",
    "        x = self.attention(x, mask=self_attention_mask)\n",
    "        print(\"output shape for multihead \",x.shape)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.norm1(x + residual_x)\n",
    "        residual_x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.norm2(x + residual_x)\n",
    "        print(\"output shape for Encoderlayer \",x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length):\n",
    "        super(SequentialEncoder, self).__init__()\n",
    "        self.layers = [EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)]\n",
    "\n",
    "    \n",
    "    def call(self, x, training=True, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, training, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn_layer = ConvolutionalLayer(input_shape=(500,40))  # Adjust input shape\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        cnn_output = self.cnn_layer(inputs)\n",
    "        return cnn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_mask = create_self_attention_mask(30)\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.max_length = max_sequence_length\n",
    "        self.layers = SequentialEncoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length)\n",
    "\n",
    "    def call(self, x, training=True):  \n",
    "        # Assuming x is the output from the convolutional layer\n",
    "        print(\"Encoder input shape is\", x.shape)\n",
    "        x = self.layers(x, training=training, mask=self_attention_mask)  # Pass 'training' to the SequentialEncoder\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64\n",
    "max_length=30\n",
    "num_heads=8\n",
    "num_layers=1\n",
    "ffn_hidden=1024\n",
    "model = tf.keras.Sequential([CNN(),\n",
    "                             Encoder(d_model=d_model,ffn_hidden=ffn_hidden,num_heads=num_heads,drop_prob=0.1,num_layers=num_layers,max_sequence_length=max_length),\n",
    "                             Flatten(),\n",
    "                             Dense(30)])\n",
    "# model.build(input_shape=(None, 500, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_data and Y_data are your input features and labels\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_data, Y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.Adam(learning_rate=5)\n",
    "model.compile(optimizer=optim, loss='mean_squared_error',metrics=['mae'])\n",
    "# model.build(input_shape=(13100,500,40))  # Replace your_input_shape with the actual input shape\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,Y_train,epochs=1,validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_val)\n",
    "print(\"prediction \",prediction[0])\n",
    "print(\"actual \",Y_val[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
