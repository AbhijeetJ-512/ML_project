{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import nn\n",
    "import keras\n",
    "from tensorflow.keras.activations import softmax\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import Dense,LayerNormalization ,Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data = np.load(\"../../data/data_mfcc.npy\")\n",
    "X_data = np.load(\"../data/data_64_30.npy\")\n",
    "X_data = np.transpose(X_data, (0, 2, 1))\n",
    "# X_data=X_data[:100]\n",
    "print(X_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"../data/LJSpeech-1.1/metadata.csv\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"ID\", \"Text1\", \"Text2\"],\n",
    ")\n",
    "texts = data[\"Text1\"].to_list()\n",
    "ID = data[\"ID\"].to_list()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "# num_classes = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "Y_data = pad_sequences(sequences, padding=\"post\", maxlen=30)\n",
    "# Y_data=Y_data[:100]\n",
    "# print(num_classes)\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_self_attention_mask(sequence_length):\n",
    "    # Create a lower triangular matrix with ones\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((sequence_length, sequence_length)), -1, 0)\n",
    "    # Add a large negative value to the upper triangle\n",
    "    mask = mask * -1e9\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask):\n",
    "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_qk = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_qk += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer():\n",
    "    def __init__(self, input_shape, filters=32, kernel_size=3, **kwargs):\n",
    "        super(ConvolutionalLayer, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Extract the number of filters from the input shape\n",
    "        if isinstance(input_shape, tuple):\n",
    "            self.filters = input_shape[-1]\n",
    "\n",
    "        self.conv1 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\")\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "\n",
    "        self.conv2 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\")\n",
    "        self.batch_norm2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.ReLU()\n",
    "\n",
    "    \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        conv1_out = self.relu1(self.batch_norm1(self.conv1(inputs), training=training))\n",
    "        conv2_out = self.relu2(self.batch_norm2(self.conv2(conv1_out), training=training))\n",
    "        print(\"CNN output shape is \",conv2_out.shape)\n",
    "        return conv2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn_layer = ConvolutionalLayer(input_shape=(500,40))  # Adjust input shape\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        cnn_output = self.cnn_layer(inputs)\n",
    "        return cnn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = tf.keras.layers.Dense(3 * d_model, use_bias=False)\n",
    "        self.linear_layer = tf.keras.layers.Dense(d_model, activation='relu')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        if len(x.shape) == 2:\n",
    "            # Expand dimensions to simulate batch_size=1 and sequence_length=30\n",
    "            x = tf.expand_dims(tf.expand_dims(x, axis=0), axis=1)\n",
    "        x = tf.reshape(x, (64, -1, self.num_heads, self.head_dim))\n",
    "\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        print(\"MultiHeadAttention input shape\",x.shape)\n",
    "        batch_size, _, _ = x.shape\n",
    "\n",
    "        qkv = self.qkv_layer(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
    "        values = tf.reshape(values, (batch_size, -1, self.num_heads * self.head_dim))\n",
    "        out = self.linear_layer(values)\n",
    "        print(\"MultiHeadAttention output shape is \",out.shape)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = tf.keras.layers.Dense(hidden)\n",
    "        self.linear2 = tf.keras.layers.Dense(d_model)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        print(\"Input shape for positonal encoding\",x.shape)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        print(\"output shape from postional encoding\",x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-5)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-5)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "\n",
    "    \n",
    "    def call(self, x, self_attention_mask, training=None):\n",
    "        residual_x = x\n",
    "        x = self.attention(x, mask=self_attention_mask)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.norm1(x + residual_x)\n",
    "\n",
    "        residual_x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.norm2(x + residual_x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length):\n",
    "        super(SequentialEncoder, self).__init__()\n",
    "        self.layers = [EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)]\n",
    "\n",
    "    \n",
    "    def call(self, x, training=True, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, training, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.convolutional_layer = ConvolutionalLayer(input_shape=(max_sequence_length, 30))  # Assuming input shape (max_sequence_length, 20)\n",
    "        self.layers = SequentialEncoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length)\n",
    "    \n",
    "    \n",
    "    def call(self, x, self_attention_mask):\n",
    "        # Assuming x is the output from the convolutional layer\n",
    "        print(\"Encoder input shape is\",x.shape)\n",
    "        x = self.convolutional_layer.call(x)\n",
    "        x = self.layers(x, self_attention_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length)\n",
    "        self.final_layer = tf.keras.layers.Dense(units=max_sequence_length, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs['input_x']\n",
    "        y = inputs['input_y']\n",
    "        self_attention_mask_encoder = inputs['self_attention_mask_encoder']\n",
    "\n",
    "        print(\"Input shape:\", x.shape)\n",
    "\n",
    "        encoder_output = self.encoder(x, self_attention_mask_encoder)\n",
    "\n",
    "        print(\"Encoder output shape:\", encoder_output.shape)\n",
    "\n",
    "        output = self.final_layer(encoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64\n",
    "ffn_hidden = 120\n",
    "num_heads = 1\n",
    "drop_prob = 0.1\n",
    "num_layers = 2\n",
    "max_sequence_length = 30\n",
    "transformer_model = Transformer(\n",
    "    d_model=d_model,\n",
    "    ffn_hidden=ffn_hidden,\n",
    "    num_heads=num_heads,\n",
    "    drop_prob=drop_prob,\n",
    "    num_layers=num_layers,\n",
    "    max_sequence_length=max_sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_data and Y_data are your input features and labels\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_data, Y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# model.build(input_shape=(13100,500,40))  # Replace your_input_shape with the actual input shape\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming max_sequence_length is the actual length of your input sequences\n",
    "max_sequence_length = X_train.shape[1]\n",
    "\n",
    "# Create self-attention mask for encoder\n",
    "self_attention_mask_encoder = create_self_attention_mask(max_sequence_length)\n",
    "self_attention_mask_encoder = tf.expand_dims(self_attention_mask_encoder, axis=0)  # Add batch dimension\n",
    "self_attention_mask_encoder = tf.tile(self_attention_mask_encoder, [X_train.shape[0], 1, 1])  # Tile to match the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.fit(\n",
    "    {\n",
    "        'input_x': X_train,\n",
    "        'input_y': Y_train,\n",
    "        'self_attention_mask_encoder': self_attention_mask_encoder,\n",
    "    },\n",
    "    Y_train,\n",
    "    epochs=1,\n",
    "    batch_size=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_val)\n",
    "print(\"prediction \",prediction[0])\n",
    "print(\"actual \",Y_val[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
