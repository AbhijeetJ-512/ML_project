{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-07 17:45:22.115556: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-07 17:45:22.142285: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-07 17:45:22.142309: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-07 17:45:22.142922: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-07 17:45:22.147041: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-07 17:45:22.657801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import nn\n",
    "import keras\n",
    "from tensorflow.keras.activations import softmax\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import Dense,LayerNormalization ,Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13100, 30, 64)\n"
     ]
    }
   ],
   "source": [
    "# X_data = np.load(\"../../data/data_mfcc.npy\")\n",
    "X_data = np.load(\"../data/data_64_30.npy\")\n",
    "X_data = np.transpose(X_data, (0, 2, 1))\n",
    "# X_data=X_data[:100]\n",
    "print(X_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13100, 30)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    \"../data/LJSpeech-1.1/metadata.csv\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"ID\", \"Text1\", \"Text2\"],\n",
    ")\n",
    "texts = data[\"Text1\"].to_list()\n",
    "ID = data[\"ID\"].to_list()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "# num_classes = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "Y_data = pad_sequences(sequences, padding=\"post\", maxlen=30)\n",
    "# Y_data=Y_data[:100]\n",
    "# print(num_classes)\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_self_attention_mask(sequence_length):\n",
    "    # Create a lower triangular matrix with ones\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((sequence_length, sequence_length)), -1, 0)\n",
    "    # Add a large negative value to the upper triangle\n",
    "    mask = mask * -1e9\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask):\n",
    "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_qk = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_qk += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer():\n",
    "    def __init__(self, input_shape, filters=32, kernel_size=3, **kwargs):\n",
    "        super(ConvolutionalLayer, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Extract the number of filters from the input shape\n",
    "        if isinstance(input_shape, tuple):\n",
    "            self.filters = input_shape[-1]\n",
    "\n",
    "        self.conv1 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\")\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "\n",
    "        self.conv2 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\")\n",
    "        self.batch_norm2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.ReLU()\n",
    "\n",
    "    \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        conv1_out = self.relu1(self.batch_norm1(self.conv1(inputs), training=training))\n",
    "        conv2_out = self.relu2(self.batch_norm2(self.conv2(conv1_out), training=training))\n",
    "        print(\"CNN output shape is \",conv2_out.shape)\n",
    "        return conv2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn_layer = ConvolutionalLayer(input_shape=(500,40))  # Adjust input shape\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        cnn_output = self.cnn_layer(inputs)\n",
    "        return cnn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = tf.keras.layers.Dense(3 * d_model, use_bias=False)\n",
    "        self.linear_layer = tf.keras.layers.Dense(d_model, activation='relu')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        if len(x.shape) == 2:\n",
    "            # Expand dimensions to simulate batch_size=1 and sequence_length=30\n",
    "            x = tf.expand_dims(tf.expand_dims(x, axis=0), axis=1)\n",
    "        x = tf.reshape(x, (64, -1, self.num_heads, self.head_dim))\n",
    "\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        print(\"MultiHeadAttention input shape\",x.shape)\n",
    "        batch_size, _, _ = x.shape\n",
    "\n",
    "        qkv = self.qkv_layer(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
    "        values = tf.reshape(values, (batch_size, -1, self.num_heads * self.head_dim))\n",
    "        out = self.linear_layer(values)\n",
    "        print(\"MultiHeadAttention output shape is \",out.shape)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = tf.keras.layers.Dense(hidden)\n",
    "        self.linear2 = tf.keras.layers.Dense(d_model)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        print(\"Input shape for positonal encoding\",x.shape)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        print(\"output shape from postional encoding\",x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-5)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-5)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate=drop_prob)\n",
    "\n",
    "    \n",
    "    def call(self, x, self_attention_mask, training=None):\n",
    "        residual_x = x\n",
    "        x = self.attention(x, mask=self_attention_mask)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.norm1(x + residual_x)\n",
    "\n",
    "        residual_x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.norm2(x + residual_x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length):\n",
    "        super(SequentialEncoder, self).__init__()\n",
    "        self.layers = [EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)]\n",
    "\n",
    "    \n",
    "    def call(self, x, training=True, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, training, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.convolutional_layer = ConvolutionalLayer(input_shape=(max_sequence_length, 30))  # Assuming input shape (max_sequence_length, 20)\n",
    "        self.layers = SequentialEncoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length)\n",
    "    \n",
    "    \n",
    "    def call(self, x, self_attention_mask):\n",
    "        # Assuming x is the output from the convolutional layer\n",
    "        print(\"Encoder input shape is\",x.shape)\n",
    "        x = self.convolutional_layer.call(x)\n",
    "        x = self.layers(x, self_attention_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length)\n",
    "        self.final_layer = tf.keras.layers.Dense(units=max_sequence_length, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs['input_x']\n",
    "        y = inputs['input_y']\n",
    "        self_attention_mask_encoder = inputs['self_attention_mask_encoder']\n",
    "\n",
    "        print(\"Input shape:\", x.shape)\n",
    "\n",
    "        encoder_output = self.encoder(x, self_attention_mask_encoder)\n",
    "\n",
    "        print(\"Encoder output shape:\", encoder_output.shape)\n",
    "\n",
    "        output = self.final_layer(encoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-07 17:45:24.654182: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-12-07 17:45:24.654207: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: Pc\n",
      "2023-12-07 17:45:24.654210: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: Pc\n",
      "2023-12-07 17:45:24.654320: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 535.129.3\n",
      "2023-12-07 17:45:24.654331: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 535.129.3\n",
      "2023-12-07 17:45:24.654333: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 535.129.3\n"
     ]
    }
   ],
   "source": [
    "d_model = 64\n",
    "ffn_hidden = 120\n",
    "num_heads = 1\n",
    "drop_prob = 0.1\n",
    "num_layers = 2\n",
    "max_sequence_length = 30\n",
    "transformer_model = Transformer(\n",
    "    d_model=d_model,\n",
    "    ffn_hidden=ffn_hidden,\n",
    "    num_heads=num_heads,\n",
    "    drop_prob=drop_prob,\n",
    "    num_layers=num_layers,\n",
    "    max_sequence_length=max_sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10480, 30, 64)\n",
      "(2620, 30, 64)\n",
      "(10480, 30)\n",
      "(2620, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_data and Y_data are your input features and labels\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_data, Y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# model.build(input_shape=(13100,500,40))  # Replace your_input_shape with the actual input shape\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming max_sequence_length is the actual length of your input sequences\n",
    "max_sequence_length = X_train.shape[1]\n",
    "\n",
    "# Create self-attention mask for encoder\n",
    "self_attention_mask_encoder = create_self_attention_mask(max_sequence_length)\n",
    "self_attention_mask_encoder = tf.expand_dims(self_attention_mask_encoder, axis=0)  # Add batch dimension\n",
    "self_attention_mask_encoder = tf.tile(self_attention_mask_encoder, [X_train.shape[0], 1, 1])  # Tile to match the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (None, 30, 64)\n",
      "Encoder input shape is (None, 30, 64)\n",
      "CNN output shape is  (None, 30, 30)\n",
      "MultiHeadAttention input shape (None, 30, 30)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_fileiv1nijj0.py\", line 14, in tf__call\n        encoder_output = ag__.converted_call(ag__.ld(self).encoder, (ag__.ld(x), ag__.ld(self_attention_mask_encoder)), None, fscope)\n    File \"/tmp/__autograph_generated_file9oddwbze.py\", line 12, in tf__call\n        x = ag__.converted_call(ag__.ld(self).layers, (ag__.ld(x), ag__.ld(self_attention_mask)), None, fscope)\n    File \"/tmp/__autograph_generated_fileqvp5d1r7.py\", line 23, in tf__call\n        ag__.for_stmt(ag__.ld(self).layers, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'layer'})\n    File \"/tmp/__autograph_generated_fileqvp5d1r7.py\", line 21, in loop_body\n        x = ag__.converted_call(ag__.ld(layer), (ag__.ld(x), ag__.ld(training), ag__.ld(mask)), None, fscope)\n    File \"/tmp/__autograph_generated_fileab2kfv2h.py\", line 11, in tf__call\n        x = ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x),), dict(mask=ag__.ld(self_attention_mask)), fscope)\n    File \"/tmp/__autograph_generated_file8rjxpjl2.py\", line 19, in tf__call\n        values = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(values), (ag__.ld(batch_size), -1, ag__.ld(self).num_heads * ag__.ld(self).head_dim)), None, fscope)\n\n    TypeError: Exception encountered when calling layer 'transformer' (type Transformer).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_8520/704554109.py\", line 21, in call  *\n            encoder_output = self.encoder(x, self_attention_mask_encoder)\n        File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_file9oddwbze.py\", line 12, in tf__call\n            x = ag__.converted_call(ag__.ld(self).layers, (ag__.ld(x), ag__.ld(self_attention_mask)), None, fscope)\n        File \"/tmp/__autograph_generated_fileqvp5d1r7.py\", line 23, in tf__call\n            ag__.for_stmt(ag__.ld(self).layers, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'layer'})\n        File \"/tmp/__autograph_generated_fileqvp5d1r7.py\", line 21, in loop_body\n            x = ag__.converted_call(ag__.ld(layer), (ag__.ld(x), ag__.ld(training), ag__.ld(mask)), None, fscope)\n        File \"/tmp/__autograph_generated_fileab2kfv2h.py\", line 11, in tf__call\n            x = ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x),), dict(mask=ag__.ld(self_attention_mask)), fscope)\n        File \"/tmp/__autograph_generated_file8rjxpjl2.py\", line 19, in tf__call\n            values = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(values), (ag__.ld(batch_size), -1, ag__.ld(self).num_heads * ag__.ld(self).head_dim)), None, fscope)\n    \n        TypeError: Exception encountered when calling layer 'encoder' (type Encoder).\n        \n        in user code:\n        \n            File \"/tmp/ipykernel_8520/589918954.py\", line 18, in call  *\n                x = self.layers(x, self_attention_mask)\n            File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/tmp/__autograph_generated_fileqvp5d1r7.py\", line 23, in tf__call\n                ag__.for_stmt(ag__.ld(self).layers, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'layer'})\n            File \"/tmp/__autograph_generated_fileqvp5d1r7.py\", line 21, in loop_body\n                x = ag__.converted_call(ag__.ld(layer), (ag__.ld(x), ag__.ld(training), ag__.ld(mask)), None, fscope)\n            File \"/tmp/__autograph_generated_fileab2kfv2h.py\", line 11, in tf__call\n                x = ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x),), dict(mask=ag__.ld(self_attention_mask)), fscope)\n            File \"/tmp/__autograph_generated_file8rjxpjl2.py\", line 19, in tf__call\n                values = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(values), (ag__.ld(batch_size), -1, ag__.ld(self).num_heads * ag__.ld(self).head_dim)), None, fscope)\n        \n            TypeError: Exception encountered when calling layer 'sequential_encoder' (type SequentialEncoder).\n            \n            in user code:\n            \n                File \"/tmp/ipykernel_8520/4062382463.py\", line 9, in call  *\n                    x = layer(x, training, mask)\n                File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"/tmp/__autograph_generated_fileab2kfv2h.py\", line 11, in tf__call\n                    x = ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x),), dict(mask=ag__.ld(self_attention_mask)), fscope)\n                File \"/tmp/__autograph_generated_file8rjxpjl2.py\", line 19, in tf__call\n                    values = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(values), (ag__.ld(batch_size), -1, ag__.ld(self).num_heads * ag__.ld(self).head_dim)), None, fscope)\n            \n                TypeError: Exception encountered when calling layer 'encoder_layer' (type EncoderLayer).\n                \n                in user code:\n                \n                    File \"/tmp/ipykernel_8520/2597745548.py\", line 14, in call  *\n                        x = self.attention(x, mask=self_attention_mask)\n                    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                        raise e.with_traceback(filtered_tb) from None\n                    File \"/tmp/__autograph_generated_file8rjxpjl2.py\", line 19, in tf__call\n                        values = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(values), (ag__.ld(batch_size), -1, ag__.ld(self).num_heads * ag__.ld(self).head_dim)), None, fscope)\n                \n                    TypeError: Exception encountered when calling layer 'multi_head_attention' (type MultiHeadAttention).\n                    \n                    in user code:\n                    \n                        File \"/tmp/ipykernel_8520/1544935900.py\", line 31, in call  *\n                            values = tf.reshape(values, (batch_size, -1, self.num_heads * self.head_dim))\n                    \n                        TypeError: Failed to convert elements of (None, -1, 64) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n                    \n                    \n                    Call arguments received by layer 'multi_head_attention' (type MultiHeadAttention):\n                      • x=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n                      • mask=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n                \n                \n                Call arguments received by layer 'encoder_layer' (type EncoderLayer):\n                  • x=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n                  • self_attention_mask=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n                  • training=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n            \n            \n            Call arguments received by layer 'sequential_encoder' (type SequentialEncoder):\n              • x=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n              • training=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n              • mask=None\n        \n        \n        Call arguments received by layer 'encoder' (type Encoder):\n          • x=tf.Tensor(shape=(None, 30, 64), dtype=float32)\n          • self_attention_mask=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n    \n    \n    Call arguments received by layer 'transformer' (type Transformer):\n      • inputs={'input_x': 'tf.Tensor(shape=(None, 30, 64), dtype=float32)', 'input_y': 'tf.Tensor(shape=(None, 30), dtype=int32)', 'self_attention_mask_encoder': 'tf.Tensor(shape=(None, 30, 30), dtype=float32)'}\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/abhi/ML/ML_project/final/CNN_and_encoder.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/CNN_and_encoder.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m transformer_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/CNN_and_encoder.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     {\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/CNN_and_encoder.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39minput_x\u001b[39;49m\u001b[39m'\u001b[39;49m: X_train,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/CNN_and_encoder.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39minput_y\u001b[39;49m\u001b[39m'\u001b[39;49m: Y_train,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/CNN_and_encoder.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mself_attention_mask_encoder\u001b[39;49m\u001b[39m'\u001b[39;49m: self_attention_mask_encoder,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/CNN_and_encoder.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     },\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/CNN_and_encoder.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     Y_train,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/CNN_and_encoder.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/CNN_and_encoder.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhi/ML/ML_project/final/CNN_and_encoder.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileb8r2ozcs.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileiv1nijj0.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     12\u001b[0m self_attention_mask_encoder \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(inputs)[\u001b[39m'\u001b[39m\u001b[39mself_attention_mask_encoder\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39m'\u001b[39m\u001b[39mInput shape:\u001b[39m\u001b[39m'\u001b[39m, ag__\u001b[39m.\u001b[39mld(x)\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 14\u001b[0m encoder_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mencoder, (ag__\u001b[39m.\u001b[39mld(x), ag__\u001b[39m.\u001b[39mld(self_attention_mask_encoder)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     15\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39m'\u001b[39m\u001b[39mEncoder output shape:\u001b[39m\u001b[39m'\u001b[39m, ag__\u001b[39m.\u001b[39mld(encoder_output)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     16\u001b[0m output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfinal_layer, (ag__\u001b[39m.\u001b[39mld(encoder_output),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file9oddwbze.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, x, self_attention_mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39m'\u001b[39m\u001b[39mEncoder input shape is\u001b[39m\u001b[39m'\u001b[39m, ag__\u001b[39m.\u001b[39mld(x)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     11\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mconvolutional_layer\u001b[39m.\u001b[39mcall, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 12\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mlayers, (ag__\u001b[39m.\u001b[39mld(x), ag__\u001b[39m.\u001b[39mld(self_attention_mask)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileqvp5d1r7.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, x, training, mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m     x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(layer), (ag__\u001b[39m.\u001b[39mld(x), ag__\u001b[39m.\u001b[39mld(training), ag__\u001b[39m.\u001b[39mld(mask)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     22\u001b[0m layer \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mlayer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m ag__\u001b[39m.\u001b[39mfor_stmt(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mlayers, \u001b[39mNone\u001b[39;00m, loop_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m,), {\u001b[39m'\u001b[39m\u001b[39miterate_names\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mlayer\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileqvp5d1r7.py:21\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mnonlocal\u001b[39;00m x\n\u001b[1;32m     20\u001b[0m layer \u001b[39m=\u001b[39m itr\n\u001b[0;32m---> 21\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(layer), (ag__\u001b[39m.\u001b[39;49mld(x), ag__\u001b[39m.\u001b[39;49mld(training), ag__\u001b[39m.\u001b[39;49mld(mask)), \u001b[39mNone\u001b[39;49;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileab2kfv2h.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, x, self_attention_mask, training)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m residual_x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(x)\n\u001b[0;32m---> 11\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mattention, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mdict\u001b[39m(mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(self_attention_mask)), fscope)\n\u001b[1;32m     12\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdropout1, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     13\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mnorm1, (ag__\u001b[39m.\u001b[39mld(x) \u001b[39m+\u001b[39m ag__\u001b[39m.\u001b[39mld(residual_x),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file8rjxpjl2.py:19\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     17\u001b[0m (values, attention) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(scaled_dot_product), (ag__\u001b[39m.\u001b[39mld(q), ag__\u001b[39m.\u001b[39mld(k), ag__\u001b[39m.\u001b[39mld(v), ag__\u001b[39m.\u001b[39mld(mask)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     18\u001b[0m values \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mtranspose, (ag__\u001b[39m.\u001b[39mld(values),), \u001b[39mdict\u001b[39m(perm\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m]), fscope)\n\u001b[0;32m---> 19\u001b[0m values \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreshape, (ag__\u001b[39m.\u001b[39mld(values), (ag__\u001b[39m.\u001b[39mld(batch_size), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mnum_heads \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mhead_dim)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     20\u001b[0m out \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mlinear_layer, (ag__\u001b[39m.\u001b[39mld(values),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     21\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39m'\u001b[39m\u001b[39mMultiHeadAttention output shape is \u001b[39m\u001b[39m'\u001b[39m, ag__\u001b[39m.\u001b[39mld(out)\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_fileiv1nijj0.py\", line 14, in tf__call\n        encoder_output = ag__.converted_call(ag__.ld(self).encoder, (ag__.ld(x), ag__.ld(self_attention_mask_encoder)), None, fscope)\n    File \"/tmp/__autograph_generated_file9oddwbze.py\", line 12, in tf__call\n        x = ag__.converted_call(ag__.ld(self).layers, (ag__.ld(x), ag__.ld(self_attention_mask)), None, fscope)\n    File \"/tmp/__autograph_generated_fileqvp5d1r7.py\", line 23, in tf__call\n        ag__.for_stmt(ag__.ld(self).layers, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'layer'})\n    File \"/tmp/__autograph_generated_fileqvp5d1r7.py\", line 21, in loop_body\n        x = ag__.converted_call(ag__.ld(layer), (ag__.ld(x), ag__.ld(training), ag__.ld(mask)), None, fscope)\n    File \"/tmp/__autograph_generated_fileab2kfv2h.py\", line 11, in tf__call\n        x = ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x),), dict(mask=ag__.ld(self_attention_mask)), fscope)\n    File \"/tmp/__autograph_generated_file8rjxpjl2.py\", line 19, in tf__call\n        values = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(values), (ag__.ld(batch_size), -1, ag__.ld(self).num_heads * ag__.ld(self).head_dim)), None, fscope)\n\n    TypeError: Exception encountered when calling layer 'transformer' (type Transformer).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_8520/704554109.py\", line 21, in call  *\n            encoder_output = self.encoder(x, self_attention_mask_encoder)\n        File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_file9oddwbze.py\", line 12, in tf__call\n            x = ag__.converted_call(ag__.ld(self).layers, (ag__.ld(x), ag__.ld(self_attention_mask)), None, fscope)\n        File \"/tmp/__autograph_generated_fileqvp5d1r7.py\", line 23, in tf__call\n            ag__.for_stmt(ag__.ld(self).layers, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'layer'})\n        File \"/tmp/__autograph_generated_fileqvp5d1r7.py\", line 21, in loop_body\n            x = ag__.converted_call(ag__.ld(layer), (ag__.ld(x), ag__.ld(training), ag__.ld(mask)), None, fscope)\n        File \"/tmp/__autograph_generated_fileab2kfv2h.py\", line 11, in tf__call\n            x = ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x),), dict(mask=ag__.ld(self_attention_mask)), fscope)\n        File \"/tmp/__autograph_generated_file8rjxpjl2.py\", line 19, in tf__call\n            values = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(values), (ag__.ld(batch_size), -1, ag__.ld(self).num_heads * ag__.ld(self).head_dim)), None, fscope)\n    \n        TypeError: Exception encountered when calling layer 'encoder' (type Encoder).\n        \n        in user code:\n        \n            File \"/tmp/ipykernel_8520/589918954.py\", line 18, in call  *\n                x = self.layers(x, self_attention_mask)\n            File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/tmp/__autograph_generated_fileqvp5d1r7.py\", line 23, in tf__call\n                ag__.for_stmt(ag__.ld(self).layers, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'layer'})\n            File \"/tmp/__autograph_generated_fileqvp5d1r7.py\", line 21, in loop_body\n                x = ag__.converted_call(ag__.ld(layer), (ag__.ld(x), ag__.ld(training), ag__.ld(mask)), None, fscope)\n            File \"/tmp/__autograph_generated_fileab2kfv2h.py\", line 11, in tf__call\n                x = ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x),), dict(mask=ag__.ld(self_attention_mask)), fscope)\n            File \"/tmp/__autograph_generated_file8rjxpjl2.py\", line 19, in tf__call\n                values = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(values), (ag__.ld(batch_size), -1, ag__.ld(self).num_heads * ag__.ld(self).head_dim)), None, fscope)\n        \n            TypeError: Exception encountered when calling layer 'sequential_encoder' (type SequentialEncoder).\n            \n            in user code:\n            \n                File \"/tmp/ipykernel_8520/4062382463.py\", line 9, in call  *\n                    x = layer(x, training, mask)\n                File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"/tmp/__autograph_generated_fileab2kfv2h.py\", line 11, in tf__call\n                    x = ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x),), dict(mask=ag__.ld(self_attention_mask)), fscope)\n                File \"/tmp/__autograph_generated_file8rjxpjl2.py\", line 19, in tf__call\n                    values = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(values), (ag__.ld(batch_size), -1, ag__.ld(self).num_heads * ag__.ld(self).head_dim)), None, fscope)\n            \n                TypeError: Exception encountered when calling layer 'encoder_layer' (type EncoderLayer).\n                \n                in user code:\n                \n                    File \"/tmp/ipykernel_8520/2597745548.py\", line 14, in call  *\n                        x = self.attention(x, mask=self_attention_mask)\n                    File \"/home/abhi/anaconda3/envs/test/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                        raise e.with_traceback(filtered_tb) from None\n                    File \"/tmp/__autograph_generated_file8rjxpjl2.py\", line 19, in tf__call\n                        values = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(values), (ag__.ld(batch_size), -1, ag__.ld(self).num_heads * ag__.ld(self).head_dim)), None, fscope)\n                \n                    TypeError: Exception encountered when calling layer 'multi_head_attention' (type MultiHeadAttention).\n                    \n                    in user code:\n                    \n                        File \"/tmp/ipykernel_8520/1544935900.py\", line 31, in call  *\n                            values = tf.reshape(values, (batch_size, -1, self.num_heads * self.head_dim))\n                    \n                        TypeError: Failed to convert elements of (None, -1, 64) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n                    \n                    \n                    Call arguments received by layer 'multi_head_attention' (type MultiHeadAttention):\n                      • x=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n                      • mask=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n                \n                \n                Call arguments received by layer 'encoder_layer' (type EncoderLayer):\n                  • x=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n                  • self_attention_mask=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n                  • training=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n            \n            \n            Call arguments received by layer 'sequential_encoder' (type SequentialEncoder):\n              • x=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n              • training=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n              • mask=None\n        \n        \n        Call arguments received by layer 'encoder' (type Encoder):\n          • x=tf.Tensor(shape=(None, 30, 64), dtype=float32)\n          • self_attention_mask=tf.Tensor(shape=(None, 30, 30), dtype=float32)\n    \n    \n    Call arguments received by layer 'transformer' (type Transformer):\n      • inputs={'input_x': 'tf.Tensor(shape=(None, 30, 64), dtype=float32)', 'input_y': 'tf.Tensor(shape=(None, 30), dtype=int32)', 'self_attention_mask_encoder': 'tf.Tensor(shape=(None, 30, 30), dtype=float32)'}\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "transformer_model.fit(\n",
    "    {\n",
    "        'input_x': X_train,\n",
    "        'input_y': Y_train,\n",
    "        'self_attention_mask_encoder': self_attention_mask_encoder,\n",
    "    },\n",
    "    Y_train,\n",
    "    epochs=1,\n",
    "    batch_size=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_val)\n",
    "print(\"prediction \",prediction[0])\n",
    "print(\"actual \",Y_val[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
