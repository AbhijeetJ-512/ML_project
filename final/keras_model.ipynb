{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.load(\"../data/data_mfcc.npy\")\n",
    "X_data = np.transpose(X_data, (0, 2, 1))\n",
    "data = pd.read_csv(\n",
    "    \"../data/LJSpeech-1.1/metadata.csv\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"ID\", \"Text1\", \"Text2\"],\n",
    ")\n",
    "texts = data[\"Text1\"].to_list()\n",
    "ID = data[\"ID\"].to_list()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "num_classes = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "Y_data = pad_sequences(sequences, padding=\"post\", maxlen=30)\n",
    "print(num_classes)\n",
    "print(Y_data.shape)\n",
    "Y_data = Y_data.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer():\n",
    "    def __init__(self, input_shape, filters=32, kernel_size=3, **kwargs):\n",
    "        super(ConvolutionalLayer, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Extract the number of filters from the input shape\n",
    "        if isinstance(input_shape, tuple):\n",
    "            self.filters = input_shape[-1]\n",
    "\n",
    "        self.conv1 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\")\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "\n",
    "        self.conv2 = layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, padding=\"same\")\n",
    "        self.batch_norm2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.ReLU()\n",
    "\n",
    "        self.global_avg_pooling = layers.GlobalAveragePooling1D()\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        conv1_out = self.relu1(self.batch_norm1(self.conv1(inputs), training=training))\n",
    "        conv2_out = self.relu2(self.batch_norm2(self.conv2(conv1_out), training=training))\n",
    "        gap_out = self.global_avg_pooling(conv2_out)\n",
    "        return gap_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncodingLayer(layers.Layer):\n",
    "#     def __init__(self, position, model_dim, **kwargs):\n",
    "#         super(PositionalEncodingLayer, self).__init__(**kwargs)\n",
    "#         self.position = position\n",
    "#         self.model_dim = model_dim\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         super(PositionalEncodingLayer, self).build(input_shape)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         position = tf.range(start=0, limit=self.position, delta=1, dtype=tf.float32)\n",
    "#         position = position / tf.cast(self.position, dtype=tf.float32)\n",
    "\n",
    "#         inputs *= tf.math.sqrt(tf.cast(self.model_dim, dtype=tf.float32))\n",
    "#         position_encoding = tf.expand_dims(position, 1) - tf.range(\n",
    "#             start=0, limit=self.model_dim, delta=2, dtype=tf.float32\n",
    "#         ) / tf.cast(self.model_dim, dtype=tf.float32)\n",
    "#         position_encoding = tf.expand_dims(position_encoding, 0)\n",
    "\n",
    "#         return inputs + position_encoding\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return input_shape\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace PositionalEncodingLayer with Embedding layer for positional embedding(input)\n",
    "class PositionalEmbeddingLayer(layers.Layer):\n",
    "    def __init__(self, position, model_dim):\n",
    "        super(PositionalEmbeddingLayer, self).__init__()\n",
    "        self.positional_embedding = layers.Embedding(\n",
    "            input_dim=position, output_dim=model_dim\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        positions = tf.range(tf.shape(inputs)[1], dtype=tf.float32)\n",
    "        return inputs + self.positional_embedding(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Positional embedding layer (output)\n",
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, **kwargs):\n",
    "        super(PositionEmbedding, self).__init__(**kwargs)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_length = input_shape[-1]\n",
    "        self.position_embeddings = self.add_weight(\n",
    "            shape=(self.sequence_length, feature_length),\n",
    "            initializer=tf.keras.initializers.RandomNormal(),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        start_index = 0\n",
    "        sequence_length = tf.shape(self.position_embeddings)[0]\n",
    "        feature_length = tf.shape(self.position_embeddings)[-1]\n",
    "\n",
    "        position_embeddings = tf.tile(\n",
    "            tf.slice(\n",
    "                self.position_embeddings,\n",
    "                (start_index, 0),\n",
    "                (sequence_length, feature_length),\n",
    "            ),\n",
    "            [tf.shape(inputs)[0] // sequence_length + 1, 1],\n",
    "        )\n",
    "\n",
    "        return tf.slice(position_embeddings, (0, 0), tf.shape(inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(keras.layers.Layer):\n",
    "    def __init__(self, head_size, num_heads, ff_dim, dropout, num_blocks=1, **kwargs):\n",
    "        super(TransformerEncoderBlock, self).__init__(**kwargs)\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout = dropout\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        # Create a list of Transformer encoder blocks\n",
    "        self.encoder_blocks = [self.build_encoder_block() for _ in range(num_blocks)]\n",
    "\n",
    "    def build_encoder_block(self):\n",
    "        return TransformerEncoderBlockSingle(\n",
    "            head_size=self.head_size,\n",
    "            num_heads=self.num_heads,\n",
    "            ff_dim=self.ff_dim,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(TransformerEncoderBlock, self).build(input_shape)\n",
    "        # Ensure that the encoder blocks are built\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            encoder_block.build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Stack multiple Transformer encoder blocks\n",
    "        x = inputs\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlockSingle(keras.layers.Layer):\n",
    "    def __init__(self, head_size, num_heads, ff_dim, dropout, **kwargs):\n",
    "        super(TransformerEncoderBlockSingle, self).__init__(**kwargs)\n",
    "\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Multi-head self-attention layer\n",
    "        self.self_attention = layers.MultiHeadAttention(\n",
    "            key_dim=self.head_size,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "        # Feed Forward Part\n",
    "        self.ffn_hidden = layers.Dense(self.ff_dim, activation=\"relu\")\n",
    "        self.ffn_output = layers.Dense(self.head_size)  # Use head_size here\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout1 = layers.Dropout(self.dropout)\n",
    "        self.dropout2 = layers.Dropout(self.dropout)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(TransformerEncoderBlockSingle, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Attention and Normalization\n",
    "        self_attn_output = self.self_attention(inputs, inputs)\n",
    "        self_attn_output = self.dropout1(self_attn_output)\n",
    "        x = self.layernorm1(self_attn_output + inputs)\n",
    "\n",
    "        # Feed Forward Part\n",
    "        ffn_output = self.ffn_output(self.ffn_hidden(x))\n",
    "        x = self.layernorm2(ffn_output + x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(keras.layers.Layer):\n",
    "    def __init__(self, num_blocks, head_size, num_heads, ff_dim, dropout, output_dim, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.num_blocks = num_blocks\n",
    "        self.decoder_blocks = [\n",
    "            TransformerDecoderBlock(\n",
    "                head_size=head_size,\n",
    "                num_heads=num_heads,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            for _ in range(num_blocks)\n",
    "        ]\n",
    "        self.output_dim = output_dim  # Add output_dim to the class attributes\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None, training=None):\n",
    "        x = inputs\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            x = decoder_block(x, encoder_outputs, mask=mask, training=training)\n",
    "        # Add a dense layer to produce the final output with the specified output_dim\n",
    "        x = layers.Dense(self.output_dim, activation=\"softmax\")(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TransformerDecoderBlock(keras.layers.Layer):\n",
    "    def __init__(self, head_size, num_heads, ff_dim, dropout, **kwargs):\n",
    "        super(TransformerDecoderBlock, self).__init__(**kwargs)\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Self Attention Layer\n",
    "        self.self_attention = layers.MultiHeadAttention(\n",
    "            key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Encoder-Decoder Attention Layer\n",
    "        self.encoder_decoder_attention = layers.MultiHeadAttention(\n",
    "            key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Feed Forward Part\n",
    "        self.ffn_hidden = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.ffn_dropout = layers.Dropout(dropout)\n",
    "        self.ffn_output = layers.Dense(head_size)\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "        self.dropout3 = layers.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None, training=None):\n",
    "        # Self Attention\n",
    "        self_attn_output = self.self_attention(\n",
    "\n",
    "            inputs, inputs, attention_mask=mask, return_attention_scores=False\n",
    "        )\n",
    "\n",
    "\n",
    "        self_attn_output = self.dropout1(self_attn_output, training=training)\n",
    "\n",
    "        attn_output1 = self.layernorm1(self_attn_output + inputs)\n",
    "\n",
    "\n",
    "        # Encoder-Decoder Attention\n",
    "        enc_dec_attn_output = self.encoder_decoder_attention(\n",
    "\n",
    "            attn_output1, encoder_outputs, return_attention_scores=False\n",
    "        )\n",
    "\n",
    "\n",
    "        enc_dec_attn_output = self.dropout2(enc_dec_attn_output, training=training)\n",
    "\n",
    "        attn_output2 = self.layernorm2(enc_dec_attn_output + attn_output1)\n",
    "\n",
    "\n",
    "        # Feed Forward\n",
    "        ffn_output = self.ffn_output(\n",
    "\n",
    "            self.ffn_dropout(self.ffn_hidden(attn_output2), training=training)\n",
    "        )\n",
    "\n",
    "\n",
    "        ffn_output = self.layernorm3(ffn_output + attn_output2)\n",
    "\n",
    "        return ffn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_data.shape[1], X_data.shape[2])\n",
    "print(input_shape)\n",
    "\n",
    "\n",
    "# conv_output = Convolutional_Layer(input_shape=(input_shape))\n",
    "\n",
    "# # Apply positional encoding to the tensor output of the convolutional layer\n",
    "# positional_encoding = PositionalEncodingLayer(position=500, model_dim=64)(conv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransformerModel(keras.Model):\n",
    "    def __init__(self, input_shape, position, model_dim, num_blocks):\n",
    "        super(MyTransformerModel, self).__init__()\n",
    "\n",
    "        # Define the Convolutional Layer\n",
    "        self.convolutional_layer = ConvolutionalLayer(input_shape=input_shape)\n",
    "\n",
    "        # Define the Positional Embedding Layer\n",
    "        self.positional_embedding = PositionalEmbeddingLayer(position, model_dim)\n",
    "\n",
    "        # Define the Transformer Encoder Block\n",
    "        self.transformer_encoder = TransformerEncoderBlock(\n",
    "            head_size=32, num_heads=4, ff_dim=128, dropout=0.1, num_blocks=num_blocks\n",
    "        )\n",
    "\n",
    "        # Define the Transformer Decoder Block\n",
    "        self.transformer_decoder = TransformerDecoder(\n",
    "            num_blocks=num_blocks,\n",
    "            head_size=32,\n",
    "            num_heads=4,\n",
    "            ff_dim=128,\n",
    "            dropout=0.1,\n",
    "            output_dim=num_classes,  # Use num_classes instead of Y_data.shape[1]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # Missing input shape\n",
    "        conv_output = self.convolutional_layer(inputs)\n",
    "\n",
    "        # Apply Positional Embedding\n",
    "        positional_embedding_output = self.positional_embedding(tf.cast(Y_data, dtype=tf.float32))\n",
    "\n",
    "        # Apply Transformer Encoder\n",
    "        transformer_encoder_output = self.transformer_encoder(conv_output)\n",
    "\n",
    "        # Apply Transformer Decoder\n",
    "        transformer_output_dec = self.transformer_decoder(\n",
    "            encoder_outputs=transformer_encoder_output, inputs=positional_embedding_output\n",
    "        )\n",
    "\n",
    "        return transformer_output_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an instance of the model\n",
    "input_shape = (X_data.shape[1], X_data.shape[2])\n",
    "num_blocks = 3\n",
    "model = MyTransformerModel(\n",
    "    input_shape=input_shape, position=500, model_dim=64, num_blocks=num_blocks\n",
    ")\n",
    "\n",
    "\n",
    "# Call the model on a batch of data to build it\n",
    "\n",
    "\n",
    "# Print the model summary\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data = Y_data.reshape((13100, 30, -1))\n",
    "model.fit(X_data, Y_data, epochs=1, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
