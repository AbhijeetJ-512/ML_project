{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13100, 40, 500)\n",
      "(13100, 40, 500, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13100, 30)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.load(\"../data/data_40_500.npy\")\n",
    "print(x.shape)\n",
    "x = x.reshape(-1, 40, 500, 1)\n",
    "print(x.shape)\n",
    "y = np.load(\"../data/norm_pad_15.npy\")  # normailized\n",
    "y = y.reshape(-1, 30)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape to add the channel dimension\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], 40, 500, 1)\n",
    "X_val_reshaped = X_val.reshape(X_val.shape[0], 40, 500, 1)\n",
    "\n",
    "\n",
    "# # # Define a learning rate scheduler function\n",
    "# def lr_scheduler(epoch, lr):\n",
    "#     return lr * 0.99  # Adjust the multiplier as needed\n",
    "\n",
    "\n",
    "# # Define early stopping callback\n",
    "# early_stopping_callback = EarlyStopping(\n",
    "#     patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "#     restore_best_weights=True,\n",
    "#     monitor=\"val_loss\",  # Metric to monitor (could be 'val_mse' or 'val_mae' based on your choice)\n",
    "# )\n",
    "\n",
    "\n",
    "# Build a simple CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation=None, input_shape=(40, 500, 1)))\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation=None, input_shape=(40, 500, 32)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=None))\n",
    "model.add(Dense(1))\n",
    "model.add(Dense(128, activation=None))\n",
    "model.add(Dense(30, activation=None))\n",
    "\n",
    "# # # Use the Adam optimizer with the learning rate scheduler\n",
    "# optimizer = Adam(learning_rate=0.002)  # Set the initial learning rate\n",
    "# model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n",
    "\n",
    "# Use MAE as the loss function\n",
    "optimizer = Adam(learning_rate=0.0025)\n",
    "model.compile(optimizer=optimizer, loss=\"mae\")\n",
    "\n",
    "# Define the learning rate scheduler callback\n",
    "# lr_scheduler_callback = LearningRateScheduler(lr_scheduler, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 14:26:07.456770: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.24GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-12-04 14:26:08.022205: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.24GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - ETA: 0s - loss: 11.6797"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 14:26:25.789089: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.17GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-12-04 14:26:26.342139: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.17GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 20s 95ms/step - loss: 11.6797 - val_loss: 0.1054\n",
      "Epoch 2/3\n",
      "164/164 [==============================] - 12s 72ms/step - loss: 0.1096 - val_loss: 0.1099\n",
      "Epoch 3/3\n",
      "164/164 [==============================] - 12s 73ms/step - loss: 0.1054 - val_loss: 0.1076\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_reshaped,\n",
    "    y_train,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val_reshaped, y_val),\n",
    "    # callbacks=[lr_scheduler_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 2s 18ms/step\n",
      "Mean Squared Error on Validation Set: 0.053969684927310586\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 38, 498, 32)       320       \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 36, 496, 16)       4624      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 18, 248, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 71424)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               9142400   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               256       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 30)                3870      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9151599 (34.91 MB)\n",
      "Trainable params: 9151599 (34.91 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_val_reshaped)\n",
    "\n",
    "# Calculate MSE on validation set\n",
    "mse_val = mean_squared_error(y_val, predictions)\n",
    "print(\"Mean Squared Error on Validation Set:\", mse_val)\n",
    "predictions[100], y[100]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/abhi/ML1/cnn/tested.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/abhi/ML1/cnn/tested.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML1/cnn/tested.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Loss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhi/ML1/cnn/tested.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidation Loss\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
