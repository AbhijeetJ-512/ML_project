{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FOwRggVcwtzP"
      },
      "outputs": [],
      "source": [
        "from transformer import Transformer  # this is the transformer.py file\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6TApzOj5xCwR"
      },
      "outputs": [],
      "source": [
        "english_file = \"E:\\\\ML\\\\transformer\\\\dataset\\\\english.txt\"  # replace this path with appropriate one\n",
        "kannada_file = \"E:\\\\ML\\\\transformer\\\\dataset\\\\kannada.txt\"  # replace this path with appropriate one\n",
        "\n",
        "# Generated this by filtering Appendix code\n",
        "\n",
        "START_TOKEN = \"<START>\"\n",
        "PADDING_TOKEN = \"<PADDING>\"\n",
        "END_TOKEN = \"<END>\"\n",
        "\n",
        "kannada_vocabulary = [\n",
        "    START_TOKEN,\n",
        "    \" \",\n",
        "    \"!\",\n",
        "    '\"',\n",
        "    \"#\",\n",
        "    \"$\",\n",
        "    \"%\",\n",
        "    \"&\",\n",
        "    \"'\",\n",
        "    \"(\",\n",
        "    \")\",\n",
        "    \"*\",\n",
        "    \"+\",\n",
        "    \",\",\n",
        "    \"-\",\n",
        "    \".\",\n",
        "    \"/\",\n",
        "    \"0\",\n",
        "    \"1\",\n",
        "    \"2\",\n",
        "    \"3\",\n",
        "    \"4\",\n",
        "    \"5\",\n",
        "    \"6\",\n",
        "    \"7\",\n",
        "    \"8\",\n",
        "    \"9\",\n",
        "    \":\",\n",
        "    \"<\",\n",
        "    \"=\",\n",
        "    \">\",\n",
        "    \"?\",\n",
        "    \"ˌ\",\n",
        "    \"ँ\",\n",
        "    \"ఆ\",\n",
        "    \"ఇ\",\n",
        "    \"ా\",\n",
        "    \"ి\",\n",
        "    \"ీ\",\n",
        "    \"ు\",\n",
        "    \"ూ\",\n",
        "    \"ಅ\",\n",
        "    \"ಆ\",\n",
        "    \"ಇ\",\n",
        "    \"ಈ\",\n",
        "    \"ಉ\",\n",
        "    \"ಊ\",\n",
        "    \"ಋ\",\n",
        "    \"ೠ\",\n",
        "    \"ಌ\",\n",
        "    \"ಎ\",\n",
        "    \"ಏ\",\n",
        "    \"ಐ\",\n",
        "    \"ಒ\",\n",
        "    \"ಓ\",\n",
        "    \"ಔ\",\n",
        "    \"ಕ\",\n",
        "    \"ಖ\",\n",
        "    \"ಗ\",\n",
        "    \"ಘ\",\n",
        "    \"ಙ\",\n",
        "    \"ಚ\",\n",
        "    \"ಛ\",\n",
        "    \"ಜ\",\n",
        "    \"ಝ\",\n",
        "    \"ಞ\",\n",
        "    \"ಟ\",\n",
        "    \"ಠ\",\n",
        "    \"ಡ\",\n",
        "    \"ಢ\",\n",
        "    \"ಣ\",\n",
        "    \"ತ\",\n",
        "    \"ಥ\",\n",
        "    \"ದ\",\n",
        "    \"ಧ\",\n",
        "    \"ನ\",\n",
        "    \"ಪ\",\n",
        "    \"ಫ\",\n",
        "    \"ಬ\",\n",
        "    \"ಭ\",\n",
        "    \"ಮ\",\n",
        "    \"ಯ\",\n",
        "    \"ರ\",\n",
        "    \"ಱ\",\n",
        "    \"ಲ\",\n",
        "    \"ಳ\",\n",
        "    \"ವ\",\n",
        "    \"ಶ\",\n",
        "    \"ಷ\",\n",
        "    \"ಸ\",\n",
        "    \"ಹ\",\n",
        "    \"಼\",\n",
        "    \"ಽ\",\n",
        "    \"ಾ\",\n",
        "    \"ಿ\",\n",
        "    \"ೀ\",\n",
        "    \"ು\",\n",
        "    \"ೂ\",\n",
        "    \"ೃ\",\n",
        "    \"ೄ\",\n",
        "    \"ೆ\",\n",
        "    \"ೇ\",\n",
        "    \"ೈ\",\n",
        "    \"ೊ\",\n",
        "    \"ೋ\",\n",
        "    \"ೌ\",\n",
        "    \"್\",\n",
        "    \"ೕ\",\n",
        "    \"ೖ\",\n",
        "    \"ೞ\",\n",
        "    \"ೣ\",\n",
        "    \"ಂ\",\n",
        "    \"ಃ\",\n",
        "    \"೦\",\n",
        "    \"೧\",\n",
        "    \"೨\",\n",
        "    \"೩\",\n",
        "    \"೪\",\n",
        "    \"೫\",\n",
        "    \"೬\",\n",
        "    \"೭\",\n",
        "    \"೮\",\n",
        "    \"೯\",\n",
        "    PADDING_TOKEN,\n",
        "    END_TOKEN,\n",
        "]\n",
        "\n",
        "english_vocabulary = [\n",
        "    START_TOKEN,\n",
        "    \" \",\n",
        "    \"!\",\n",
        "    '\"',\n",
        "    \"#\",\n",
        "    \"$\",\n",
        "    \"%\",\n",
        "    \"&\",\n",
        "    \"'\",\n",
        "    \"(\",\n",
        "    \")\",\n",
        "    \"*\",\n",
        "    \"+\",\n",
        "    \",\",\n",
        "    \"-\",\n",
        "    \".\",\n",
        "    \"/\",\n",
        "    \"0\",\n",
        "    \"1\",\n",
        "    \"2\",\n",
        "    \"3\",\n",
        "    \"4\",\n",
        "    \"5\",\n",
        "    \"6\",\n",
        "    \"7\",\n",
        "    \"8\",\n",
        "    \"9\",\n",
        "    \":\",\n",
        "    \"<\",\n",
        "    \"=\",\n",
        "    \">\",\n",
        "    \"?\",\n",
        "    \"@\",\n",
        "    \"[\",\n",
        "    \"\\\\\",\n",
        "    \"]\",\n",
        "    \"^\",\n",
        "    \"_\",\n",
        "    \"`\",\n",
        "    \"a\",\n",
        "    \"b\",\n",
        "    \"c\",\n",
        "    \"d\",\n",
        "    \"e\",\n",
        "    \"f\",\n",
        "    \"g\",\n",
        "    \"h\",\n",
        "    \"i\",\n",
        "    \"j\",\n",
        "    \"k\",\n",
        "    \"l\",\n",
        "    \"m\",\n",
        "    \"n\",\n",
        "    \"o\",\n",
        "    \"p\",\n",
        "    \"q\",\n",
        "    \"r\",\n",
        "    \"s\",\n",
        "    \"t\",\n",
        "    \"u\",\n",
        "    \"v\",\n",
        "    \"w\",\n",
        "    \"x\",\n",
        "    \"y\",\n",
        "    \"z\",\n",
        "    \"{\",\n",
        "    \"|\",\n",
        "    \"}\",\n",
        "    \"~\",\n",
        "    PADDING_TOKEN,\n",
        "    END_TOKEN,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gA8ESmCrNoc7"
      },
      "outputs": [],
      "source": [
        "index_to_kannada = {k: v for k, v in enumerate(kannada_vocabulary)}\n",
        "kannada_to_index = {v: k for k, v in enumerate(kannada_vocabulary)}\n",
        "index_to_english = {k: v for k, v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v: k for k, v in enumerate(english_vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9SYGjRdoxRg-"
      },
      "outputs": [],
      "source": [
        "with open(english_file, \"r\") as file:\n",
        "    english_sentences = file.readlines()\n",
        "with open(kannada_file, \"r\") as file:\n",
        "    kannada_sentences = file.readlines()\n",
        "\n",
        "# Limit Number of sentences\n",
        "TOTAL_SENTENCES = 200000\n",
        "english_sentences = english_sentences[:TOTAL_SENTENCES]\n",
        "kannada_sentences = kannada_sentences[:TOTAL_SENTENCES]\n",
        "english_sentences = [sentence.rstrip(\"\\n\").lower() for sentence in english_sentences]\n",
        "kannada_sentences = [sentence.rstrip(\"\\n\") for sentence in kannada_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUB-BkgFxXfM",
        "outputId": "bcf7e19c-d1df-4b69-bdfa-eb74ac1a4338"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hes a scientist.',\n",
              " \"'but we speak the truth aur ye sach hai ke gujarat mein vikas pagal hogaya hai,'' rahul gandhi further said in banaskantha\",\n",
              " '8 lakh crore have been looted.',\n",
              " 'i read a lot into this as well.',\n",
              " \"she was found dead with the phone's battery exploded close to her head the following morning.\",\n",
              " 'how did mankind come under satans rival sovereignty?',\n",
              " 'and then i became prime minister.',\n",
              " 'what about corruption?',\n",
              " 'no differences',\n",
              " '\"\"\"the shooting of the film is 90 percent done.\"']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "english_sentences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OT-aznAxc5U",
        "outputId": "716c4145-fdc2-4bb0-e883-c3dcad30c2da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.',\n",
              " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"',\n",
              " 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.',\n",
              " 'ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.',\n",
              " 'ಮಾನವಕುಲವು ಸೈತಾನನ ಆಳಿಕೆಯ ಕೆಳಗೆ ಬಂದದ್ದು ಹೇಗೆ?',\n",
              " 'ನಂತರ ಪ್ರಧಾನಿ ಕೂಡ ಆಗುತ್ತೇನೆ.',\n",
              " 'ಭ್ರಷ್ಟಾಚಾರ ಏಕಿದೆ?',\n",
              " 'ಆ ಚಿತ್ರದ ಶೇ 90ರಷ್ಟು ಚಿತ್ರೀಕರಣವೂ ಈಗಾಗಲೇ ಮುಗಿದು ಹೋಗಿದೆ.',\n",
              " 'ವಿಶೇಷ ಕಾನೂನು',\n",
              " 'ಆಗ ಅರಸನು ಗಿತ್ತೀಯನಾದ ಇತ್ತೈಯನ್ನು ನೋಡಿ--ನೀನು ನಮ್ಮ ಸಂಗಡ ಬರುವದು ಯಾಕೆ? ನಿನ್ನ ಸ್ಥಳಕ್ಕೆ ಹಿಂದಿರುಗಿ ಹೋಗಿ ಅರಸನ ಸಂಗಡ ಇರು. ಯಾಕಂದರೆ ನೀನು ಸೆರೆಹಿಡಿಯಲ್ಪಟ್ಟವನಾದ ಅನ್ಯದೇಶದವನು.']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kannada_sentences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8VAutsTxlaR",
        "outputId": "ff8fba72-020d-4f0c-b3c5-31c102b6fe9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "97th percentile length Kannada: 136.0\n",
            "97th percentile length English: 142.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "PERCENTILE = 97\n",
        "print(\n",
        "    f\"{PERCENTILE}th percentile length Kannada: {np.percentile([len(x) for x in kannada_sentences], PERCENTILE)}\"\n",
        ")\n",
        "print(\n",
        "    f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG9ezqvaxl4b",
        "outputId": "d13be774-ca07-4333-856e-76186f71caae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 164022\n",
            "Number of valid sentences: 164022\n"
          ]
        }
      ],
      "source": [
        "max_sequence_length = 200\n",
        "\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (\n",
        "        max_sequence_length - 1\n",
        "    )  # need to re-add the end token so leaving 1 space\n",
        "\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(kannada_sentences)):\n",
        "    kannada_sentence, english_sentence = (\n",
        "        kannada_sentences[index],\n",
        "        english_sentences[index],\n",
        "    )\n",
        "    if (\n",
        "        is_valid_length(kannada_sentence, max_sequence_length)\n",
        "        and is_valid_length(english_sentence, max_sequence_length)\n",
        "        and is_valid_tokens(kannada_sentence, kannada_vocabulary)\n",
        "    ):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(kannada_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "o80QDn4CxsV7"
      },
      "outputs": [],
      "source": [
        "kannada_sentences = [kannada_sentences[i] for i in valid_sentence_indicies]\n",
        "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35xhLztQiLIQ",
        "outputId": "aa70ad04-2e45-4c78-c852-61e92c51a96a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.',\n",
              " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"',\n",
              " 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kannada_sentences[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xqOFnclmyxAE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "d_model = 512\n",
        "batch_size = 30\n",
        "ffn_hidden = 2048\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "max_sequence_length = 200\n",
        "kn_vocab_size = len(kannada_vocabulary)\n",
        "\n",
        "transformer = Transformer(\n",
        "    d_model,\n",
        "    ffn_hidden,\n",
        "    num_heads,\n",
        "    drop_prob,\n",
        "    num_layers,\n",
        "    max_sequence_length,\n",
        "    kn_vocab_size,\n",
        "    english_to_index,\n",
        "    kannada_to_index,\n",
        "    START_TOKEN,\n",
        "    END_TOKEN,\n",
        "    PADDING_TOKEN,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc2hYQk9yxX0",
        "outputId": "c060f588-6a2e-4179-9475-5acafd641f1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(71, 512)\n",
              "      (position_encoder): PositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialEncoder(\n",
              "      (0): EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(125, 512)\n",
              "      (position_encoder): PositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialDecoder(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
              "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
              "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm3): LayerNormalization()\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=512, out_features=125, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "asUJX-STy7fg"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, english_sentences, kannada_sentences):\n",
        "        self.english_sentences = english_sentences\n",
        "        self.kannada_sentences = kannada_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.english_sentences[idx], self.kannada_sentences[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-auNWjkdzDge"
      },
      "outputs": [],
      "source": [
        "dataset = TextDataset(english_sentences, kannada_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roH2A4m4zF4z",
        "outputId": "f4353aa8-2f37-43b6-be0f-12aab9a35145"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "164022"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGeHNlzozIGF",
        "outputId": "ec3596fe-feee-426c-dce8-373fb07080fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(\"'but we speak the truth aur ye sach hai ke gujarat mein vikas pagal hogaya hai,'' rahul gandhi further said in banaskantha\",\n",
              " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5YDttjQ0zMrv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5468"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_loader = DataLoader(dataset, batch_size)\n",
        "iterator = iter(train_loader)\n",
        "len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EnjHKB1zM8Y",
        "outputId": "1a825e56-6706-46ee-85b5-ed7f0ac657fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('hes a scientist.', \"'but we speak the truth aur ye sach hai ke gujarat mein vikas pagal hogaya hai,'' rahul gandhi further said in banaskantha\", '8 lakh crore have been looted.', 'i read a lot into this as well.', 'how did mankind come under satans rival sovereignty?', 'and then i became prime minister.', 'what about corruption?', '\"\"\"the shooting of the film is 90 percent done.\"', 'the special statute', '\"then the king said to ittai the gittite, \"\"why do you also go with us? return, and stay with the king. for you are a foreigner, and also an exile. return to your own place.\"', 'what happened at the un general assembly?', 'the meeting was attended by prime minister narendra modi, home minister amit shah and defence minister rajnath singh, among others.', 'it has been under discussion for a long time.', 'buses cannot get there.', 'why then this tradition was not thought of?', 'kashmiri youth join indian army', 'basic amenities elude this village', 'off-budget borrowings of the state increased from rs853 crore in 2011-12 to rs,173 crore in 2017-18', 'during the monsoon season, the rubbish is swept on to the road by rainwater, creating problems for the traffic.', 'however, the other two, ramesh jarkiholi and mahesh kumatahalli, had not given any reason, he said.', 'how to make pasta salad?', 'he accused the modi government of ruining the countrys economy.', 'add chopped carrots and potatoes.', 'first shot', 'after the incident, police reached the spot and admitted the injured to the hospital.', 'her father gave kunti to his childless cousin kuntibhoja.', 'how to eat', 'granted, the standard of living varies from country to country.', 'for example, medical.', 'thats a fine plan.'), ('ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.', '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"', 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.', 'ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.', 'ಮಾನವಕುಲವು ಸೈತಾನನ ಆಳಿಕೆಯ ಕೆಳಗೆ ಬಂದದ್ದು ಹೇಗೆ?', 'ನಂತರ ಪ್ರಧಾನಿ ಕೂಡ ಆಗುತ್ತೇನೆ.', 'ಭ್ರಷ್ಟಾಚಾರ ಏಕಿದೆ?', 'ಆ ಚಿತ್ರದ ಶೇ 90ರಷ್ಟು ಚಿತ್ರೀಕರಣವೂ ಈಗಾಗಲೇ ಮುಗಿದು ಹೋಗಿದೆ.', 'ವಿಶೇಷ ಕಾನೂನು', 'ಆಗ ಅರಸನು ಗಿತ್ತೀಯನಾದ ಇತ್ತೈಯನ್ನು ನೋಡಿ--ನೀನು ನಮ್ಮ ಸಂಗಡ ಬರುವದು ಯಾಕೆ? ನಿನ್ನ ಸ್ಥಳಕ್ಕೆ ಹಿಂದಿರುಗಿ ಹೋಗಿ ಅರಸನ ಸಂಗಡ ಇರು. ಯಾಕಂದರೆ ನೀನು ಸೆರೆಹಿಡಿಯಲ್ಪಟ್ಟವನಾದ ಅನ್ಯದೇಶದವನು.', 'ವಿಶ್ವ ಗೋ ಸಮ್ಮೇಳನದ ಅಂಗಳದಲ್ಲಿ ಏನೇನು ನಡೆದಿದೆ?', 'ಪ್ರಧಾನ ಮಂತ್ರಿ ನರೇಂದ್ರ ಮೋದಿ, ರಕ್ಷಣಾ ಸಚಿವ ರಾಜನಾಥ್ ಸಿಂಗ್ ಮತ್ತು ಕೇಂದ್ರ ಗೃಹ ಸಚಿವ ಅಮಿತ್ ಷಾ ಅವರು ಮಸೂದೆಯ ಬಗ್ಗೆ ಸಾರ್ವಜನಿಕ ಚರ್ಚೆ ಗೆ ಬರುವಂತೆ ಸಂಘ ಸವಾಲು ಹಾಕಿದೆ.', 'ಎಂಬುದು ಬಹಳ ದೀರ್ಘ ಕಾಲದಿಂದಲೂ ಚರ್ಚಿತವಾಗುತ್ತಿರುವ ವಿಷಯ.', 'ಇಲ್ಲಿಗೆ ಬರಲು ಬಸ್ ಸೌಕರ್ಯವೂ ಇಲ್ಲ.', 'ಆ ಪರಂಪರೆ ಯಾಕೆ ಮುನ್ನೆಲೆಗೆ ಬರಲಿಲ್ಲ?', 'ಭಾರತೀಯ ಸೇನೆ ಸೇರಲು ಮುಗಿಬೀಳುತ್ತಿರುವ ಕಾಶ್ಮೀರಿ ಯುವಕರು !', 'ಕುಗ್ರಾಮವಾದ ಈ ಹಳ್ಳಿಯಲ್ಲಿ ಮೂಲಭೂತ ಸೌಕರ್ಯಗಳು', 'ರಾಜ್ಯದ ಬಜೆಟ್ ಯೇತರ ಸಾಲವು 2011-12ರಲ್ಲಿದ್ದ 1,853ಕೋಟಿ ರೂಪಾಯಿಯಿಂದ 2017-18ರಲ್ಲಿ 13,173 ಕ್ಕೆ ಏರಿಕಯಾಗಿದೆ.', 'ಮಳೆಗಾಲದಲ್ಲಿ ಕೊಳಚೆ ನೀರಿನೊಂದಿಗೆ ಮಳೆನೀರು ರಸ್ತೆಯಲ್ಲಿಯೇ ಮಡುಗಟ್ಟಿ ನಿಂತು ಸೊಳ್ಳೆಗಳ ಹೆಚ್ಚಳಕ್ಕೆ ಕಾರಣವಾಗುತ್ತಿದೆ.', 'ಯಾವುದೇ ಕಾರಣ ನೀಡದೆ ರಮೇಶ್ ಜಾರಕಿಹೊಳಿ ಮತ್ತು ಮಹೇಶ್ ಕಮಟಹಳ್ಳಿ ಗೈರು ಹಾಜರಾಗಿದ್ದಾರೆ ಎಂದು ಮಾಹಿತಿ ನೀಡಿದ್ದಾರೆ.', '\"ಹೇಗೆ \"\"ಗೂಳಿಕಾಳಗ\"\" ಒಂದು ಸಲಾಡ್ ತಯಾರು ಹೇಗೆ?\"', 'ಅದ್ರಲ್ಲೂ ಪ್ರಮುಖವಾಗಿ ದೇಶದ ಆರ್ಥಿಕತೆ ಪಾತಾಳಕ್ಕೆ ಕುಸಿದಿರೋದಕ್ಕೆ ಮೋದಿ ಸರ್ಕಾರವೇ ಸರ್ಕಾರ ಅಂತಾ ಟೀಕಿಸುತ್ತಿದ್ದಾರೆ.', 'ಅವರಿಗೆ ಕತ್ತರಿಸಿದ ಪಾರ್ಸ್ಲಿ ಮತ್ತು ತುಳಸಿ ಸೇರಿಸಿ.', 'ಮೊದಲ ಚಿತ್ರ ಶೂಟಿಂಗ್', 'ಅಪಘಾತದ ನಡೆದ ಕೂಡಲೇ ಮಾಹಿತಿ ಪಡೆದು ಸ್ಥಳಕ್ಕೆ ಆಗಮಿಸಿದ ಪೋಲಿಸರು ಗಾಯಗೊಂಡವರನ್ನು ಆಸ್ಪತ್ರೆ ದಾಖಲಿಸಿದ್ದಾರೆ.', 'ಆಕೆಯ ತಂದೆ ತನ್ನ ಮಕ್ಕಳಿಲ್ಲದ ಸೋದರಸಂಬಂಧಿ ಕುಂತಿಭೋಜನಿಗೆ ಕುಂತಿಯನ್ನು ಕೊಟ್ಟರು.', 'ಆಹಾರ ಬೇಯಿಸುವುದು ಹೇಗೆ', 'ಜೀವನಮಟ್ಟವು ದೇಶದಿಂದ ದೇಶಕ್ಕೆ ಬದಲಾಗುತ್ತದೆ ನಿಜ.', 'ಉದಾಹರಣೆಗೆ, ಖಗೋಳಶಾಸ್ತ್ರ.', 'ಇದೊಂದು ಉತ್ತಮ ಯೋಜನೆ.')]\n",
            "[(\"you don't know this.\", 'you are respected in society.', 'is this pic real?', '\"\"\"felicitations to all for the foundation laying of ram temple in ayodhya.\"', 'she looked stunningly beautiful in that', '{ -brand-name-nightly } blog', 'it was consistent.', 'case has been registered in malpe police station.', 'they still exist to this day.', 'breaking up isnt easy', 'students should abide by the rules.', 'we try to understand her well.', 'then the angel of yahweh commanded gad to tell david that david should go up, and raise an altar to yahweh in the threshing floor of ornan the jebusite.', 'for example, he forbids sexual immorality, idolatry, stealing, and drunkenness.', 'it is also a famous tourist destination', 'doctors and other persons concerned have been questioned.', 'but in politics.', 'to understand gods actions, we need to answer three questions: (1) who starts the war?', '14,000 crores.', 'what are those words?', 'shivakumars assumption of office as president of karnataka pradesh congress committee (kpcc).', 'now we were notified that we would receive the magazines in russian only.', 'they are found along coastlines around the world except antarcticas.', 'bill clinton went on to become president.', 'the culmination of the three stories towards the end. gives a new dimension to the film as a whole and concludes shuddhi.', 'while individuals may be allowed to die, god will never allow the extermination of his people as a whole.', '(the author is an educationist)', 'therefore i will not refrain my mouth. i will speak in the anguish of my spirit. i will complain in the bitterness of my soul.', 'are they hungry for knowledge?', 'the door did not open.'), ('ಈ ಬಗ್ಗೆ ನೀವಿನ್ನು ತಿಳಿದಿಲ್ಲ ಎ .', 'ಸಮಾಜದಲ್ಲಿ ಗೌರವವಿರುತ್ತದೆ.', 'ಈ ಫೋಟೋ ವಾಸ್ತವೋ, ಅಸಲಿಯೋ ?', \"'ಅಯೋಧ್ಯೆಯ ರಾಮ ದೇವಾಲಯದ ಅಡಿಪಾಯ ಹಾಕಿದ್ದಕ್ಕಾಗಿ ಎಲ್ಲರಿಗೂ ಶುಭಾಶಯಗಳು.\", 'ಅವಳು ಅಥವಾ ಆತ ನೋಡಲು ತುಂಬಾ ಸುಂದರ', 'ನೈಟ್ಲಿ ಬ್ಲಾಗ್', 'ಇದು ಸಂಕೇತವಾಗಿತ್ತು.', 'ಪ್ರಕರಣ ಮಳವಳ್ಳಿ ಪೊಲೀಸ್ ಠಾಣೆಯಲ್ಲಿ ದಾಖಲಾಗಿದೆ.', 'ಅಂತೆಯೇ ಅವರು ಇಂದಿಗೂ ಪ್ರಸ್ತುತವಾಗಿದ್ದಾರೆ.', 'ವೈರಾಗ್ಯ ಸುಲಭವಲ್ಲ', 'ವಿದ್ಯಾರ್ಥಿಗಳು ಕೂಡಾ ಶಿಸ್ತು ಪಾಲಿಸಬೇಕು.', '\"\"\" ಅವರೊಂದಿಗೆ ಅವನಿಗೆ ವಿವರವಾಗಿ ಅರ್ಥಮಾಡಿಕೊಳ್ಳಲು ಪ್ರಯತ್ನಿಸೋಣ.\"', 'ಆಗ ಕರ್ತನ ದೂತನು ಗಾದನಿಗೆ--ದಾವೀದನು ಹೋಗಿ ಯೆಬೂಸಿಯನಾದ ಒರ್ನಾನನ ಕಣದಲ್ಲಿ ಕರ್ತನಿಗೆ ಬಲಿಪೀಠವನ್ನು ಕಟ್ಟುವ ಹಾಗೆ ದಾವೀದನಿಗೆ ಹೇಳು ಅಂದನು.', 'ಬೈಬಲಿನಲ್ಲಿ ಯೆಹೋವನು ನಮಗೆ ಸ್ಪಷ್ಟ ಆಜ್ಞೆಗಳನ್ನು ಕೊಟ್ಟಿದ್ದಾನೆ.', 'ಇದೊಂದು ಬಹು ಜನಪ್ರಿಯವಾದ ವಿಹಾರ ತಾಣವಾಗಿದೆ', 'ಶಸ್ತ್ರ ಚಿಕಿತ್ಸೆ ಮಾಡಿದ ವೈದ್ಯರು ಮತ್ತು ಇತರ ಸಿಬ್ಬಂದಿಯನ್ನು ವಿಚಾರಣೆಗೆ ಒಳಪಡಿಸಿದ್ದಾರೆ.', 'ಆದರೆ ರಾಜಕೀಯದಲ್ಲಿರುತ್ತೇನೆ.', '( ನೆಹೆಮಿಾಯ 9: 17) ಇದನ್ನು ಅರ್ಥಮಾಡಿಕೊಳ್ಳಲು ಈ ಮೂರು ಪ್ರಶ್ನೆಗಳಿಗೆ ಉತ್ತರ ತಿಳಿಯಿರಿ: (1) ಯುದ್ಧ ಆರಂಭಿಸುವವರು ಯಾರು?', '14,000 ಒಟ್ಟುಗೂಡಿತು.', 'ಈ ಶಬ್ದಗಳು ಯಾವುವು?', 'ಕೊನೆಗೂ ಡಿಕೆ ಶಿವಕುಮಾರ್ ಅವರನ್ನ ಕರ್ನಾಟಕ ಪ್ರದೇಶ ಕಾಂಗ್ರೆಸ್ (ಕೆಪಿಸಿಸಿ) ಅಧ್ಯಕ್ಷರನ್ನಾಗಿ ಆಯ್ಮೆ ಮಾಡಿದೆ.', 'ಪತ್ರಿಕೆಯು ನಿಷೇಧಿಸಲ್ಪಟ್ಟಿತ್ತು, ಆದರೆ ಇತರ ಸ್ಥಳಗಳಿಂದ ಗುಟ್ಟಾಗಿ ನಾವು ಪತ್ರಿಕೆಗಳನ್ನು ಪಡೆಯುತ್ತಿದ್ದೆವು.', 'ಅಂಟಾರ್ಟಿಕಾ ಖಂಡದ ಹೊರತುಪಡಿಸಿ ಅವು ವಿಶ್ವದಾದ್ಯಂತ ಕಂಡುಬರುತ್ತವೆ.', 'ಇವರ ನಂತರ ಬಿಲ್ ಕ್ಲಿಂಟನ್ ಅಧ್ಯಕ್ಷ ಪಟ್ಟ ಅಲಂಕರಿಸಿದ್ದರು.', 'ಕೊನೆಯಲ್ಲಿ ಮೂರು ಕಥೆಗಳ ಪರಾಕಾಷ್ಠೆ. ಒಟ್ಟಾರೆಯಾಗಿ ಚಿತ್ರಕ್ಕೆ ಹೊಸ ಆಯಾಮವನ್ನು ನೀಡುತ್ತದೆ ಮತ್ತು ಸುಧಿಯನ್ನು ಮುಕ್ತಾಯಗೊಳಿಸುತ್ತದೆ.', 'ಕೆಲವು ವ್ಯಕ್ತಿಗಳು ಸಾಯುವಂತೆ ಅನುಮತಿಸಲ್ಪಡುವುದಾದರೂ, ತನ್ನ ಜನರೆಲ್ಲರೂ ಸಂಪೂರ್ಣವಾಗಿ ನಿರ್ಮೂಲರಾಗುವಂತೆ ದೇವರು ಎಂದಿಗೂ ಅನುಮತಿಸುವುದಿಲ್ಲ.', '(ಲೇಖಕರು ಶಿಕ್ಷಣತಜ್ಞರು)', 'ಆದದರಿಂದ ನಾನು ನನ್ನ ಬಾಯಿಯನ್ನು ಮುಚ್ಚು ವದಿಲ್ಲ. ನಾನು ಆತ್ಮ ವೇದನೆಯಿಂದ ಮಾತನಾಡು ವೆನು. ನನ್ನ ಮನೋವ್ಯಥೆಯಲ್ಲಿ ನಾನು ಗುಣುಗುಟ್ಟು ವೆನು.', 'ನಿಂಗೆ ಜ್ಞಾನದ ಹಸಿವು ನೀಗಸ್ತಾರೆ ?', 'ಅಂಗಳದ ಬಾಗಿಲು ಹಾಕಿರಲಿಲ್ಲ.')]\n",
            "[('i am non-vegetarian.', 'most of the government schools lack the teachers.', 'to the fans.', 'during the investigation, it was found that the 130 activists were in regular contact with the jmb leadership, he said.', 'it is 260 km away from mumbai city.', 'not everything happens as we expect.', 'what are you talking?', 'that young couple certainly started their marriage off on the best of foundations.', 'abbey waterfall flows from a height of 60 feet.', 'the treatment depends largely on the type and stage of the cancer.', 'i learned how to cook.', 'is coconut oil harmful?', 'may i answer?', 'what is love?', 'symptoms include rashes, diarrhea, vomiting, stomach cramps and bloating.', 'during 2015, they plan on introducing two new amg vehicles the c 63 s and gt s supercar', 'candidates belonging to st/sc, pwd category are exempted from paying any fee.', 'i love', 'you cant have everything always.', 'the death toll of coronavirus rose to 718 as 37 casualties were reported in 24 hours', 'his life was devoted to the service of the country.', 'he lost his father at an early age.', 'the vehicle is completely burnt.', 'but it was cancelled at the last moment.', 'many people are dead due to floods and extensive damage to property has occurred.', 'the scheme will be rolled out in the next couple of months.', 'what is not nice?', 'and he said, bring me a new cruse, and put salt therein. and they brought it to him.', 'the tvs entorq 125 will be up against the likes of honda activa, suzuki access and others', 'he handed over the documents for the 900 sq ft plot to nagar panchayat chairman zahir farooqui.'), ('ನಾನೇನು ಸಂಪೂರ್ಣ ಸಸ್ಯಾಹಾರಿ ಅಲ್ಲ.', 'ಬಹುತೇಕ ಸರ್ಕಾರಿ ಶಾಲೆಗಳಲ್ಲಿ ಮೂಲಭೂತ ಸೌಕರ್ಯಗಳೇ ಇಲ್ಲದಿರುವುದು ಕಂಡುಬಂದಿದೆ.', 'ಎಂಬ ಅಭಿಮಾನಿಗಳಿಗೆ ಕಾಡುತ್ತಿದೆ.', 'ಜೆಎಂಬಿ ನಾಯಕತ್ವದ ಜತೆ 130 ಕಾರ್ಯಕರ್ತರು ನಿಕಟ ಸಂಪರ್ಕದಲ್ಲಿರುವುದು ತನಿಖಾ ಹಂತದಲ್ಲಿ ಬೆಳಕಿಗೆ ಬಂದಿದೆ ಎಂದು ತಿಳಿಸಿದರು.', 'ಇದು ರಾಜ್ಯದ ರಾಜಧಾನಿಯಾದ ಮುಂಬೈನಿಂದ ಸುಮಾರು 260 ಕಿಮೀ ದೂರದಲ್ಲಿದೆ.', 'ಎಲ್ಲವೂ ನಾವು ಅಂದುಕೊಂಡಂತೆಯೇ ಆಗುವುದಿಲ್ಲ.', '\"\"\"ಏನೆಲ್ಲ ಮಾತಾಡತಾನೆ?\"', 'ಕೊನೆಯದಾಗಿ, ಈ ಘಟನೆಗಳು ಯೋಸೇಫ ಮರಿಯ ಇಬ್ಬರಿಗೂ ಪ್ರಾಮಾಣಿಕವೂ ಮುಕ್ತವೂ ಆದ ಸಂವಾದವು ಎಷ್ಟು ಮಹತ್ವ ಎಂಬ ವಿಷಯದಲ್ಲಿ ಹೆಚ್ಚನ್ನು ಕಲಿಸಿದ್ದಿರಬೇಕು.', '60 ಅಡಿ ಎತ್ತರದಿಂದ ನೀರು ಧರೆಗೆ ಧುಮ್ಮಿಕ್ಕುತ್ತದೆ.', 'ಚಿಕಿತ್ಸೆಯ ವಿಧಾನದ ಆಯ್ಕೆ ಹೆಚ್ಚಾಗಿ ರೋಗದ ಹಂತ ಮತ್ತು ಸ್ವರೂಪವನ್ನು ಅವಲಂಬಿಸಿರುತ್ತದೆ.', 'ಅಷ್ಟುಇಷ್ಟು ಅಡುಗೆ ಮಾಡುವುದನ್ನು ಕಲಿತುಬಿಟ್ಟೆ.', 'ಕಾರ್ನ್ ಎಣ್ಣೆ ಉಪಯುಕ್ತವಾಗಿದೆ?', '\"ಪ್ರಶ್ನೆ, \"\"ನಾನು ಉತ್ತರಿಸಬೇಕೇ?\"', 'ಪ್ರೀತಿ ಅಂದರೆ ಇದೇನಾ?', 'ರೋಗಲಕ್ಷಣಗಳು ಕ್ಯಾಂಪಿಂಗ್, ಭಾರಿ ಅವಧಿ, ಹೆಪ್ಪುಗಟ್ಟುವಿಕೆ, ಕೆಳ ಹೊಟ್ಟೆ ನೋವು, ಮತ್ತು ಉಬ್ಬುವುದು.', 'ಈ ಸಾಲಿಗೆ ಪ್ರಸಕ್ತ ವರ್ಷದಲ್ಲಿ ಎಎಂಜಿ ಸಿ 63 ಎಸ್ ಮತ್ತು ಜಿಟಿ ಎಸ್ ಸೂಪರ್ ಕಾರು ಸೇರ್ಪಡೆಯಾಗಲಿದೆ', 'ಎಸ್ಸಿ, ಎಸ್ಟಿ / ಪಿಡಬ್ಲ್ಯೂಡಿ ವಿಭಾಗಗಳಿಗೆ ಸೇರಿದ ಅಭ್ಯರ್ಥಿಗಳು ಅರ್ಜಿ ಪ್ರಕ್ರಿಯೆ ಶುಲ್ಕದಿಂದ ವಿನಾಯಿತಿ ಪಡೆದಿರುತ್ತಾರೆ.', 'ನಾನು ಪ್ರೀತಿಸಿದ್ದೇನೆ.', 'ಯಾವಾಗಲೂ ಎಲ್ಲಾ ಪದಾರ್ಥಗಳನ್ನು ಹೊಂದಿಲ್ಲ.', 'ದೇಶಾದ್ಯಂತ ಕರೋನಾ ಸೋಂಕಿತರ ಸಂಖ್ಯೆ 23 ಸಾವಿರಕ್ಕೂ ಹೆಚ್ಚಿದ್ದು, ಒಟ್ಟು ಈವರೆಗೆ 718 ಜನರು ಮೃತಪಟ್ಟಿದ್ದಾರೆ', 'ಅವರ ಜೀವನವನ್ನು ರಾಷ್ಟ್ರ ಸೇವೆಗೆ ಮಾತ್ರ ಮುಡಿಪಾಗಿಟ್ಟಿದ್ದರು ಎಂದು ತಿಳಿಸಿದರು.', 'ಬಾಲ್ಯದಲ್ಲೇ ತಂದೆಯನ್ನು ಕಳೆದುಕೊಂಡರು.', 'ಕಾರು ಸಂಪೂರ್ಣ ಬೆಂಕಿಗೆ ಆಹುತಿ ಆಗಿದೆ.', 'ಆದರೆ ಕೊನೇ ಕ್ಷಣದಲ್ಲಿ ತಮ್ಮ ನಿರ್ಧಾರದಿಂದ ಹಿಂದೆ ಸರಿದಿದ್ದಾರೆ.', 'ಪ್ರವಾಹದಿಂದ ಬಹಳಷ್ಟು ಆಸ್ತಿ ನಷ್ಟವಾಗಿದ್ದು, ಜನರು ಸಂಕಷ್ಟದಲ್ಲಿದ್ದಾರೆ.', 'ಮುಂದಿನ ಕೆಲವು ತಿಂಗಳಲ್ಲಿ ಈ ಯೋಜನೆಯನ್ನು ಪ್ರಕಟಿಸಲಾಗುವುದು.', 'ಚೆಂದ ಇಲ್ಲದಿದ್ದರೇನು?', 'ಅದಕ್ಕ ವನು--ನನ್ನ ಬಳಿಗೆ ಹೊಸ ಗಡಿಗೆಯನ್ನು ತಕ್ಕೊಂಡು ಬಂದು ಅದರಲ್ಲಿ ಉಪ್ಪು ಹಾಕಿರಿ ಅಂದನು. ಅವರು ಅದನ್ನು ಅವನ ಬಳಿಗೆ ತಕ್ಕೊಂಡು ಬಂದರು.', 'ಪ್ರಮುಖವಾಗಿಯೂ ಹೋಂಡಾ ಆಕ್ಟಿವಾ 125, ಸುಜುಕಿ ಆಕ್ಸೆಸ್ 125 ಹಾಗೂ ಮಹೀಂದ್ರ ಗಸ್ಟೊ 125 ಮಾದರಿಗಳಿಗೆ ಟಿವಿಎಸ್ ನೂತನ ಸ್ಕೂಟರ್ ಪ್ರತಿಸ್ಪರ್ಧಿಯಾಗಲಿದೆ', '900 ಚದರ ಅಡಿ ಜಾಗದ ಭೂ ದಾಖಲೆಗಳನ್ನು ಅವರು ನಗರ ಪಂಚಾಯತ್ ಅಧ್ಯಕ್ಷ ಜಹೀರ್ ಫಾರೂಕಿ ಅವರಿಗೆ ಹಸ್ತಾಂತರಿಸಿದರು.')]\n",
            "[('the engine is mated to a six-speed gearbox with a slip and assist clutch', 'this is what he has said.', 'but it is not taking off.', \"conflicting reports on amitabh bachchan's health confuse fans\", 'howsoever powerful.', 'the conversation thereafter went as follows:', 'what is the opposition doing?', 'best of luck to all team members.', 'but it is very expensive.', \"shah rukh khan's next film is titled sanki and will be directed by tamil director atlee.\", 'thats exactly what organizations are saying.', 'bike rider killed in motorcycle-tanker collision', 'students need not worry about it, he said.', 'there is no risk to anyone.', 'the matter had created uproar across the state.', 'parents are overjoyed to see the success of their children.', 'elections will be held soon.', 'to and fro vehicular traffic to the hill from uttanahalli road side has been completely banned.', 'do you leave?', 'define your goals and objectives.', \"what's up.\", 'how long will you take for completing the investigation?', 'there are two engine options.', 'the film in question has not yet received the certificate from censor board.', '\"even chhatrapati shivaji maharaj faced opposition from his own family,\"\" he said.\"', 'how to make a cake at home?', 'there was no harm to any passenger due to the fire.', 'they too moved their forces.', 'development has taken a hit in the state.', 'and this can be done by:'), ('ಈ ಎಂಜಿನ್ ಅನ್ನು 6- ಸ್ಪೀಡ್ ಗೇರ್ಬಾಕ್ಸ್ಗೆ ಅಸಿಸ್ಟ್ ಮತ್ತು ಸ್ಲಿಪ್ಪರ್ ಕ್ಲಚ್ನೊಂದಿಗೆ ಜೋಡಿಸಲಾಗಿದೆ', 'ಈ ಮೂಲಕ ತಾವೇನೆಂಬುದನ್ನು ತಿಳಿಸಿದ್ದಾರೆ.', 'ಆದರೆ, ಅದನ್ನು ಬಿಡುಗಡೆ ಮಾಡುತ್ತಿಲ್ಲ.', 'ಅಭಿಮಾನಿಗಳಲ್ಲಿ ಗೊಂದಲ ಸೃಷ್ಟಿಸಿದ ಅಮಿತಾಬ್ ಬಚ್ಚನ್ ಅನಾರೋಗ್ಯ ಸುದ್ದಿ', 'ಷ್ಟೇ ಬಲಶಾಲಿ ಆಗಿರಲಿ.', 'ಆನಂತರ ನಡೆದ ಸಂಭಾಷಣೆಯ ಸಾರಾಂಶ ಹೀಗಿದೆ.', 'ವಿರೋಧ ಪಕ್ಷದವರ ಕೆಲಸ ಏನಿದೆ?', 'ತಂಡದ ಸದಸ್ಯರಿಗೆಲ್ಲಾ ಅದೃಷ್ಟ ತಂದುಕೊಡುತ್ತವೆ.', 'ಆದರೆ ಆರ್ಥಿಕ ದೃಷ್ಟಿಯಿಂದ ಬಹಳ ದುಬಾರಿಯಾಗಿರುತ್ತದೆ.', 'ಶಾರುಖ್ ಖಾನ್ ಅವರ ಮುಂದಿನ ಚಲನಚಿತ್ರವನ್ನು ತಮಿಳು ನಿರ್ದೇಶಕ ಅಟ್ಲೀ ನಿರ್ದೇಶನ ಮಾಡಲಿದ್ದಾರೆ.', 'ಹಾಗೆಂದು ಆ ಸಂಸ್ಥೆಯೇ ಹೇಳಿಕೊಳ್ಳುತ್ತಿದೆ.', 'ಕಬ್ಬು ಸಾಗಿಸುತ್ತಿದ್ದ ಲಾರಿ ಹಾಗೂ ಬೈಕ್ ನಡುವೆ ಡಿಕ್ಕಿ : ಸವಾರ ಸಾವು', 'ಇದರ ಬಗ್ಗೆ ಯಾವುದೇ ಆತಂಕ ಬೇಡ ಎಂದು ವಿದ್ಯಾರ್ಥಿಗಳಿಗೆ ತಿಳಿಸಿದರು.', 'ಯಾರಿಗೂ ಅಂತಹ ಅಪಾಯಗಳಾಗಿಲ್ಲ ಎಂದರು.', 'ಈ ಸುದ್ದಿ ರಾಜ್ಯಾದ್ಯಂತ ಚರ್ಚೆಗೆ ಕಾರಣವಾಗಿತ್ತು.', 'ಪ್ರತಿ ಹೆತ್ತವರಿಗೂ ಅವರವರ ಮಕ್ಕಳ ಸಾಧನೆ ನೋಡೋ ಖುಷಿ ಇದ್ದೇ ಇರುತ್ತೆ.', 'ಚುನಾವಣೆ ಇನ್ನೇನು ಸಧ್ಯದಲ್ಲೇ ನಡೆಯಲಿದೆ.', 'ಉತ್ತನಹಳ್ಳಿ ರಸ್ತೆಯ ಕಡೆಯಿಂದ ಚಾಮುಂಡಿ ಬೆಟ್ಟಕ್ಕೆ ಬರುವ ಮತ್ತು ಹೋಗುವ ವಾಹನಗಳಿಗೆ ಸಂಚಾರವನ್ನು ಸಂಪೂರ್ಣವಾಗಿ ನಿರ್ಬಂಧಿಸಲಾಗಿದೆ.', 'ಬಿಡಲಿಕ್ಕೆ ಆಗುತ್ತಾ?', 'ನಿಮ್ಮ ಗುರಿ ಮತ್ತು ಉದ್ದೇಶಗಳನ್ನು ಗುರುತಿಸಿ.', 'ಏನು ಉರುಳೇ.', 'ತನಿಖೆಯನ್ನು ಪೂರ್ಣಗೊಳಿಸಲು ನಿಮಗೆ ಇನ್ನೂ ಎಷ್ಟು ಸಮಯ ಬೇಕು?', 'ಎರಡು ಎಂಜಿನ್ ಆಯ್ಕೆಗಳಿವೆ.', 'ಸೆನ್ಸಾರ್ ಮಂಡಳಿಯಿಂದ ಈ ಸಿನಿಮಾಗೆ ಇನ್ನೂ ಪ್ರಮಾಣಪತ್ರ ನೀಡಿಲ್ಲ.', 'ಛತ್ರಪತಿ ಶಿವಾಜಿ ಕೂಡ ತಮ್ಮದೇ ಕುಟುಂಬದಿಂದಲೇ ವಿರೋಧವನ್ನು ಎದುರಿಸಿದ್ದರು.', 'ಹೇಗೆ ಮನೆಯಲ್ಲಿ ಒಂದು ಕಾರ್ಟೂನ್ ರಚಿಸಲು?', 'ಬೆಂಕಿ ಅವಘಡದಲ್ಲಿ ಪ್ರಯಾಣಿಕರ ಪ್ರಾಣಕ್ಕೆ ಹಾನಿಯಾಗಿಲ್ಲ.', 'ತಮ್ಮ ಅಧಿಕಾರವನ್ನು ಚಲಾಯಿಸಿದ್ದರು ಕೂಡಾ.', 'ರಾಜ್ಯದಲ್ಲಿ ಅಭಿವೃದ್ಧಿ ಪರ್ವ ಬಂದಿದೆ.', 'ಮತ್ತು ನೀವು ಇದನ್ನು ಪರಿಹರಿಸಬಹುದು:')]\n",
            "[('a proposal has been made.', 'but the government is unmoved.', 'the programme was attended by students and teachers of the school.', 'they taught me much about the true holy father in heaven, jehovah god.', 'what we can do', 'the bjp is trying to gain a strong foothold in the state.', 'farmers are in distress all over the country.', 'but euphoria, relief, and achievement likewise provoke emotional tears in this case, tears of joy.', 'and to pass by you into macedonia, and to come again out of macedonia unto you, and of you to be brought on my way toward judaea.', 'in case of an attack, we need to respond swiftly to minimise the damage.', 'for unknown reasons.', 'community spread', 'which board?', 'there are 14 girls and 12 junior players.', 'preparing for the role', 'recipe: banana blossom salad', 'do not isolate yourself from your faithful christian brothers and sisters.', 'patient with leukaemia disease.', 'the total length of the highway road is 250 km.', 'the governors post is a constitutional post.', 'development and growth', 'it was even found among the ones he had chosen as apostles!', 'in their hands lies the future of india.', 'government high school', 'they stay in their own world.', 'you cant be regretful.', 'location: ranga rao road, near shankar mutt, shankarapura, near basavanagudi, bangalore', \"that's all humbug.\", 'they were all there.', 'he was keen to learn english also.'), ('ಪ್ರಸ್ತಾವನೆ ಸಲ್ಲಿಸಿತ್ತು.', 'ಆದರೆ ಈ ಸರ್ಕಾರ ಸದ್ಯ ಅತಂತ್ರದಲ್ಲಿದೆ.', 'ಈ ಕಾರ್ಯಕ್ರಮದಲ್ಲಿ ಶಿಕ್ಷಕ ವೃಂದ ಹಾಗೂ ವಿದ್ಯಾರ್ಥಿಗಳು ಭಾಗಿಯಾಗಿದ್ದರು.', 'ಅವರು ನನಗೆ ಸ್ವರ್ಗದಲ್ಲಿರುವ ನಿಜವಾದ ಪವಿತ್ರ ತಂದೆ ಯೆಹೋವ ದೇವರ ಬಗ್ಗೆ ಬಹಳಷ್ಟನ್ನು ಕಲಿಸಿದರು.', 'ನಾವೇನು ಮಾಡಬಹುದು ?', 'ರಾಜ್ಯದಲ್ಲಿ ಬಿಜೆಪಿ ಅಧಿಕಾರ ಪಡೆದುಕೊಳ್ಳಲು ಶಥಾಯ ಗಥಾಯ ಪ್ರಯತ್ನ ಮಾಡುತ್ತಿದೆ', 'ದೇಶಾದ್ಯಂತ ರೈತರು ಸಂಕಷ್ಟದಲ್ಲಿದ್ದಾರೆ.', 'ಭಾವನಾತ್ಮಕ ಕಣ್ಣೀರು ಉಕ್ಕಿ ಬರಲು ಅನೇಕ ಕಾರಣಗಳಿವೆ.', 'ತರುವಾಯ ನಿಮ್ಮ ಮಾರ್ಗವಾಗಿ ಮಕೆದೋನ್ಯಕ್ಕೆ ಹೋಗಿ ತಿರಿಗಿ ಮಕೆದೋನ್ಯದಿಂದ ನಿಮ್ಮ ಬಳಿಗೆ ಬಂದು ನಿಮ್ಮಿಂದ ಯೂದಾಯಕ್ಕೆ ಸಾಗಕಳುಹಿಸಲ್ಪಡುವಂತೆಯೂ ಯೋಚಿಸಿದ್ದೆನು.', 'ದಾಳಿಯ ಸಂದರ್ಭದಲ್ಲಿ ಹಾನಿಯನ್ನು ಕನಿಷ್ಠಗೊಳಿಸಲು ನಾವು ಚುರುಕಾಗಿ ಕಾರ್ಯಾಚರಿಸಬೇಕಾಗುತ್ತದೆ.', 'ಸ್ಪಷ್ಟೀಕರಿಸದ ಕಾರಣಗಳಿಗಾಗಿ', 'ಸಮುದಾಯ ಹರಡುವಿಕೆ', 'ಯಾರಿಗೆ ಯಾವ ಮಂಡಳಿ?', 'ಇವುಗಳಲ್ಲಿ 14 ಗಂಡು, 12 ಹೆಣ್ಣು ಚಿರತೆ ಸೇರಿವೆ.', 'ಪಾತ್ರಕ್ಕಾಗಿ ತಯಾರಿ', '\"ಪಾಕವಿಧಾನ: ಹೂಕೋಸು \"\"ಆಲೂಗಡ್ಡೆ\"\" ಸಲಾಡ್\"', 'ನಂಬಿಗಸ್ತ ಸಹೋದರ ಸಹೋದರಿಯರಿಂದ ನಿಮ್ಮನ್ನು ಪ್ರತ್ಯೇಕಿಸಿಕೊಳ್ಳಬೇಡಿ.', 'ತೀವ್ರತರವಾದ ಲ್ಯುಕೇಮಿಯಾ ಬಳಲುತ್ತಿರುವ ರೋಗಿಗಳು.', 'ರಸ್ತೆಯಲ್ಲಿ ಒಟ್ಟು ಅಗಲ - 250 ಮೀಟರ್.', 'ರಾಜ್ಯಪಾಲರದು ಸಂವಿಧಾನಾತ್ಮಕ ಹುದ್ದೆ.', 'ಅಭಿವೃದ್ಧಿ ಮತ್ತು ಅನಾರೋಗ್ಯ', 'ಯೇಸು ಸಾಯುವ ದಿನದ ವರೆಗೂ ಅವರು ಹೆಬ್ಬಯಕೆಯನ್ನು ತೋರ್ಪಡಿಸಿದರು.', 'ಇವರ ಆಟದ ಮೇಲೆ ಭಾರತದ ಭವಿಷ್ಯ ನಿಂತಿದೆ.', 'ಸಮಸ್ಯೆಗಳ ಆಗರ ಸರ್ಕಾರಿ ಪ್ರೌಢಶಾಲೆ', 'ಇವರು ತಮ್ಮದೇ ಆಗಿರುವ ಲೋಕದಲ್ಲಿ ವಿಹರಿಸುತ್ತಾ ಇರುವರು.', 'ನಿಮಗೆ ಕ್ಷಮಿಸುವ ಮನಸ್ಸಿಲ್ಲದಿರಬಹುದು.', 'ಸ್ಥಳ : ಜಕ್ಕಸಂದ್ರ, ಸರ್ಜಾಪುರ ರಸ್ತೆ, ಬೆಂಗಳೂರು', 'ಅದೆಲ್ಲ ಗಿಮಿಕ್ ಅಷ್ಟೆ.', 'ಅವರೆಲ್ಲರೂ ಇದ್ದರು.', 'ಅವರಿಗೆ ಇಂಗ್ಲಿಷ್ ಕಲಿಯುವುದಕ್ಕೆ ತುಂಬಾ ಆಸೆ ಇತ್ತು.')]\n"
          ]
        }
      ],
      "source": [
        "for batch_num, batch in enumerate(iterator):\n",
        "    print(batch)\n",
        "    if batch_num > 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XnanjzqtzQi8"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "criterian = nn.CrossEntropyLoss(\n",
        "    ignore_index=kannada_to_index[PADDING_TOKEN], reduction=\"none\"\n",
        ")\n",
        "\n",
        "# When computing the loss, we are ignoring cases when the label is the padding token\n",
        "for params in transformer.parameters():\n",
        "    if params.dim() > 1:\n",
        "        nn.init.xavier_uniform_(params)\n",
        "\n",
        "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_saWU5QmVem2"
      },
      "outputs": [],
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "\n",
        "def create_masks(eng_batch, kn_batch):\n",
        "    num_sentences = len(eng_batch)\n",
        "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length], True)\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
        "    encoder_padding_mask = torch.full(\n",
        "        [num_sentences, max_sequence_length, max_sequence_length], False\n",
        "    )\n",
        "    decoder_padding_mask_self_attention = torch.full(\n",
        "        [num_sentences, max_sequence_length, max_sequence_length], False\n",
        "    )\n",
        "    decoder_padding_mask_cross_attention = torch.full(\n",
        "        [num_sentences, max_sequence_length, max_sequence_length], False\n",
        "    )\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "        eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(\n",
        "            kn_batch[idx]\n",
        "        )\n",
        "        eng_chars_to_padding_mask = np.arange(\n",
        "            eng_sentence_length + 1, max_sequence_length\n",
        "        )\n",
        "        kn_chars_to_padding_mask = np.arange(\n",
        "            kn_sentence_length + 1, max_sequence_length\n",
        "        )\n",
        "        encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "        encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "        decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
        "        decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "        decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "        decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask = torch.where(\n",
        "        look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0\n",
        "    )\n",
        "    decoder_cross_attention_mask = torch.where(\n",
        "        decoder_padding_mask_cross_attention, NEG_INFTY, 0\n",
        "    )\n",
        "    return (\n",
        "        encoder_self_attention_mask,\n",
        "        decoder_self_attention_mask,\n",
        "        decoder_cross_attention_mask,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdgtTSKvwN9_"
      },
      "source": [
        "Modify mask such that the padding tokens cannot look ahead.\n",
        "In Encoder, tokens before it should be -1e9 while tokens after it should be -inf.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLcXI4wkMLck"
      },
      "source": [
        "Note the target mask starts with 2 rows of non masked items: https://github.com/SamLynnEvans/Transformer/blob/master/Beam.py#L55\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju59VDGLuOqf",
        "outputId": "0ad34e31-521a-4ca2-f444-26b5374946f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Iteration 0 : 5.489313125610352\n",
            "English: hes a scientist.\n",
            "Kannada Translation: ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.\n",
            "Kannada Prediction: ೃೃँँೃರೃೃ,+,,3ి>ೃಥಙೃೃు3+ೃँి,,+,+ಥಏಥ೧3೧ೄँ(ँ,3ಥ,್3್ँ್್್ಏ್್ಥ,,(್ಥಥ೧೧ँ,ಥँ.ˌ,,,೧ಥ,ಥೄ,,೧ಥ,,,,,ಥಥಥಥಥಥಥಥ,ೄೄೄೄೄ,ೄ,ೄೄ೧ೄಱ,,ೄ>>೧ँೄಶಶಶ೧ಥಥ೧ೄ೧ಥಥಥೄಥಥಥ,ಥಥಥಥಥಥಥಥಥ,,,ಥ,ಳ್ಇುಙಇಥಥಥ,ಥ,ಇ,್೧್೧ಥ<,<ಇ<,,,,<<<<<೧<<<ಙ್<<<<<<೭೭<<<್್\n",
            "Evaluation translation (should we go to the mall?) : ('                  ್್್           ್್್      ್್್್್್  ್್್್್್ ್್್್್              ್್್್್್್್್ ್್್್್್್್್್್್್್್್್್ ್್್್್್್       ರರ್್್್್್್  ್್್್            ್್್್್   ್್್್್್್್್್್           ್್್್್್್್್್್್್್್್್್್್್್್್್',)\n",
            "-------------------------------------------\n",
            "Iteration 100 : 3.5280466079711914\n",
            "English: she ate it.\n",
            "Kannada Translation: ಅವಳು ಅವನಿಗೆ ಊಟ ಹಾಕಿದಳೂ.\n",
            "Kannada Prediction: ನರದ್   ್ ್ ್್  ಿ್  ಿ್\n",
            "Evaluation translation (should we go to the mall?) : ('ಕಕ್                   ು  ಿ ಿ <END>',)\n",
            "-------------------------------------------\n",
            "Iteration 200 : 3.2629799842834473\n",
            "English: caste and religion were unknown.\n",
            "Kannada Translation: ಜಾತಿ, ಬೇಧ ಎಂಬುದೇ ಗೊತ್ತಿರಲಿಲ್ಲ.\n",
            "Kannada Prediction: ಇದ್್        ್  ್ ನ    ್  ್ ್ ಿ\n",
            "Evaluation translation (should we go to the mall?) : ('ಇದರ್    ನ್    ನ್ ನಿ   ನಿ  ದಿ ದಿ ದಿ ದಿದಿ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 300 : 2.9576070308685303\n",
            "English: seeing this, ruler was elated and told his son that the strength of the rabbit is due to the valour of the region's citizenry.\n",
            "Kannada Translation: ಇದನ್ನು ನೋಡಿ, ಆಡಳಿತಗಾರನು ಉತ್ಸಾಹದಿಂದ ಮತ್ತು ಮೊಲದ ಬಲವು ಪ್ರದೇಶದ ನಾಗರಿಕರ ಶೌರ್ಯದ ಕಾರಣ ಎಂದು ತನ್ನ ಮಗನಿಗೆ ತಿಳಿಸಿದನು.\n",
            "Kannada Prediction: ಅದ್ು ಿ ಕ್ ್  ಸ ಿಿ ್್ ುು ಸ ್  ರಿ್ ದ್ಸ ್ರ್ ವಿ ್್ಮ ಿು ಮ್ ್್ ್್ಹ್ದಿ್ ್ುಕುದಿ  ್ಸ್ರಿ ಮರದು ಸ್ು ್ಅಾಿ್ ಿ ಸ್ ಿ ು ುಿ \n",
            "Evaluation translation (should we go to the mall?) : ('ಇದ್ರು ಅ ಮ್ ಮ್ರ್ರಿ ಮಾದ್ದ್ಲ್ದ್ದ್ದ್ದ್ದ್ದ್ದ್ದು.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 400 : 2.892193078994751\n",
            "English: i also had such a feeling.\n",
            "Kannada Translation: ನನಗಂತೂ ಅಂಥ ಅನುಭೂತಿಯೇ ಆಗಿದ್ದು.\n",
            "Kannada Prediction: ಇುಿ್ದ  ನವದ ಅವ್ ಾ ್  ವನರ್ದ್ಲ್ \n",
            "Evaluation translation (should we go to the mall?) : ('ಇದು ಮ ಮಾಗಿ ಮಾಗೆ ಮಾಗಿದು ಮಾರು ಮು.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 500 : 2.937441349029541\n",
            "English: what if its too late?\n",
            "Kannada Translation: ದೀರ್ಘಕಾಲ ಇದ್ದರೆ ನಾಟ್ ಏನಾಗುತ್ತದೆ?\n",
            "Kannada Prediction: ಅರದದುಯನೆಗ್ಸದ್ತುು ಹ್ರೆತಹಲಿಗಳದ್ದ್ೆ.\n",
            "Evaluation translation (should we go to the mall?) : ('ಅದರು ಅವಿ ಸ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 600 : 2.710297107696533\n",
            "English: i am happy that our principals and teachers are enthusiastically participating in this campaign to implement the national education policy.\n",
            "Kannada Translation: ರಾಷ್ಟ್ರೀಯ ಶಿಕ್ಷಣ ನೀತಿಯನ್ನು ಜಾರಿಗೆ ತರುವ ಈ ಅಭಿಯಾನದಲ್ಲಿ ನಮ್ಮ ಪ್ರಾಂಶುಪಾಲರು ಮತ್ತು ಶಿಕ್ಷಕರು ಉತ್ಸಾಹದಿಂದ ಭಾಗವಹಿಸುತ್ತಿರುವುದು ನನಗೆ ಸಂತೋಷವಾಗಿದೆ.\n",
            "Kannada Prediction: ಆಾರ್ತ ತ ರ ಪಿ ್ಯ್ ಪ್ವ್ಯಾ್ರ್ ಸಿರು ಿ ಪ್್ ಾಸ ಮನಾ ಾರು ್ತಿ ಮ್ಾತಿಪ್ಲಿಗದ್ ್ಂ್ಿ ನಿ್ನ್ ನ್ದ್ಯಿಿ  ನ ುತಿರಿ್ ತ ಸಾಗಿುಿ ್ ಿನ್ದು ು ು ಪ್್ಿ ಪಿದ್ದ್ಾರಿ   \n",
            "Evaluation translation (should we go to the mall?) : ('ಇದರು ಅದು ನು ಸ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ಲ.<END>',)\n",
            "-------------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32me:\\ML project\\transformer\\Transformer-Neural-Network\\Transformer_Trainer_Notebook.ipynb Cell 23\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML%20project/transformer/Transformer-Neural-Network/Transformer_Trainer_Notebook.ipynb#X32sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m valid_indicies \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML%20project/transformer/Transformer-Neural-Network/Transformer_Trainer_Notebook.ipynb#X32sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     labels\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m kannada_to_index[PADDING_TOKEN], \u001b[39mFalse\u001b[39;00m, \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML%20project/transformer/Transformer-Neural-Network/Transformer_Trainer_Notebook.ipynb#X32sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML%20project/transformer/Transformer-Neural-Network/Transformer_Trainer_Notebook.ipynb#X32sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39msum() \u001b[39m/\u001b[39m valid_indicies\u001b[39m.\u001b[39msum()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/ML%20project/transformer/Transformer-Neural-Network/Transformer_Trainer_Notebook.ipynb#X32sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML%20project/transformer/Transformer-Neural-Network/Transformer_Trainer_Notebook.ipynb#X32sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML%20project/transformer/Transformer-Neural-Network/Transformer_Trainer_Notebook.ipynb#X32sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# train_losses.append(loss.item())\u001b[39;00m\n",
            "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
            "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "transformer.train()\n",
        "transformer.to(device)\n",
        "total_loss = 0\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    iterator = iter(train_loader)\n",
        "    for batch_num, batch in enumerate(iterator):\n",
        "        transformer.train()\n",
        "        eng_batch, kn_batch = batch\n",
        "        (\n",
        "            encoder_self_attention_mask,\n",
        "            decoder_self_attention_mask,\n",
        "            decoder_cross_attention_mask,\n",
        "        ) = create_masks(eng_batch, kn_batch)\n",
        "        optim.zero_grad()\n",
        "        kn_predictions = transformer(\n",
        "            eng_batch,\n",
        "            kn_batch,\n",
        "            encoder_self_attention_mask.to(device),\n",
        "            decoder_self_attention_mask.to(device),\n",
        "            decoder_cross_attention_mask.to(device),\n",
        "            enc_start_token=False,\n",
        "            enc_end_token=False,\n",
        "            dec_start_token=True,\n",
        "            dec_end_token=True,\n",
        "        )\n",
        "        labels = transformer.decoder.sentence_embedding.batch_tokenize(\n",
        "            kn_batch, start_token=False, end_token=True\n",
        "        )\n",
        "        loss = criterian(\n",
        "            kn_predictions.view(-1, kn_vocab_size).to(device),\n",
        "            labels.view(-1).to(device),\n",
        "        ).to(device)\n",
        "        valid_indicies = torch.where(\n",
        "            labels.view(-1) == kannada_to_index[PADDING_TOKEN], False, True\n",
        "        )\n",
        "        loss = loss.sum() / valid_indicies.sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        # train_losses.append(loss.item())\n",
        "        if batch_num % 100 == 0:\n",
        "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
        "            print(f\"English: {eng_batch[0]}\")\n",
        "            print(f\"Kannada Translation: {kn_batch[0]}\")\n",
        "            kn_sentence_predicted = torch.argmax(kn_predictions[0], axis=1)\n",
        "            predicted_sentence = \"\"\n",
        "            for idx in kn_sentence_predicted:\n",
        "                if idx == kannada_to_index[END_TOKEN]:\n",
        "                    break\n",
        "                predicted_sentence += index_to_kannada[idx.item()]\n",
        "            print(f\"Kannada Prediction: {predicted_sentence}\")\n",
        "\n",
        "            transformer.eval()\n",
        "            kn_sentence = (\"\",)\n",
        "            eng_sentence = (\"should we go to the mall?\",)\n",
        "            for word_counter in range(max_sequence_length):\n",
        "                (\n",
        "                    encoder_self_attention_mask,\n",
        "                    decoder_self_attention_mask,\n",
        "                    decoder_cross_attention_mask,\n",
        "                ) = create_masks(eng_sentence, kn_sentence)\n",
        "                predictions = transformer(\n",
        "                    eng_sentence,\n",
        "                    kn_sentence,\n",
        "                    encoder_self_attention_mask.to(device),\n",
        "                    decoder_self_attention_mask.to(device),\n",
        "                    decoder_cross_attention_mask.to(device),\n",
        "                    enc_start_token=False,\n",
        "                    enc_end_token=False,\n",
        "                    dec_start_token=True,\n",
        "                    dec_end_token=False,\n",
        "                )\n",
        "                next_token_prob_distribution = predictions[0][\n",
        "                    word_counter\n",
        "                ]  # not actual probs\n",
        "                next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "                next_token = index_to_kannada[next_token_index]\n",
        "                kn_sentence = (kn_sentence[0] + next_token,)\n",
        "                if next_token == END_TOKEN:\n",
        "                    break\n",
        "\n",
        "            print(f\"Evaluation translation (should we go to the mall?) : {kn_sentence}\")\n",
        "            print(\"-------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nosVPGVijId"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOQe-juylBiJ"
      },
      "outputs": [],
      "source": [
        "transformer.eval()\n",
        "\n",
        "\n",
        "def translate(eng_sentence):\n",
        "    eng_sentence = (eng_sentence,)\n",
        "    kn_sentence = (\"\",)\n",
        "    for word_counter in range(max_sequence_length):\n",
        "        (\n",
        "            encoder_self_attention_mask,\n",
        "            decoder_self_attention_mask,\n",
        "            decoder_cross_attention_mask,\n",
        "        ) = create_masks(eng_sentence, kn_sentence)\n",
        "        predictions = transformer(\n",
        "            eng_sentence,\n",
        "            kn_sentence,\n",
        "            encoder_self_attention_mask.to(device),\n",
        "            decoder_self_attention_mask.to(device),\n",
        "            decoder_cross_attention_mask.to(device),\n",
        "            enc_start_token=False,\n",
        "            enc_end_token=False,\n",
        "            dec_start_token=True,\n",
        "            dec_end_token=False,\n",
        "        )\n",
        "        next_token_prob_distribution = predictions[0][word_counter]\n",
        "        next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "        next_token = index_to_kannada[next_token_index]\n",
        "        kn_sentence = (kn_sentence[0] + next_token,)\n",
        "        if next_token == END_TOKEN:\n",
        "            break\n",
        "    return kn_sentence[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDVH_YsxlK6q",
        "outputId": "83c47f99-53c0-4c2d-c26a-aaa426f50563"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"what should we do when the day starts?\")\n",
        "print(translation)\n",
        "# ದಿನ ಪ್ರಾರಂಭವಾದಾಗ ನಾವು ಏನು ಮಾಡಬೇಕು?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9yfawBnul0W",
        "outputId": "d9e6e6b7-683b-45f9-f013-c53c31038306"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"how is this the truth?\")\n",
        "print(translation)\n",
        "# ಇದು ಹೇಗೆ ಸತ್ಯ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpdYBk5-urcQ",
        "outputId": "ca7249c5-efda-4f41-f052-ecef9691be82"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"the world is a large place with different people\")\n",
        "print(translation)\n",
        "# ಪ್ರಪಂಚವು ವಿಭಿನ್ನ ಜನರೊಂದಿಗೆ ದೊಡ್ಡ ಸ್ಥಳವಾಗಿದೆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni9e2UYUuxi3",
        "outputId": "b93968e6-3f12-4794-b277-3a3821af221e"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"my name is ajay\")\n",
        "print(translation)\n",
        "# ನನ್ನ ಹೆಸರು ಅಜಯ್"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJuJKHqFldW3",
        "outputId": "71aa2c6c-ec77-4b02-d39b-bd724012fb53"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"i cannot stand this smell\")\n",
        "print(translation)\n",
        "# ನಾನು ಈ ವಾಸನೆಯನ್ನು ಸಹಿಸುವುದಿಲ್ಲ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxHC4Lirlfu8",
        "outputId": "5a3ca401-abac-41af-d9db-99fb7a57fe00"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"noodles are the best\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLVOSI0Oli16",
        "outputId": "9f9445a1-2802-4688-900c-d7ecf2bd5c35"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"why care about this?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MStWCoAt0Ixp"
      },
      "source": [
        "This translated pretty well : \"What is the reason. Why\" without punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB6TEJfGlkRT",
        "outputId": "adb465d5-b0ed-4be4-9251-62c079f49491"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"this is the best thing ever\")\n",
        "print(translation)\n",
        "# ಇದು ಎಂದೆಂದಿಗೂ ಉತ್ತಮವಾಗಿದೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxsUjSybxYkh"
      },
      "source": [
        "The translation : \"This is very unusual\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQwDbuWBlmmA",
        "outputId": "7ae0e2c0-02c0-4c74-bc47-26b67da55a06"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"i am here\")\n",
        "print(translation)\n",
        "# ನಾನು ಇಲ್ಲಿದ್ದೇನೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmyZ2-I6x0Yf"
      },
      "source": [
        "Translation: \"I have heard\". \n",
        "This is why word based translator may perform better than character translator. This is actually very good at optimizing the objective of the current transformer even though the translation is off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ifeV4bGluIj",
        "outputId": "6bce922d-d0db-432c-e6c6-97d482f1dea6"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"click this\")\n",
        "print(translation)\n",
        "# ಇದನ್ನು ಕ್ಲಿಕ್ ಮಾಡಿ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RB5DUBEl1kD",
        "outputId": "9109e504-8b9e-45dc-e13c-e878b3741e4e"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"where is the mall?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdeJ9CvMn5LM",
        "outputId": "044b5dac-29a9-4b60-e66a-4e10739a9756"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"what should we do?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoikFnov1rj-"
      },
      "source": [
        "This is correct; but it absolutely fumbles on the next one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GFeyzrg1fIZ",
        "outputId": "2b4dfb04-def2-4725-9e76-5476bf85e2ef"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"today, what should we do\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0upygLS-sXcO",
        "outputId": "53a62435-ab16-4d4c-8a9f-0bf07af2a501"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"why did they activate?\")\n",
        "print(translation)\n",
        "# ಅವರು ಏಕೆ ಸಕ್ರಿಯಗೊಳಿಸಿದರು?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSrsqEGmtcl2",
        "outputId": "1b8b8faa-5370-426a-f2f2-fe852696bf7a"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"why did they do this?\")\n",
        "print(translation)\n",
        "# ಅವರು ಇದನ್ನು ಏಕೆ ಮಾಡಿದರು?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7ISM5rd3BLJ"
      },
      "source": [
        "That turned out well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjTcH2HFtyld",
        "outputId": "9dea5498-f139-4cbd-ee8c-6108e1b92fc4"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"i am well.\")\n",
        "print(translation)\n",
        "# ನಾನು ಆರಾಮವಾಗಿದ್ದೇನೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP0YX2g74eP7"
      },
      "source": [
        "Translation: \"I will give you something\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pGdN13kt5Br",
        "outputId": "240256c5-f594-41b0-8218-2c70a22a156f"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"whats the word on the street?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFYZ6pOe4o-X"
      },
      "source": [
        "Kind of close semantically. Translation is something like: \"What is this about\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzrOcNUk1-e5"
      },
      "source": [
        "## Insights\n",
        "\n",
        "- When training, we can treat every alphabet as a single unit instead of splitting it into it's corresponding parts to preserve meaning. For example, ಮಾ should be 1 unit when comuting a loss. It should not be decomposed into ಮ + ఆ\n",
        "- Using word-based or BPE based tokenizations may help mitigate (1). Also, we will get valid word (or BPE) units if we do so. \n",
        "- Make sure the training set has a large variety of sentences that are not just about one topic like \"work\" and \"government\"\n",
        "- Increase the number of encoder / decoder units for better translations. It was set to the minimum of 1 of each unit here.\n",
        "- Create a translator with a language you understand ideally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd6_k0Uu5V7f"
      },
      "source": [
        "Overall, this model definately learned something. And you can use other languages instead of this kannada language and might see better luck"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
